{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Regime-Aware Multifactor + ML/RL Alpha Engine with Backward & Forward Testing\n",
        "\n",
        "## Project Description\n",
        "\n",
        "This project is a **modular trading research system** designed to generate **pure alpha** (market-neutral returns independent of market beta) by combining **proven multifactor investing principles** with **modern machine learning and reinforcement learning techniques**, and testing them with **rigorous statistical validation**.\n",
        "\n",
        "The strategyâ€™s profit engine comes from exploiting cross-sectional mispricings in a broad large-cap U.S. universe (**S&P 500 training set with dynamic top-N selection by confidence**) by identifying which stocks are likely to outperform or underperform others over the next 5â€“10 days. This is achieved through:\n",
        "\n",
        "- **Multifactor Alpha Layer:**  \n",
        "  - **Value** (cheap stocks with potential to mean-revert up)  \n",
        "  - **Momentum** (stocks in persistent trends)  \n",
        "  - **Quality** (financially strong, operationally robust companies)  \n",
        "  - Per-regime factor blending with shrinkage to avoid overfitting.\n",
        "\n",
        "- **Machine Learning Overlays:**  \n",
        "  - **LSTM** (sequence model) to capture time-series patterns in returns, volatility, and technicals.  \n",
        "  - **LightGBM/XGBoost/MLP** (tabular models) to detect nonlinear interactions in cross-sectional features.  \n",
        "  - **Stacking meta-learner** to optimally blend factor and ML outputs.  \n",
        "  - **Uncertainty quantification** via MC-dropout and quantile models to control position sizing.\n",
        "\n",
        "- **Regime Detection:**  \n",
        "  - Hidden Markov Model (HMM) to classify markets as **Risk-On**, **Risk-Off**, or **Transition**, adjusting model weights and risk accordingly.\n",
        "\n",
        "- **Portfolio Construction & Risk Management:**  \n",
        "  - **Blackâ€“Litterman optimization** to integrate model views with market-implied returns.  \n",
        "  - **Risk parity** to balance sector/factor exposures.  \n",
        "  - **Dynamic hedging** against SPY/sector ETFs to maintain market neutrality.\n",
        "\n",
        "- **Reinforcement Learning (PPO):**  \n",
        "  - Learns a sizing and hedging policy that adapts risk-taking to forecast strength, uncertainty, and current market regime, maximizing return per unit of tail risk (CVaR-aware reward).\n",
        "\n",
        "## Testing & Validation\n",
        "\n",
        "The project integrates **both backward and forward testing** to ensure robustness:\n",
        "\n",
        "- **Backward Testing (Historical):**  \n",
        "  - Walk-forward analysis with purged cross-validation to avoid look-ahead bias.  \n",
        "  - Statistical significance tests (Dieboldâ€“Mariano, SPA/White Reality Check) to confirm non-randomness.  \n",
        "  - Monte Carlo block bootstrap to estimate confidence intervals and failure probabilities.  \n",
        "  - VaR/CVaR analysis and stress testing against historical crisis scenarios.\n",
        "\n",
        "- **Forward Testing (Shadow, No Trades):**  \n",
        "  - Daily simulation using only forward data, logging PnL and risk metrics without sending orders.  \n",
        "  - Weekly retraining and monthly auto-generated tear sheets to track live performance against backtest expectations.  \n",
        "  - Recommended forward-testing period: 4â€“12 weeks before considering paper/live execution.\n",
        "\n",
        "## Goal\n",
        "\n",
        "The systemâ€™s goal is to produce **consistent, statistically validated alpha** with low correlation to the market and controlled drawdowns, using a combination of **factor investing, machine learning, and reinforcement learning**. This approach maximizes the probability of sustainable profitability before any real capital is risked.\n",
        "\n"
      ],
      "metadata": {
        "id": "Lh0jFF7ysB3v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Objectives & Success Criteria\n",
        "- Primary objective: Generate statistically significant pure alpha (market-neutral) with controlled drawdowns after transaction costs.\n",
        "\n",
        "- Secondary objective: Build a repeatable process capable of ongoing, unattended forward testing that outputs monthly tear sheets.\n",
        "\n",
        "- Pass/Fail gates (OO-S):\n",
        "  - Annualized Sharpe â‰¥ 1.0 (cost-adjusted) across walk-forward windows.\n",
        "  - SPA/White Reality Check non-rejection vs family of alternatives at 5â€“10% level.\n",
        "  - Max DD â‰¤ 15â€“20% (tunable) in backtests.\n",
        "  - Forward test (4â€“8+ weeks): positive return, rolling Sharpe > 0.8, tail losses consistent with backtest VaR/CVaR.\n",
        "\n"
      ],
      "metadata": {
        "id": "33IwP6ukt9c3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Data & Universe"
      ],
      "metadata": {
        "id": "dc5X57hiwx36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "if device == \"cuda\":\n",
        "    print(torch.cuda.get_device_name(0))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mwrsmOX39bI",
        "outputId": "486ef422-d2ae-4c48-b3d0-1258b4bd57af"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip -q install yfinance pandas numpy PyYAML pyarrow statsmodels tenacity"
      ],
      "metadata": {
        "id": "8t8ZL73mzgiv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 1.1 UNIVERSE (UPDATED)\n",
        "# S&P 500 training set with dynamic top-N selection by confidence (later in pipeline).\n",
        "# Hedging instruments: SPY + sector ETFs.\n",
        "# Source: Yahoo Finance (daily bars). Lookback from 2006-01-01 to today.\n",
        "# Saves: universe.csv and raw_prices.parquet (OHLCV + Adj Close for all tickers incl. hedges + ^VIX)\n",
        "# ============================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yfinance as yf\n",
        "from datetime import datetime\n",
        "\n",
        "START_DATE = \"2006-01-01\"\n",
        "END_DATE = datetime.today().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "def to_fmp_symbol(sym: str) -> str:\n",
        "    # map Yahoo/WSJ style class tickers to FMP\n",
        "    return sym.replace(\"-\", \".\") if \"-\" in sym else sym\n",
        "\n",
        "def is_index_like(sym: str) -> bool:\n",
        "    # skip ^VIX and other index-style series for FMP backfill\n",
        "    return sym.startswith(\"^\")\n",
        "\n",
        "# --- Get S&P 500 constituents from Wikipedia (survivorship bias acknowledged) ---\n",
        "sp500_url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
        "tables = pd.read_html(sp500_url)\n",
        "sp500 = tables[0]  # first table\n",
        "tickers_raw = sp500[\"Symbol\"].tolist()\n",
        "# Some tickers on Wikipedia have periods; yfinance uses dashes for certain cases\n",
        "tickers = [t.replace(\".\", \"-\") for t in tickers_raw]\n",
        "\n",
        "# --- Hedging instruments (market & sector ETFs) ---\n",
        "hedges = [\"SPY\", \"XLY\", \"XLF\", \"XLV\", \"XLK\", \"XLI\", \"XLE\", \"XLP\", \"XLB\", \"XLU\", \"XLRE\"]\n",
        "context_symbols = [\"^VIX\"]  # market context series\n",
        "\n",
        "universe = sorted(set(tickers))\n",
        "universe_all = sorted(set(universe + hedges + context_symbols))\n",
        "\n",
        "# --- Save universe to CSV ---\n",
        "pd.DataFrame({\"ticker\": universe}).to_csv(f\"universe_{END_DATE}.csv\", index=False)\n",
        "pd.DataFrame({\"ticker\": universe}).to_csv(\"universe.csv\", index=False)  # pointer\n",
        "\n",
        "\n",
        "# --- Download daily OHLCV for all symbols ---\n",
        "# yfinance handles adjusted prices; weâ€™ll keep both Close & Adj Close.\n",
        "data = yf.download(\n",
        "    universe_all,\n",
        "    start=START_DATE,\n",
        "    end=END_DATE,\n",
        "    auto_adjust=False,\n",
        "    group_by=\"ticker\",\n",
        "    progress=False,\n",
        "    threads=True,\n",
        ")\n",
        "\n",
        "if data is None or getattr(data, \"empty\", False):\n",
        "    raise RuntimeError(\"yfinance returned no data â€” try rerunning or chunking the request.\")\n",
        "\n",
        "def top_level_symbols(df):\n",
        "    # Handles both MultiIndex (normal multi-ticker) and flat columns (edge cases)\n",
        "    if isinstance(df.columns, pd.MultiIndex):\n",
        "        return set(df.columns.get_level_values(0))\n",
        "    # flat columns -> we can only have one symbol; yfinance puts OHLCV names as columns\n",
        "    return set()  # treat as empty to trigger backfill logic safely\n",
        "\n",
        "# added: tells us if yfinance skipped any tickers\n",
        "available = top_level_symbols(data)\n",
        "missing = [sym for sym in universe_all if sym not in available]\n",
        "if missing:\n",
        "    pd.Series(missing, name=\"missing_symbols\").to_csv(\"missing_symbols.csv\", index=False)\n",
        "    print(f\"WARNING: {len(missing)} symbols missing from download. Saved to missing_symbols.csv\")\n",
        "\n",
        "# Normalize to tidy format: MultiIndex -> long DataFrame\n",
        "frames = []\n",
        "if isinstance(data.columns, pd.MultiIndex):\n",
        "    for sym in universe_all:\n",
        "        if sym not in available:\n",
        "            continue\n",
        "        df = data[sym].copy()\n",
        "        df.columns = [c.lower().replace(\" \", \"_\") for c in df.columns]\n",
        "        df[\"ticker\"] = sym\n",
        "        frames.append(df.reset_index().rename(columns={\"Date\": \"date\"}))\n",
        "else:\n",
        "    # Edge: flat columns â€” shouldn't happen with many symbols, but keep it safe\n",
        "    df = data.copy()\n",
        "    df.columns = [c.lower().replace(\" \", \"_\") for c in df.columns]\n",
        "    df[\"ticker\"] = universe_all[0]\n",
        "    frames.append(df.reset_index().rename(columns={\"Date\": \"date\"}))\n",
        "\n",
        "prices = pd.concat(frames, ignore_index=True).sort_values([\"ticker\", \"date\"])\n",
        "prices[\"date\"] = pd.to_datetime(prices[\"date\"])\n",
        "\n",
        "# Basic sanity: drop rows with all NaNs for OHLCV\n",
        "keep_cols = [\"open\", \"high\", \"low\", \"close\", \"adj_close\", \"volume\"]\n",
        "prices = prices.dropna(subset=keep_cols, how=\"all\")\n",
        "\n",
        "# Save raw prices\n",
        "prices.to_parquet(\"raw_prices.parquet\", index=False)\n",
        "\n",
        "print(f\"Universe size (S&P 500): {len(universe)} tickers\")\n",
        "print(f\"Total symbols incl. hedges/context: {len(universe_all)}\")\n",
        "print(\"Saved: universe.csv, raw_prices.parquet\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7kItZpBzQyM",
        "outputId": "d1908575-710c-4722-93c0-5869f98c529e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Universe size (S&P 500): 502 tickers\n",
            "Total symbols incl. hedges/context: 514\n",
            "Saved: universe.csv, raw_prices.parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- Optional: Backfill any missing tickers with FMP (skip ^VIX etc.) ----\n",
        "import os, requests, time\n",
        "from getpass import getpass\n",
        "\n",
        "if os.path.exists(\"missing_symbols.csv\"):\n",
        "    missing = pd.read_csv(\"missing_symbols.csv\")[\"missing_symbols\"].tolist()\n",
        "else:\n",
        "    uni = pd.read_csv(\"universe.csv\")[\"ticker\"].tolist()\n",
        "    hedges = [\"SPY\",\"XLY\",\"XLF\",\"XLV\",\"XLK\",\"XLI\",\"XLE\",\"XLP\",\"XLB\",\"XLU\",\"XLRE\"]\n",
        "    context = [\"^VIX\"]\n",
        "    universe_all = sorted(set(uni + hedges + context))\n",
        "    base_prices = pd.read_parquet(\"raw_prices.parquet\")\n",
        "    present = set(base_prices[\"ticker\"].unique())\n",
        "    missing = [s for s in universe_all if s not in present]\n",
        "\n",
        "missing = [s for s in missing if not is_index_like(s)]\n",
        "if not missing:\n",
        "    print(\"No missing symbols to backfill.\")\n",
        "else:\n",
        "    print(f\"Backfilling {len(missing)} symbols from FMP (skipping indexes):\", missing[:8], \"...\")\n",
        "    FMP_API_KEY = os.environ.get(\"FMP_API_KEY\", \"\").strip() or getpass(\"Enter FMP API key for price backfill: \").strip()\n",
        "    if not FMP_API_KEY:\n",
        "        raise RuntimeError(\"FMP_API_KEY required for backfill.\")\n",
        "\n",
        "    base_url = \"https://financialmodelingprep.com/api/v3/historical-price-full\"\n",
        "    def fetch_fmp_prices(sym):\n",
        "        fmp_sym = to_fmp_symbol(sym)\n",
        "        url = f\"{base_url}/{fmp_sym}?from={START_DATE}&to={END_DATE}&serietype=line&apikey={FMP_API_KEY}\"\n",
        "        r = requests.get(url, timeout=30)\n",
        "        r.raise_for_status()\n",
        "        js = r.json()\n",
        "        hist = js.get(\"historical\", [])\n",
        "        if not hist:\n",
        "            return None\n",
        "        df = pd.DataFrame(hist)\n",
        "        df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
        "        # Map columns; fall back if adjClose missing\n",
        "        df = df.rename(columns={\"adjClose\":\"adj_close\"})\n",
        "        if \"adj_close\" not in df.columns:\n",
        "            df[\"adj_close\"] = df[\"close\"]\n",
        "        cols = [\"date\",\"open\",\"high\",\"low\",\"close\",\"adj_close\",\"volume\"]\n",
        "        for c in cols:\n",
        "            if c not in df.columns: df[c] = np.nan\n",
        "        df = df[cols]\n",
        "        df[\"ticker\"] = sym\n",
        "        return df.sort_values(\"date\")\n",
        "\n",
        "    filled = []\n",
        "    for i, sym in enumerate(missing, 1):\n",
        "        try:\n",
        "            df = fetch_fmp_prices(sym)\n",
        "            if df is not None and len(df):\n",
        "                filled.append(df)\n",
        "        except Exception:\n",
        "            pass\n",
        "        if i % 10 == 0:\n",
        "            time.sleep(0.5)  # be polite\n",
        "\n",
        "    if filled:\n",
        "        add = pd.concat(filled, ignore_index=True)\n",
        "        base_prices = pd.read_parquet(\"raw_prices.parquet\")\n",
        "        prices_fixed = pd.concat([base_prices, add], ignore_index=True).sort_values([\"ticker\",\"date\"])\n",
        "        prices_fixed.to_parquet(\"raw_prices.parquet\", index=False)\n",
        "        print(f\"Backfilled {add['ticker'].nunique()} symbols and re-saved raw_prices.parquet\")\n",
        "    else:\n",
        "        print(\"FMP backfill returned no data; proceeding without these tickers.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcFnrcEUU52H",
        "outputId": "3bb27109-0daf-40df-ab2a-e0d93c0c34c8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No missing symbols to backfill.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 1.2 FEATURES (FMP Premium, no hard-coded key)\n",
        "# ------------------------------------------------------------\n",
        "# Builds:\n",
        "#   â€¢ Price/technical features (returns/vol/ATR/momentum/trend)\n",
        "#   â€¢ Market context (SPY vol, ^VIX, breadth)\n",
        "#   â€¢ Fundamentals via FMP (quarterly BS/IS/CF), cached per ticker,\n",
        "#     forward-filled to daily, and ratio metrics (Value + Quality)\n",
        "# Post-merge:\n",
        "#   â€¢ Leakage control (shift all predictive features by 1 day)\n",
        "#   â€¢ Winsorize & cross-sectional z-score (by date)\n",
        "#   â€¢ Fundamentals imputation + missing masks\n",
        "# Saves:\n",
        "#   â€¢ features.parquet\n",
        "#   â€¢ funda_quarterly.parquet, funda_daily.parquet\n",
        "#   â€¢ cache/funda_q_<TICKER>.parquet (per-ticker cache)\n",
        "# Notes:\n",
        "#   - API key is taken from env var FMP_API_KEY or prompted securely.\n",
        "#   - GPU not used here (CPU/I/O heavy); thatâ€™s normal.\n",
        "# ============================================================\n",
        "\n",
        "# %pip -q install yfinance pyarrow tenacity\n",
        "\n",
        "import os, time, random, gc\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yfinance as yf\n",
        "from getpass import getpass\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n",
        "\n",
        "# ---------- Config / Toggles ----------\n",
        "COMPUTE_SLOPE = True      # slope_20 via vectorized method\n",
        "SLOPE_WINDOW = 20\n",
        "RV_WIN = 20\n",
        "ATR_WIN = 14\n",
        "\n",
        "# Fundamentals provider config (FMP Premium)\n",
        "PROVIDER = \"fmp\"          # fixed to FMP for reliability\n",
        "FMP_API_KEY = os.environ.get(\"FMP_API_KEY\", \"\").strip()\n",
        "if not FMP_API_KEY:\n",
        "    # Prompt securely; not echoed, not written to disk\n",
        "    FMP_API_KEY = getpass(\"Enter your FMP API key (kept in-memory for this session): \").strip()\n",
        "if not FMP_API_KEY:\n",
        "    raise RuntimeError(\"FMP_API_KEY is required. Set env var FMP_API_KEY or enter it when prompted.\")\n",
        "\n",
        "def to_fmp_symbol(sym: str) -> str:\n",
        "    \"\"\"\n",
        "    Convert Yahoo-style tickers to FMP-style.\n",
        "    Yahoo uses '-' for class/shared tickers (e.g., BRK-B),\n",
        "    while FMP uses '.' (e.g., BRK.B). Everything else stays the same.\n",
        "    \"\"\"\n",
        "    # common class/delimiter cases\n",
        "    # e.g., BRK-B, BF-B, FOXA (no change), META (no change)\n",
        "    if \"-\" in sym:\n",
        "        return sym.replace(\"-\", \".\")\n",
        "    return sym\n",
        "\n",
        "# Chunking: Premium can fetch all at once. If you ever need throttling, set CHUNK_TICKERS to an int.\n",
        "CHUNK_TICKERS = 20        # TOCHANGE: None = process entire universe in one go > change this later to None to check all stocks instead of just 100\n",
        "START_AT = 0              # offset if chunking\n",
        "SKIP_IF_CACHED = True     # skip ticker if cache exists\n",
        "\n",
        "MAX_WORKERS = 4           # Premium can handle more concurrency; tune 4â€“12 as you like\n",
        "RETRY_ATTEMPTS = 5\n",
        "BATCH_SLEEP = (0.2, 0.6)  # polite jitter between HTTP calls\n",
        "CACHE_DIR = \"cache\"\n",
        "os.makedirs(CACHE_DIR, exist_ok=True)\n",
        "\n",
        "# ---------- Load raw prices & universe (from 1.1) ----------\n",
        "prices = pd.read_parquet(\"raw_prices.parquet\")\n",
        "universe_full = list(pd.read_csv(\"universe.csv\")[\"ticker\"])\n",
        "hedges = {\"SPY\", \"XLY\", \"XLF\", \"XLV\", \"XLK\", \"XLI\", \"XLE\", \"XLP\", \"XLB\", \"XLU\", \"XLRE\"}\n",
        "context_symbols = {\"^VIX\"}\n",
        "\n",
        "# ============================================================\n",
        "# A) PRICE / TECHNICAL FEATURES\n",
        "# ============================================================\n",
        "\n",
        "def compute_atr(df, window=ATR_WIN):\n",
        "    high, low, close = df[\"high\"], df[\"low\"], df[\"close\"]\n",
        "    prev_close = close.shift(1)\n",
        "    tr = pd.concat([(high - low),\n",
        "                    (high - prev_close).abs(),\n",
        "                    (low - prev_close).abs()], axis=1).max(axis=1)\n",
        "    return tr.rolling(window).mean()\n",
        "\n",
        "def vectorized_rolling_slope(y: pd.Series, window=SLOPE_WINDOW) -> pd.Series:\n",
        "    N = window\n",
        "    if N <= 1:\n",
        "        return pd.Series(np.nan, index=y.index, dtype=float)\n",
        "    x = np.arange(N, dtype=float)\n",
        "    Sx = x.sum()\n",
        "    Sxx = (x**2).sum()\n",
        "    yv = y.to_numpy(dtype=float)\n",
        "    yv = np.where(np.isfinite(yv), yv, 0.0)\n",
        "    k = np.ones(N, dtype=float)\n",
        "    Sy  = np.convolve(yv, k[::-1], mode=\"full\")[N-1:len(yv)+N-1]\n",
        "    Sxy = np.convolve(yv, x[::-1], mode=\"full\")[N-1:len(yv)+N-1]\n",
        "    denom = N * Sxx - Sx * Sx + 1e-12\n",
        "    slope = (N * Sxy - Sx * Sy) / denom\n",
        "    out = pd.Series(np.nan, index=y.index, dtype=float)\n",
        "    out.iloc[N-1:] = slope[N-1:]\n",
        "    return out\n",
        "\n",
        "def mom_over_n(adj_close, n):\n",
        "    return np.log(adj_close / adj_close.shift(n))\n",
        "\n",
        "feat_frames = []\n",
        "tickers = sorted(prices[\"ticker\"].unique())\n",
        "total = len(tickers)\n",
        "\n",
        "for i, (sym, df_sym) in enumerate(prices.groupby(\"ticker\"), start=1):\n",
        "    if sym in context_symbols:\n",
        "        continue\n",
        "    if i % 25 == 0:\n",
        "        print(f\"[Features] {i}/{total} processedâ€¦ ({sym})\")\n",
        "\n",
        "    df = df_sym.sort_values(\"date\").copy()\n",
        "    df[\"ret_1d\"] = np.log(df[\"adj_close\"] / df[\"adj_close\"].shift(1))\n",
        "    for l in range(1, 61):\n",
        "        df[f\"ret_lag_{l}\"] = df[\"ret_1d\"].shift(l)\n",
        "\n",
        "    df[\"rv_20\"] = df[\"ret_1d\"].rolling(RV_WIN).std() * np.sqrt(252)\n",
        "    df[\"atr_14\"] = compute_atr(df, ATR_WIN)\n",
        "\n",
        "    df[\"mom_20\"]  = mom_over_n(df[\"adj_close\"], 20)\n",
        "    df[\"mom_6m\"]  = mom_over_n(df[\"adj_close\"], 126)\n",
        "    df[\"mom_12m\"] = mom_over_n(df[\"adj_close\"], 252)\n",
        "    df[\"mom_12_1\"] = np.log(df[\"adj_close\"].shift(21) / df[\"adj_close\"].shift(252))\n",
        "    df[\"mom_6_1\"]  = np.log(df[\"adj_close\"].shift(21) / df[\"adj_close\"].shift(126))\n",
        "\n",
        "    df[\"sma_20\"] = df[\"adj_close\"].rolling(20).mean()\n",
        "    df[\"sma_50\"] = df[\"adj_close\"].rolling(50).mean()\n",
        "    df[\"sma_20_gt_50\"] = (df[\"sma_20\"] > df[\"sma_50\"]).astype(\"float32\")\n",
        "    df[\"slope_20\"] = vectorized_rolling_slope(df[\"adj_close\"], window=SLOPE_WINDOW) if COMPUTE_SLOPE else np.nan\n",
        "\n",
        "    df[\"mom_20_vs_vol\"] = df[\"mom_20\"] / (df[\"ret_1d\"].rolling(20).std() + 1e-8)\n",
        "\n",
        "    feat_frames.append(df)\n",
        "\n",
        "features = pd.concat(feat_frames, ignore_index=True)\n",
        "\n",
        "# ============================================================\n",
        "# B) MARKET CONTEXT (SPY vol, VIX, breadth)\n",
        "# ============================================================\n",
        "\n",
        "vix = prices[prices[\"ticker\"] == \"^VIX\"][[\"date\", \"adj_close\"]].rename(columns={\"adj_close\": \"vix_close\"})\n",
        "spy = prices[prices[\"ticker\"] == \"SPY\"].copy()\n",
        "spy[\"spy_ret\"] = np.log(spy[\"adj_close\"] / spy[\"adj_close\"].shift(1))\n",
        "spy[\"spy_rv_20\"] = spy[\"spy_ret\"].rolling(20).std() * np.sqrt(252)\n",
        "ctx = spy[[\"date\", \"spy_rv_20\"]].merge(vix, on=\"date\", how=\"left\")\n",
        "\n",
        "rets = features.pivot(index=\"date\", columns=\"ticker\", values=\"ret_1d\")\n",
        "advancers = (rets > 0).sum(axis=1)\n",
        "# Fixed denominator for stability = full S&P 500 count from universe.csv\n",
        "breadth = (advancers / len(universe_full)).rename(\"breadth\")\n",
        "ctx = ctx.merge(breadth.reset_index(), on=\"date\", how=\"left\")\n",
        "\n",
        "features = features.merge(ctx, on=\"date\", how=\"left\")\n",
        "\n",
        "# ============================================================\n",
        "# C) FUNDAMENTALS (FMP Premium primary; cached per ticker)\n",
        "# ============================================================\n",
        "\n",
        "px_daily_all = prices[prices[\"ticker\"].isin(universe_full)][[\"date\", \"ticker\", \"adj_close\"]].copy()\n",
        "px_daily_all[\"date\"] = pd.to_datetime(px_daily_all[\"date\"])\n",
        "dates_all = px_daily_all[[\"date\"]].drop_duplicates().sort_values(\"date\")\n",
        "\n",
        "def _tidy_quarterly_df(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    if \"date\" in df.columns:\n",
        "        df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
        "    elif \"fillingDate\" in df.columns:\n",
        "        df[\"date\"] = pd.to_datetime(df[\"fillingDate\"])\n",
        "    return df\n",
        "\n",
        "def _coalesce_cols(df: pd.DataFrame, cols: list[str], default=np.nan) -> pd.Series:\n",
        "    avail = [c for c in cols if c in df.columns]\n",
        "    if not avail:\n",
        "        return pd.Series(default, index=df.index)\n",
        "    tmp = df[avail].apply(pd.to_numeric, errors=\"coerce\")\n",
        "    # first non-null across the candidate columns\n",
        "    s = tmp.bfill(axis=1).iloc[:, 0]\n",
        "    return s\n",
        "\n",
        "def _fetch_quarterly_funda_fmp(ticker: str) -> pd.DataFrame:\n",
        "    import requests\n",
        "    base = \"https://financialmodelingprep.com/api/v3\"\n",
        "    fmp_ticker = to_fmp_symbol(ticker)   # BRK-B -> BRK.B\n",
        "\n",
        "    def jget(url):\n",
        "        r = requests.get(url, timeout=30)\n",
        "        r.raise_for_status()\n",
        "        return r.json()\n",
        "\n",
        "    # pull a long history (Premium supports it)\n",
        "    bs = _tidy_quarterly_df(pd.DataFrame(jget(f\"{base}/balance-sheet-statement/{fmp_ticker}?period=quarter&limit=120&apikey={FMP_API_KEY}\")))\n",
        "    is_ = _tidy_quarterly_df(pd.DataFrame(jget(f\"{base}/income-statement/{fmp_ticker}?period=quarter&limit=120&apikey={FMP_API_KEY}\")))\n",
        "    cf = _tidy_quarterly_df(pd.DataFrame(jget(f\"{base}/cash-flow-statement/{fmp_ticker}?period=quarter&limit=120&apikey={FMP_API_KEY}\")))\n",
        "    if bs.empty and is_.empty and cf.empty:\n",
        "        raise RuntimeError(f\"FMP fundamentals empty for {ticker} (queried as {fmp_ticker})\")\n",
        "\n",
        "    out = bs.merge(is_, on=\"date\", how=\"outer\").merge(cf, on=\"date\", how=\"outer\")\n",
        "\n",
        "    # Coalesce across schema variants\n",
        "    out[\"book_equity\"]  = _coalesce_cols(out, [\"totalStockholdersEquity\",\"totalShareholderEquity\",\"totalEquity\"]).astype(float)\n",
        "    out[\"net_income\"]   = _coalesce_cols(out, [\"netIncome\",\"netIncomeApplicableToCommonShares\"]).astype(float)\n",
        "    out[\"ocf\"]          = _coalesce_cols(out, [\n",
        "        \"netCashProvidedByOperatingActivities\",\n",
        "        \"netCashProvidedByUsedInOperatingActivities\",\n",
        "        \"netCashProvidedByUsedInOperatingActivitiesContinuingOperations\"\n",
        "    ]).astype(float)\n",
        "    out[\"gross_profit\"] = _coalesce_cols(out, [\"grossProfit\"]).astype(float)\n",
        "    out[\"total_assets\"] = _coalesce_cols(out, [\"totalAssets\"]).astype(float)\n",
        "\n",
        "    # total_debt: prefer totalDebt; else short + long\n",
        "    td = _coalesce_cols(out, [\"totalDebt\"])\n",
        "    if td.isna().all():\n",
        "        short = _coalesce_cols(out, [\"shortTermDebt\",\"shortLongTermDebtTotal\"])\n",
        "        long  = _coalesce_cols(out, [\"longTermDebt\"])\n",
        "        td = (short.fillna(0) + long.fillna(0)).replace({0: np.nan})\n",
        "    out[\"total_debt\"] = td.astype(float)\n",
        "\n",
        "    # dividends / buybacks (raw signs as provided by FMP)\n",
        "    out[\"dividends\"] = _coalesce_cols(out, [\"dividendsPaid\",\"dividendsPaidCashFlow\"]).astype(float)\n",
        "    out[\"buybacks\"]  = _coalesce_cols(out, [\"commonStockRepurchased\",\"purchaseOfCommonStock\"]).astype(float)\n",
        "\n",
        "    out[\"ticker\"] = ticker  # keep Yahoo-style symbol for our dataset\n",
        "\n",
        "    cols = [\"date\",\"ticker\",\"book_equity\",\"net_income\",\"ocf\",\"gross_profit\",\n",
        "            \"total_assets\",\"total_debt\",\"dividends\",\"buybacks\"]\n",
        "    return out[cols].dropna(subset=[\"date\"])\n",
        "\n",
        "\n",
        "def fetch_or_load_cached_quarterly(ticker: str) -> pd.DataFrame | None:\n",
        "    path = os.path.join(CACHE_DIR, f\"funda_q_{ticker}.parquet\")\n",
        "    if SKIP_IF_CACHED and os.path.exists(path):\n",
        "        try:\n",
        "            return pd.read_parquet(path)\n",
        "        except Exception:\n",
        "            pass\n",
        "    try:\n",
        "        df = _fetch_quarterly_funda_fmp(ticker)\n",
        "        if df is None or df.empty:\n",
        "            return None\n",
        "        df.to_parquet(path, index=False)\n",
        "        time.sleep(random.uniform(*BATCH_SLEEP))  # polite pause\n",
        "        return df\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "# ---- SMOKE TEST (run once, then you can comment it out) ----\n",
        "try:\n",
        "    from IPython.display import display\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "test_syms = [\"AAPL\", \"MSFT\", \"BRK-B\", \"BF-B\"]\n",
        "for s in test_syms:\n",
        "    try:\n",
        "        df = _fetch_quarterly_funda_fmp(s)\n",
        "        # ðŸ‘‡ trim preview to match your price history\n",
        "        df = df[df[\"date\"] >= pd.to_datetime(START_DATE)]\n",
        "        print(s, \"â†’\", to_fmp_symbol(s), \"rows:\", len(df))\n",
        "        try:\n",
        "            display(df.head(2))\n",
        "        except Exception:\n",
        "            print(df.head(2))\n",
        "    except Exception as e:\n",
        "        print(\"ERR\", s, e)\n",
        "\n",
        "# Determine chunk (or all)\n",
        "if CHUNK_TICKERS:\n",
        "    end_at = min(len(universe_full), START_AT + CHUNK_TICKERS)\n",
        "    tickers_chunk = sorted(universe_full[START_AT:end_at])\n",
        "    print(f\"[FMP] Processing chunk {START_AT}:{end_at} (size={len(tickers_chunk)})\")\n",
        "else:\n",
        "    tickers_chunk = sorted(universe_full)\n",
        "    print(f\"[FMP] Processing entire universe (size={len(tickers_chunk)})\")\n",
        "\n",
        "# Parallel fetch with caching\n",
        "funda_parts, successes = [], 0\n",
        "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
        "    futs = {ex.submit(fetch_or_load_cached_quarterly, t): t for t in tickers_chunk}\n",
        "    for j, fut in enumerate(as_completed(futs), start=1):\n",
        "        df = fut.result()\n",
        "        if df is not None and len(df):\n",
        "            funda_parts.append(df)\n",
        "            successes += 1\n",
        "        if j % 25 == 0:\n",
        "            print(f\"[Fundamentals/FMP] {j}/{len(tickers_chunk)} processedâ€¦ (successes: {successes})\")\n",
        "\n",
        "if successes == 0:\n",
        "    raise RuntimeError(\"No fundamentals fetched. Check your FMP key or try a smaller CHUNK_TICKERS with fewer workers.\")\n",
        "\n",
        "# Merge chunk with existing quarterly file (so multiple runs accumulate)\n",
        "q_path = \"funda_quarterly.parquet\"\n",
        "funda_q_chunk = pd.concat(funda_parts, ignore_index=True).sort_values([\"ticker\",\"date\"])\n",
        "\n",
        "# after you build funda_q_chunk\n",
        "cutoff = pd.to_datetime(px_daily_all[\"date\"].min())\n",
        "funda_q_chunk = funda_q_chunk[funda_q_chunk[\"date\"] >= cutoff]\n",
        "\n",
        "if os.path.exists(q_path):\n",
        "    old = pd.read_parquet(q_path)\n",
        "    old[\"date\"] = pd.to_datetime(old[\"date\"])\n",
        "    old = old[old[\"date\"] >= cutoff]  # <- trim the old file too\n",
        "    funda_q = (\n",
        "        pd.concat([old, funda_q_chunk], ignore_index=True)\n",
        "          .drop_duplicates([\"ticker\",\"date\"], keep=\"last\")\n",
        "          .sort_values([\"ticker\",\"date\"])\n",
        "    )\n",
        "else:\n",
        "    funda_q = funda_q_chunk\n",
        "\n",
        "funda_q = funda_q.sort_values([\"ticker\",\"date\"])\n",
        "funda_q.to_parquet(q_path, index=False)\n",
        "print(\n",
        "    f\"Saved: {q_path}  \"\n",
        "    f\"(tickers with funda total: {funda_q['ticker'].nunique()}, \"\n",
        "    f\"rows: {len(funda_q)})\"\n",
        ")\n",
        "\n",
        "# Quarterly â†’ daily (forward-fill per ticker) across ALL tickers collected so far\n",
        "ff = []\n",
        "for sym, grp in funda_q.groupby(\"ticker\"):\n",
        "    g = dates_all.merge(grp, on=\"date\", how=\"left\")\n",
        "    g[\"ticker\"] = sym\n",
        "    g = g.sort_values(\"date\").ffill()\n",
        "    ff.append(g)\n",
        "funda_daily = pd.concat(ff, ignore_index=True)\n",
        "\n",
        "# Ratios (Value & Quality)\n",
        "fd = funda_daily.merge(px_daily_all, on=[\"date\",\"ticker\"], how=\"left\")\n",
        "price = fd[\"adj_close\"].replace(0, np.nan)\n",
        "\n",
        "fd[\"book_to_price\"]     = fd[\"book_equity\"] / price\n",
        "fd[\"earnings_yield\"]    = fd[\"net_income\"]  / price\n",
        "fd[\"cf_yield\"]          = fd[\"ocf\"]         / price\n",
        "fd[\"shareholder_yield\"] = (fd[\"dividends\"].fillna(0) * -1 + fd[\"buybacks\"].fillna(0)) / price\n",
        "\n",
        "fd[\"gross_profitability\"] = fd[\"gross_profit\"] / fd[\"total_assets\"].replace(0, np.nan)\n",
        "fd[\"roe\"]                 = fd[\"net_income\"] / fd[\"book_equity\"].replace(0, np.nan)\n",
        "fd[\"accruals\"]            = (fd[\"net_income\"] - fd[\"ocf\"]) / fd[\"total_assets\"].replace(0, np.nan)\n",
        "fd[\"leverage\"]            = fd[\"total_debt\"] / fd[\"total_assets\"].replace(0, np.nan)\n",
        "\n",
        "funda_daily = fd[[\n",
        "    \"date\",\"ticker\",\"book_to_price\",\"earnings_yield\",\"cf_yield\",\"shareholder_yield\",\n",
        "    \"gross_profitability\",\"roe\",\"accruals\",\"leverage\"\n",
        "]]\n",
        "funda_daily.to_parquet(\"funda_daily.parquet\", index=False)\n",
        "print(\"Saved: funda_daily.parquet\")\n",
        "\n",
        "# Merge fundamentals into features\n",
        "features = features.merge(funda_daily, on=[\"date\",\"ticker\"], how=\"left\")\n",
        "\n",
        "# ============================================================\n",
        "# D) POST-MERGE HYGIENE\n",
        "# ============================================================\n",
        "\n",
        "# 1) Leakage control\n",
        "non_feature_cols = {\"date\",\"ticker\",\"open\",\"high\",\"low\",\"close\",\"adj_close\",\"volume\"}\n",
        "cols_to_shift = [c for c in features.columns if c not in non_feature_cols]\n",
        "features[cols_to_shift] = features.groupby(\"ticker\")[cols_to_shift].shift(1)\n",
        "\n",
        "# 2) Winsorize & cross-sectional z-score\n",
        "def winsorize_cs(s, lo=0.01, hi=0.99):\n",
        "    ql, qh = s.quantile(lo), s.quantile(hi)\n",
        "    return s.clip(ql, qh)\n",
        "\n",
        "# --- choose features for cross-sectional standardization (exclude context & raw SMAs) ---\n",
        "cs_cols = [\n",
        "    \"rv_20\",\"atr_14\",\"mom_20\",\"mom_6m\",\"mom_12m\",\"mom_12_1\",\"mom_6_1\",\n",
        "    \"sma_20_gt_50\",\"slope_20\",\"mom_20_vs_vol\",\n",
        "    # fundamentals\n",
        "    \"book_to_price\",\"earnings_yield\",\"cf_yield\",\"shareholder_yield\",\n",
        "    \"gross_profitability\",\"roe\",\"accruals\",\"leverage\"\n",
        "] + [f\"ret_lag_{l}\" for l in range(1,61)]\n",
        "\n",
        "# keep context raw (no CS z-score)\n",
        "context_keep_raw = [\"spy_rv_20\",\"vix_close\",\"breadth\"]\n",
        "\n",
        "present = [c for c in cs_cols if c in features.columns]\n",
        "\n",
        "print(f\"[Standardize] Cross-sectional z-score on {len(present)} features\")\n",
        "\n",
        "def cs_standardize_fast(df, cols, lo=0.01, hi=0.99):\n",
        "    out = df.copy()\n",
        "    out[cols] = out[cols].astype(\"float32\")\n",
        "\n",
        "    d = out[\"date\"]\n",
        "    for c in cols:\n",
        "        s = out[c]\n",
        "\n",
        "        ql = s.groupby(d).transform(lambda x: x.quantile(lo))\n",
        "        qh = s.groupby(d).transform(lambda x: x.quantile(hi))\n",
        "        s_clip = s.clip(ql, qh)\n",
        "\n",
        "        mu = s_clip.groupby(d).transform(\"mean\")\n",
        "        sd = s_clip.groupby(d).transform(\"std\")\n",
        "\n",
        "        # if std is 0 or NaN (date-constant or all-NaN), set denom=1 to avoid blowing up / NaNs\n",
        "        denom = sd.fillna(0.0).replace(0.0, 1.0)\n",
        "\n",
        "        out[c] = ((s_clip - mu) / (denom + 1e-9)).astype(\"float32\")\n",
        "\n",
        "    return out\n",
        "\n",
        "features = cs_standardize_fast(features, present)\n",
        "\n",
        "# 3) Fundamentals imputation + masks\n",
        "funda_cols = [\"book_to_price\",\"earnings_yield\",\"cf_yield\",\"shareholder_yield\",\n",
        "              \"gross_profitability\",\"roe\",\"accruals\",\"leverage\"]\n",
        "for c in funda_cols:\n",
        "    if c in features.columns:\n",
        "        features[f\"{c}_is_missing\"] = features[c].isna().astype(int)\n",
        "        features[c] = features.groupby(\"date\")[c].transform(lambda s: s.fillna(s.median()))\n",
        "\n",
        "# Save final\n",
        "features.to_parquet(\"features.parquet\", index=False)\n",
        "print(\"Saved: features.parquet (lagged, winsorized, cross-sectional z-scored)\")\n",
        "print(\"Artifacts: funda_quarterly.parquet, funda_daily.parquet, cache/funda_q_*.parquet\")\n",
        "\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 998
        },
        "id": "5qa3M9pAzvtv",
        "outputId": "1046112b-d118-4aac-fa8d-35b277e63ae1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your FMP API key (kept in-memory for this session): Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "[Features] 25/514 processedâ€¦ (AMAT)\n",
            "[Features] 50/514 processedâ€¦ (BA)\n",
            "[Features] 75/514 processedâ€¦ (CARR)\n",
            "[Features] 100/514 processedâ€¦ (CNP)\n",
            "[Features] 125/514 processedâ€¦ (DAL)\n",
            "[Features] 150/514 processedâ€¦ (EA)\n",
            "[Features] 175/514 processedâ€¦ (EXE)\n",
            "[Features] 200/514 processedâ€¦ (GEHC)\n",
            "[Features] 225/514 processedâ€¦ (HON)\n",
            "[Features] 250/514 processedâ€¦ (IT)\n",
            "[Features] 275/514 processedâ€¦ (LDOS)\n",
            "[Features] 300/514 processedâ€¦ (MCO)\n",
            "[Features] 325/514 processedâ€¦ (MTCH)\n",
            "[Features] 350/514 processedâ€¦ (OKE)\n",
            "[Features] 375/514 processedâ€¦ (PNR)\n",
            "[Features] 400/514 processedâ€¦ (RTX)\n",
            "[Features] 425/514 processedâ€¦ (SYF)\n",
            "[Features] 450/514 processedâ€¦ (TT)\n",
            "[Features] 475/514 processedâ€¦ (VTR)\n",
            "[Features] 500/514 processedâ€¦ (XLI)\n",
            "AAPL â†’ AAPL rows: 78\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "         date ticker   book_equity  net_income           ocf  gross_profit  \\\n",
              "42 2006-04-01   AAPL  8.682000e+09         NaN -1.250000e+08  1.297000e+09   \n",
              "43 2006-07-01   AAPL  9.330000e+09         NaN  1.007000e+09  1.325000e+09   \n",
              "\n",
              "    total_assets  total_debt  dividends   buybacks  \n",
              "42  1.391100e+10         0.0        0.0        0.0  \n",
              "43  1.511400e+10         0.0        0.0 -1000000.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5c6e66d1-549c-412c-a125-21078950e648\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>ticker</th>\n",
              "      <th>book_equity</th>\n",
              "      <th>net_income</th>\n",
              "      <th>ocf</th>\n",
              "      <th>gross_profit</th>\n",
              "      <th>total_assets</th>\n",
              "      <th>total_debt</th>\n",
              "      <th>dividends</th>\n",
              "      <th>buybacks</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>2006-04-01</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>8.682000e+09</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-1.250000e+08</td>\n",
              "      <td>1.297000e+09</td>\n",
              "      <td>1.391100e+10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>2006-07-01</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>9.330000e+09</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.007000e+09</td>\n",
              "      <td>1.325000e+09</td>\n",
              "      <td>1.511400e+10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1000000.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5c6e66d1-549c-412c-a125-21078950e648')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5c6e66d1-549c-412c-a125-21078950e648 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5c6e66d1-549c-412c-a125-21078950e648');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-75e1ca4b-8266-4c76-b957-165496a3bb7a\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-75e1ca4b-8266-4c76-b957-165496a3bb7a')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-75e1ca4b-8266-4c76-b957-165496a3bb7a button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"gc\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"date\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2006-04-01 00:00:00\",\n        \"max\": \"2006-07-01 00:00:00\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"2006-07-01 00:00:00\",\n          \"2006-04-01 00:00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ticker\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"AAPL\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"book_equity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 458205194.2088828,\n        \"min\": 8682000000.0,\n        \"max\": 9330000000.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          9330000000.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"net_income\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ocf\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 800444876.3031718,\n        \"min\": -125000000.0,\n        \"max\": 1007000000.0,\n        \"num_unique_values\": 2,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"gross_profit\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 19798989.87322333,\n        \"min\": 1297000000.0,\n        \"max\": 1325000000.0,\n        \"num_unique_values\": 2,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"total_assets\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 850649457.7674167,\n        \"min\": 13911000000.0,\n        \"max\": 15114000000.0,\n        \"num_unique_values\": 2,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"total_debt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dividends\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"buybacks\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 707106.7811865475,\n        \"min\": -1000000.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 2,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSFT â†’ MSFT rows: 78\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "         date ticker   book_equity  net_income           ocf  gross_profit  \\\n",
              "42 2006-03-31   MSFT  4.203800e+10         NaN  4.563000e+09  8.872000e+09   \n",
              "43 2006-06-30   MSFT  4.010400e+10         NaN  3.281000e+09  9.674000e+09   \n",
              "\n",
              "    total_assets  total_debt    dividends      buybacks  \n",
              "42  6.685400e+10         0.0 -925000000.0 -4.675000e+09  \n",
              "43  6.959700e+10         0.0 -917000000.0 -3.981000e+09  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5b1cd709-dfe7-49a5-8910-50fb321d8d40\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>ticker</th>\n",
              "      <th>book_equity</th>\n",
              "      <th>net_income</th>\n",
              "      <th>ocf</th>\n",
              "      <th>gross_profit</th>\n",
              "      <th>total_assets</th>\n",
              "      <th>total_debt</th>\n",
              "      <th>dividends</th>\n",
              "      <th>buybacks</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>2006-03-31</td>\n",
              "      <td>MSFT</td>\n",
              "      <td>4.203800e+10</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.563000e+09</td>\n",
              "      <td>8.872000e+09</td>\n",
              "      <td>6.685400e+10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-925000000.0</td>\n",
              "      <td>-4.675000e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>2006-06-30</td>\n",
              "      <td>MSFT</td>\n",
              "      <td>4.010400e+10</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.281000e+09</td>\n",
              "      <td>9.674000e+09</td>\n",
              "      <td>6.959700e+10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-917000000.0</td>\n",
              "      <td>-3.981000e+09</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5b1cd709-dfe7-49a5-8910-50fb321d8d40')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5b1cd709-dfe7-49a5-8910-50fb321d8d40 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5b1cd709-dfe7-49a5-8910-50fb321d8d40');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-d0bb13da-e6d9-4678-8fb7-e9eeb4fff49a\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d0bb13da-e6d9-4678-8fb7-e9eeb4fff49a')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-d0bb13da-e6d9-4678-8fb7-e9eeb4fff49a button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"gc\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"date\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2006-03-31 00:00:00\",\n        \"max\": \"2006-06-30 00:00:00\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"2006-06-30 00:00:00\",\n          \"2006-03-31 00:00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ticker\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"MSFT\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"book_equity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1367544514.8147829,\n        \"min\": 40104000000.0,\n        \"max\": 42038000000.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          40104000000.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"net_income\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ocf\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 906510893.481154,\n        \"min\": 3281000000.0,\n        \"max\": 4563000000.0,\n        \"num_unique_values\": 2,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"gross_profit\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 567099638.5116111,\n        \"min\": 8872000000.0,\n        \"max\": 9674000000.0,\n        \"num_unique_values\": 2,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"total_assets\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1939593900.7947,\n        \"min\": 66854000000.0,\n        \"max\": 69597000000.0,\n        \"num_unique_values\": 2,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"total_debt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dividends\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5656854.24949238,\n        \"min\": -925000000.0,\n        \"max\": -917000000.0,\n        \"num_unique_values\": 2,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"buybacks\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 490732106.14346397,\n        \"min\": -4675000000.0,\n        \"max\": -3981000000.0,\n        \"num_unique_values\": 2,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BRK-B â†’ BRK.B rows: 78\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "         date ticker   book_equity  net_income           ocf  gross_profit  \\\n",
              "42 2006-03-31  BRK-B  9.534900e+10         NaN  2.359000e+09  6.162000e+09   \n",
              "43 2006-06-30  BRK-B  9.761300e+10         NaN  1.092000e+09  8.197000e+09   \n",
              "\n",
              "    total_assets    total_debt  dividends  buybacks  \n",
              "42  2.302060e+11  3.047900e+10        0.0       0.0  \n",
              "43  2.323310e+11  3.055700e+10        0.0       0.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3a205df3-fb9e-4b22-b92f-8c81015ffdd8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>ticker</th>\n",
              "      <th>book_equity</th>\n",
              "      <th>net_income</th>\n",
              "      <th>ocf</th>\n",
              "      <th>gross_profit</th>\n",
              "      <th>total_assets</th>\n",
              "      <th>total_debt</th>\n",
              "      <th>dividends</th>\n",
              "      <th>buybacks</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>2006-03-31</td>\n",
              "      <td>BRK-B</td>\n",
              "      <td>9.534900e+10</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.359000e+09</td>\n",
              "      <td>6.162000e+09</td>\n",
              "      <td>2.302060e+11</td>\n",
              "      <td>3.047900e+10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>2006-06-30</td>\n",
              "      <td>BRK-B</td>\n",
              "      <td>9.761300e+10</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.092000e+09</td>\n",
              "      <td>8.197000e+09</td>\n",
              "      <td>2.323310e+11</td>\n",
              "      <td>3.055700e+10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3a205df3-fb9e-4b22-b92f-8c81015ffdd8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3a205df3-fb9e-4b22-b92f-8c81015ffdd8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3a205df3-fb9e-4b22-b92f-8c81015ffdd8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-042456c7-05cb-40be-ad73-6d7282613052\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-042456c7-05cb-40be-ad73-6d7282613052')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-042456c7-05cb-40be-ad73-6d7282613052 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"gc\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"date\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2006-03-31 00:00:00\",\n        \"max\": \"2006-06-30 00:00:00\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"2006-06-30 00:00:00\",\n          \"2006-03-31 00:00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ticker\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"BRK-B\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"book_equity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1600889752.6063435,\n        \"min\": 95349000000.0,\n        \"max\": 97613000000.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          97613000000.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"net_income\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ocf\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 895904291.7633557,\n        \"min\": 1092000000.0,\n        \"max\": 2359000000.0,\n        \"num_unique_values\": 2,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"gross_profit\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1438962299.7146242,\n        \"min\": 6162000000.0,\n        \"max\": 8197000000.0,\n        \"num_unique_values\": 2,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"total_assets\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1502601910.0214136,\n        \"min\": 230206000000.0,\n        \"max\": 232331000000.0,\n        \"num_unique_values\": 2,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"total_debt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 55154328.932550706,\n        \"min\": 30479000000.0,\n        \"max\": 30557000000.0,\n        \"num_unique_values\": 2,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dividends\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"buybacks\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERR BF-B FMP fundamentals empty for BF-B (queried as BF.B)\n",
            "[FMP] Processing chunk 0:20 (size=20)\n",
            "Saved: funda_quarterly.parquet  (tickers with funda total: 20, rows: 1486)\n",
            "Saved: funda_daily.parquet\n",
            "[Standardize] Cross-sectional z-score on 78 features\n",
            "Saved: features.parquet (lagged, winsorized, cross-sectional z-scored)\n",
            "Artifacts: funda_quarterly.parquet, funda_daily.parquet, cache/funda_q_*.parquet\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 1.3 DATA HYGIENE / QC (non-destructive)\n",
        "# ------------------------------------------------------------\n",
        "# - Summarize coverage & missingness (post 1.2)\n",
        "# - Optional pruning: drop early warmup dates & low-coverage dates\n",
        "# - Write meta.yaml and QC CSVs\n",
        "# ============================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yaml\n",
        "\n",
        "FEATURES_PATH = \"features.parquet\"\n",
        "UNIVERSE_PATH = \"universe.csv\"\n",
        "\n",
        "features = pd.read_parquet(FEATURES_PATH)\n",
        "universe_df = pd.read_csv(UNIVERSE_PATH)\n",
        "\n",
        "# ---------- QC: basics ----------\n",
        "min_date = pd.to_datetime(features[\"date\"]).min()\n",
        "max_date = pd.to_datetime(features[\"date\"]).max()\n",
        "n_rows = len(features)\n",
        "n_tickers = features[\"ticker\"].nunique()\n",
        "\n",
        "# Columns we standardized in 1.2 (will exist if 1.2 ran)\n",
        "feature_cols = [c for c in features.columns\n",
        "                if c not in {\"date\",\"ticker\",\"open\",\"high\",\"low\",\"close\",\"adj_close\",\"volume\"}]\n",
        "\n",
        "# Per-column missingness (after 1.2; should be low except earliest windows)\n",
        "missing_pct = (1.0 - features[feature_cols].notna().mean()).sort_values(ascending=False)\n",
        "missing_pct.to_csv(\"qc_missing_by_feature.csv\", header=[\"missing_pct\"])\n",
        "\n",
        "# Coverage by date (# of tickers with at least 1 valid feature on that date)\n",
        "valid_any = features[feature_cols].notna().sum(axis=1) > 0\n",
        "coverage_by_date = (features.assign(valid_any=valid_any)\n",
        "                             .groupby(\"date\")[\"ticker\"]\n",
        "                             .nunique()\n",
        "                             .rename(\"n_tickers\"))\n",
        "coverage_by_date.to_csv(\"qc_coverage_by_date.csv\")\n",
        "\n",
        "# ---------- Optional: pruning rules (non-destructive by default) ----------\n",
        "# 1) Warmup: many features need long windows (max â‰ˆ 252 + 21). Keep dates after first 273 trading days.\n",
        "#    We'll infer a warmup cutoff from SPY availability to be robust.\n",
        "spy_dates = features.loc[features[\"ticker\"]==\"SPY\", \"date\"].sort_values().unique()\n",
        "if len(spy_dates) > 300:\n",
        "    warmup_cutoff = pd.to_datetime(spy_dates[min(273, len(spy_dates)-1)])\n",
        "else:\n",
        "    warmup_cutoff = min_date  # fallback\n",
        "\n",
        "# 2) Low coverage: drop dates with very few names (e.g., <300) â€” tweak if you want.\n",
        "COVERAGE_MIN = 300\n",
        "low_cov_dates = coverage_by_date[coverage_by_date < COVERAGE_MIN].index\n",
        "\n",
        "# We donâ€™t mutate features here; write a recommended mask so training can filter.\n",
        "date_mask_keep = (~pd.Series(features[\"date\"]).isin(low_cov_dates)) & (features[\"date\"] >= warmup_cutoff)\n",
        "keep_rate = date_mask_keep.mean()\n",
        "pd.DataFrame({\n",
        "    \"warmup_cutoff\":[warmup_cutoff],\n",
        "    \"coverage_min\":[COVERAGE_MIN],\n",
        "    \"keep_rate\":[float(keep_rate)]\n",
        "}).to_csv(\"qc_recommendations.csv\", index=False)\n",
        "\n",
        "# ---------- Meta ----------\n",
        "meta = {\n",
        "    \"universe\": {\n",
        "        \"description\": \"S&P 500 (current constituents; survivorship bias acknowledged).\",\n",
        "        \"count\": int(len(universe_df)),\n",
        "        \"hedges\": [\"SPY\",\"XLY\",\"XLF\",\"XLV\",\"XLK\",\"XLI\",\"XLE\",\"XLP\",\"XLB\",\"XLU\",\"XLRE\"],\n",
        "        \"context_symbols\": [\"^VIX\"],\n",
        "        \"lookback\": {\"start\": str(min_date.date()), \"end\": str(max_date.date())}\n",
        "    },\n",
        "    \"pricing\": {\n",
        "        \"source\": \"Yahoo Finance via yfinance\",\n",
        "        \"adjusted_prices_used\": True,\n",
        "        \"file\": \"raw_prices.parquet\"\n",
        "    },\n",
        "    \"features\": {\n",
        "        \"file\": \"features.parquet\",\n",
        "        \"rows\": int(n_rows),\n",
        "        \"tickers\": int(n_tickers),\n",
        "        \"leakage_control\": \"All predictive features shifted by 1 day.\",\n",
        "        \"cross_sectional_processing\": \"Winsorized [1%,99%] & z-scored by date (see 1.2).\",\n",
        "        \"imputation\": \"Fundamentals imputed (cross-sectional median) in 1.2; *_is_missing masks present.\"\n",
        "    },\n",
        "    \"qc\": {\n",
        "        \"missing_by_feature_csv\": \"qc_missing_by_feature.csv\",\n",
        "        \"coverage_by_date_csv\": \"qc_coverage_by_date.csv\",\n",
        "        \"recommendations_csv\": \"qc_recommendations.csv\",\n",
        "        \"warmup_cutoff\": str(warmup_cutoff.date()),\n",
        "        \"coverage_min\": COVERAGE_MIN,\n",
        "        \"recommendation\": \"Filter training rows to dates >= warmup_cutoff and dates with coverage >= coverage_min.\"\n",
        "    },\n",
        "    \"deliverables\": [\"universe.csv\", \"raw_prices.parquet\", \"features.parquet\",\n",
        "                     \"funda_quarterly.parquet\", \"funda_daily.parquet\", \"meta.yaml\",\n",
        "                     \"qc_missing_by_feature.csv\", \"qc_coverage_by_date.csv\", \"qc_recommendations.csv\"]\n",
        "}\n",
        "\n",
        "with open(\"meta.yaml\", \"w\") as f:\n",
        "    yaml.safe_dump(meta, f, sort_keys=False)\n",
        "\n",
        "print(\"Saved: meta.yaml + QC CSVs\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXCXZLXEzw6j",
        "outputId": "2487d887-e241-4bd2-b9ed-293208acd737"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: meta.yaml + QC CSVs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 1.4 DATA QC & ASSERTIONS (non-destructive; optional filtered view)\n",
        "# Produces: qc_summary.json, qc_constant_cols.csv, qc_missing_by_feature.csv (again),\n",
        "#           qc_skew_kurtosis.csv, qc_outlier_rate.csv, qc_drift.csv,\n",
        "#           features_filtered.parquet (optional, if you turn on APPLY_FILTERS)\n",
        "# ============================================================\n",
        "\n",
        "import json, pandas as pd, numpy as np\n",
        "from scipy.stats import skew, kurtosis\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", message=\"Precision loss occurred in moment calculation\")\n",
        "warnings.filterwarnings(\"ignore\", message=\"Degrees of freedom <= 0 for slice\")\n",
        "\n",
        "FEATURES_PATH = \"features.parquet\"\n",
        "\n",
        "APPLY_FILTERS = True          # set False if you only want reports\n",
        "COVERAGE_MIN = 300            # min tickers per date\n",
        "Z_OUTLIER = 5.0               # |z| threshold post-standardization\n",
        "EARLY_YEARS = 5               # windows for drift check\n",
        "RECENT_YEARS = 5\n",
        "\n",
        "df = pd.read_parquet(FEATURES_PATH)\n",
        "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
        "feature_cols = [c for c in df.columns if c not in {\"date\",\"ticker\",\"open\",\"high\",\"low\",\"close\",\"adj_close\",\"volume\"}]\n",
        "\n",
        "# Basic shape / duplicates\n",
        "dup_count = df.duplicated([\"date\",\"ticker\"]).sum()\n",
        "idx_dupes = int(dup_count)\n",
        "\n",
        "# Per-ticker monotonic date check\n",
        "monotonic_bad = []\n",
        "for t, g in df.groupby(\"ticker\"):\n",
        "    if not g[\"date\"].sort_values().is_monotonic_increasing:\n",
        "        monotonic_bad.append(t)\n",
        "\n",
        "# Constant/empty columns\n",
        "MIN_N = 200  # only compute moments if weâ€™ve got enough points\n",
        "sk_stats = []\n",
        "\n",
        "const_cols, empty_cols = [], []\n",
        "for c in feature_cols:\n",
        "    nn = df[c].notna().sum()\n",
        "    if nn == 0:\n",
        "        empty_cols.append(c)\n",
        "        continue\n",
        "    # treat â€œconstantâ€ as very low variance or single unique value\n",
        "    if df[c].nunique(dropna=True) == 1 or np.nanstd(df[c].to_numpy(dtype=float)) < 1e-12:\n",
        "        const_cols.append(c)\n",
        "\n",
        "pd.Series(const_cols, name=\"constant_cols\").to_csv(\"qc_constant_cols.csv\", index=False)\n",
        "pd.Series(empty_cols,  name=\"empty_cols\").to_csv(\"qc_empty_cols.csv\", index=False)\n",
        "\n",
        "# Missingness\n",
        "missing_pct = (1.0 - df[feature_cols].notna().mean()).sort_values(ascending=False)\n",
        "missing_pct.to_csv(\"qc_missing_by_feature.csv\", header=[\"missing_pct\"])\n",
        "\n",
        "# Coverage by date and warmup/low-coverage mask (reuse warmup logic from 1.3)\n",
        "spy_dates = df.loc[df[\"ticker\"]==\"SPY\", \"date\"].sort_values().unique()\n",
        "warmup_cutoff = pd.to_datetime(spy_dates[min(273, len(spy_dates)-1)]) if len(spy_dates) > 300 else df[\"date\"].min()\n",
        "coverage = df.groupby(\"date\")[\"ticker\"].nunique()\n",
        "low_cov_dates = coverage[coverage < COVERAGE_MIN].index\n",
        "keep_mask = (df[\"date\"] >= warmup_cutoff) & (~df[\"date\"].isin(low_cov_dates))\n",
        "keep_rate = float(keep_mask.mean())\n",
        "\n",
        "# Outlier rate (features are z-scored per date already)\n",
        "outlier_rate = {}\n",
        "for c in feature_cols:\n",
        "    s = df[c]\n",
        "    outlier_rate[c] = float((s.abs() > Z_OUTLIER).mean())\n",
        "pd.Series(outlier_rate, name=\"outlier_rate\").sort_values(ascending=False).to_csv(\"qc_outlier_rate.csv\")\n",
        "\n",
        "# Skew/Kurtosis (global, ignoring NaNs)\n",
        "sk_rows = []\n",
        "for c in feature_cols:\n",
        "    x = df[c].to_numpy(dtype=float)\n",
        "    x = x[np.isfinite(x)]\n",
        "    if len(x) < MIN_N or np.nanstd(x) < 1e-8:\n",
        "        # optional: quantile-based skew as fallback\n",
        "        try:\n",
        "            q1,q2,q3 = np.nanpercentile(x, [25,50,75])\n",
        "            bowley = ((q3 + q1) - 2*q2) / ((q3 - q1) + 1e-9)\n",
        "        except Exception:\n",
        "            bowley = np.nan\n",
        "        sk_rows.append([c, np.nan, np.nan, bowley, np.nan, np.nan])\n",
        "        continue\n",
        "    sk = float(skew(x, bias=False))\n",
        "    ku = float(kurtosis(x, fisher=True, bias=False))\n",
        "    p99 = float(np.nanpercentile(x, 99))\n",
        "    med = float(np.nanmedian(x))\n",
        "    dom = abs(p99) / (abs(med) + 1e-9)\n",
        "    sk_rows.append([c, sk, ku, np.nan, dom, p99])\n",
        "\n",
        "pd.DataFrame(sk_rows, columns=[\"feature\",\"skew\",\"kurtosis_fisher\",\"bowley_skew\",\"p99_to_median_abs\",\"p99\"])\\\n",
        "  .sort_values(\"p99_to_median_abs\", ascending=False)\\\n",
        "  .to_csv(\"qc_skew_kurtosis.csv\", index=False)\n",
        "\n",
        "# Drift: early vs recent windows\n",
        "dstart, dend = df[\"date\"].min(), df[\"date\"].max()\n",
        "span_years = (dend - dstart).days / 365.25\n",
        "if span_years < (EARLY_YEARS + RECENT_YEARS):\n",
        "    # fallback: split the dataset in half\n",
        "    mid = dstart + (dend - dstart) / 2\n",
        "    early = df[(df[\"date\"] >= dstart) & (df[\"date\"] <= mid)]\n",
        "    late  = df[(df[\"date\"] >  mid) & (df[\"date\"] <= dend)]\n",
        "else:\n",
        "    early_end    = pd.Timestamp(dstart) + pd.DateOffset(years=EARLY_YEARS)\n",
        "    recent_start = pd.Timestamp(dend)   - pd.DateOffset(years=RECENT_YEARS)\n",
        "    early = df[(df[\"date\"] >= dstart) & (df[\"date\"] <= early_end)]\n",
        "    late  = df[(df[\"date\"] >= recent_start) & (df[\"date\"] <= dend)]\n",
        "\n",
        "drift_rows = []\n",
        "for c in feature_cols:\n",
        "    e = early[c].astype(\"float64\"); l = late[c].astype(\"float64\")\n",
        "    e = e[np.isfinite(e)]; l = l[np.isfinite(l)]\n",
        "    if len(e) < MIN_N or len(l) < MIN_N:\n",
        "        drift_rows.append([c, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan])\n",
        "        continue\n",
        "    e_mean, e_std = float(np.nanmean(e)), float(np.nanstd(e))\n",
        "    l_mean, l_std = float(np.nanmean(l)), float(np.nanstd(l))\n",
        "    drift_rows.append([c, e_mean, l_mean, l_mean - e_mean, e_std, l_std, (l_std+1e-9)/(e_std+1e-9)])\n",
        "\n",
        "pd.DataFrame(\n",
        "    drift_rows,\n",
        "    columns=[\"feature\",\"early_mean\",\"late_mean\",\"mean_diff\",\"early_std\",\"late_std\",\"std_ratio_late_over_early\"]\n",
        ").to_csv(\"qc_drift.csv\", index=False)\n",
        "\n",
        "def bowley_skew(x):\n",
        "    q1, q2, q3 = np.nanpercentile(x, [25,50,75])\n",
        "    denom = (q3 - q1) + 1e-9\n",
        "    return float(((q3 + q1) - 2*q2) / denom)\n",
        "# you can compute this alongside or instead of moment skew for each feature\n",
        "\n",
        "# Optionally write filtered view for modeling\n",
        "if APPLY_FILTERS:\n",
        "    # Also drop truly empty/constant cols from the filtered file only\n",
        "    drop_cols = list(set(empty_cols) | set(const_cols))\n",
        "    cols_keep = [c for c in df.columns if c not in drop_cols]\n",
        "    df_filt = df.loc[keep_mask, cols_keep].copy()\n",
        "    df_filt.to_parquet(\"features_filtered.parquet\", index=False)\n",
        "\n",
        "# Summary JSON (for quick eyeball)\n",
        "summary = {\n",
        "    \"rows\": int(len(df)),\n",
        "    \"tickers\": int(df[\"ticker\"].nunique()),\n",
        "    \"dates\": int(df[\"date\"].nunique()),\n",
        "    \"date_min\": str(df[\"date\"].min().date()),\n",
        "    \"date_max\": str(df[\"date\"].max().date()),\n",
        "    \"duplicates_idx\": idx_dupes,\n",
        "    \"monotonic_date_issues\": len(monotonic_bad),\n",
        "    \"constant_cols\": len(const_cols),\n",
        "    \"empty_cols\": len(empty_cols),\n",
        "    \"warmup_cutoff\": str(warmup_cutoff.date()),\n",
        "    \"coverage_min\": COVERAGE_MIN,\n",
        "    \"keep_rate_after_filters\": keep_rate,\n",
        "    \"median_missing_pct\": float(missing_pct.median()),\n",
        "    \"max_missing_pct\": float(missing_pct.max()),\n",
        "    \"mean_outlier_rate_|z|>5\": float(pd.Series(outlier_rate).mean()),\n",
        "    \"filtered_file_written\": APPLY_FILTERS\n",
        "}\n",
        "\n",
        "# ðŸš¦ Hard QC checks â€” stop if these fail\n",
        "assert summary[\"duplicates_idx\"] == 0, \"Duplicate (date,ticker) rows found.\"\n",
        "assert summary[\"keep_rate_after_filters\"] >= 0.85, \"Too many rows dropped by filters.\"\n",
        "assert summary[\"constant_cols\"] <= 10, \"Suspicious number of constant columns.\"\n",
        "\n",
        "with open(\"qc_summary.json\",\"w\") as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "print(\"QC done â†’ qc_summary.json, qc_* CSVs\",\n",
        "      \"and features_filtered.parquet\" if APPLY_FILTERS else \"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8eWz-gvOXhR",
        "outputId": "6ecfa4d9-2c9b-452d-e498-d403d4a3232a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/numpy/lib/_nanfunctions_impl.py:1633: RuntimeWarning: Mean of empty slice\n",
            "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QC done â†’ qc_summary.json, qc_* CSVs and features_filtered.parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json, pandas as pd\n",
        "\n",
        "with open(\"qc_summary.json\") as f:\n",
        "    s = json.load(f)\n",
        "\n",
        "print(\"=== QC SUMMARY ===\")\n",
        "for k in [\n",
        "    \"rows\",\"tickers\",\"dates\",\"date_min\",\"date_max\",\n",
        "    \"duplicates_idx\",\"monotonic_date_issues\",\n",
        "    \"constant_cols\",\"empty_cols\",\n",
        "    \"warmup_cutoff\",\"coverage_min\",\"keep_rate_after_filters\",\n",
        "    \"median_missing_pct\",\"max_missing_pct\",\"mean_outlier_rate_|z|>5\",\n",
        "    \"filtered_file_written\"\n",
        "]:\n",
        "    print(f\"{k}: {s.get(k)}\")\n",
        "\n",
        "print(\"\\n=== Top 10 most-missing features ===\")\n",
        "print(pd.read_csv(\"qc_missing_by_feature.csv\").head(10))\n",
        "\n",
        "print(\"\\n=== Top 10 highest outlier rates (|z|>5) ===\")\n",
        "print(pd.read_csv(\"qc_outlier_rate.csv\").head(10))\n",
        "\n",
        "print(\"\\n=== Constant / Empty columns ===\")\n",
        "try: print(pd.read_csv(\"qc_constant_cols.csv\").head())\n",
        "except: print(\"none\")\n",
        "try: print(pd.read_csv(\"qc_empty_cols.csv\").head())\n",
        "except: print(\"none\")\n",
        "\n",
        "print(\"\\n=== Drift (largest mean change earlyâ†’late) ===\")\n",
        "drift = pd.read_csv(\"qc_drift.csv\")\n",
        "drift[\"abs_mean_diff\"] = drift[\"mean_diff\"].abs()\n",
        "print(drift.sort_values(\"abs_mean_diff\", ascending=False).head(10))\n",
        "\n",
        "print(\"\\n=== Filtered file shape ===\")\n",
        "ff = pd.read_parquet(\"features_filtered.parquet\")\n",
        "print(ff.shape, \"rows x cols; dates:\", ff['date'].min(), \"â†’\", ff['date'].max(), \"; tickers:\", ff['ticker'].nunique())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OApdlkYwWSsC",
        "outputId": "83e27a74-3653-49bc-965b-b04efae5c883"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== QC SUMMARY ===\n",
            "rows: 2331764\n",
            "tickers: 513\n",
            "dates: 4931\n",
            "date_min: 2006-01-03\n",
            "date_max: 2025-08-08\n",
            "duplicates_idx: 0\n",
            "monotonic_date_issues: 0\n",
            "constant_cols: 3\n",
            "empty_cols: 3\n",
            "warmup_cutoff: 2007-02-05\n",
            "coverage_min: 300\n",
            "keep_rate_after_filters: 0.9515559893711371\n",
            "median_missing_pct: 0.005390125244235655\n",
            "max_missing_pct: 1.0\n",
            "mean_outlier_rate_|z|>5: 0.03207488631670757\n",
            "filtered_file_written: True\n",
            "\n",
            "=== Top 10 most-missing features ===\n",
            "       Unnamed: 0  missing_pct\n",
            "0  earnings_yield     1.000000\n",
            "1             roe     1.000000\n",
            "2        accruals     1.000000\n",
            "3        mom_12_1     0.055661\n",
            "4         mom_12m     0.055661\n",
            "5         mom_6_1     0.027941\n",
            "6          mom_6m     0.027941\n",
            "7      ret_lag_60     0.013640\n",
            "8      ret_lag_59     0.013420\n",
            "9      ret_lag_58     0.013200\n",
            "\n",
            "=== Top 10 highest outlier rates (|z|>5) ===\n",
            "     Unnamed: 0  outlier_rate\n",
            "0     vix_close      0.999780\n",
            "1        sma_20      0.973347\n",
            "2        sma_50      0.967120\n",
            "3        atr_14      0.005802\n",
            "4      slope_20      0.002136\n",
            "5  sma_20_gt_50      0.000211\n",
            "6         rv_20      0.000153\n",
            "7     ret_lag_8      0.000039\n",
            "8     ret_lag_1      0.000039\n",
            "9     ret_lag_2      0.000039\n",
            "\n",
            "=== Constant / Empty columns ===\n",
            "               constant_cols\n",
            "0  earnings_yield_is_missing\n",
            "1             roe_is_missing\n",
            "2        accruals_is_missing\n",
            "       empty_cols\n",
            "0  earnings_yield\n",
            "1             roe\n",
            "2        accruals\n",
            "\n",
            "=== Drift (largest mean change earlyâ†’late) ===\n",
            "                feature  early_mean   late_mean   mean_diff  early_std  \\\n",
            "68               sma_20   27.258920  170.296715  143.037795  44.341893   \n",
            "69               sma_50   27.224041  169.016571  141.792529  44.140373   \n",
            "74            vix_close   23.547258   20.028145   -3.519113  11.924482   \n",
            "76        book_to_price   -0.288591   -0.410065   -0.121474   0.202021   \n",
            "79    shareholder_yield    0.058055   -0.056577   -0.114633   0.307089   \n",
            "75              breadth    0.428997    0.526166    0.097169   0.224669   \n",
            "73            spy_rv_20    0.203215    0.158993   -0.044222   0.150976   \n",
            "83             leverage   -0.265546   -0.226554    0.038993   0.225815   \n",
            "78             cf_yield   -0.332743   -0.363793   -0.031049   0.233052   \n",
            "80  gross_profitability   -0.127273   -0.152312   -0.025039   0.216276   \n",
            "\n",
            "      late_std  std_ratio_late_over_early  abs_mean_diff  \n",
            "68  348.427112                   7.857741     143.037795  \n",
            "69  344.943834                   7.814701     141.792529  \n",
            "74    5.563185                   0.466535       3.519113  \n",
            "76    0.205313                   1.016296       0.121474  \n",
            "79    0.251059                   0.817543       0.114633  \n",
            "75    0.242172                   1.077905       0.097169  \n",
            "73    0.075436                   0.499658       0.044222  \n",
            "83    0.207351                   0.918236       0.038993  \n",
            "78    0.220385                   0.945647       0.031049  \n",
            "80    0.214287                   0.990801       0.025039  \n",
            "\n",
            "=== Filtered file shape ===\n",
            "(2218804, 94) rows x cols; dates: 2007-02-05 00:00:00 â†’ 2025-08-08 00:00:00 ; tickers: 513\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>ðŸ“¦ Summary â€” Section 1 (Data & Universe)</summary>\n",
        "\n",
        "In this section, we **built the full, modeling-ready dataset** by merging historical prices, technical indicators, market context, and fundamentals into a single leakage-controlled feature matrix.  \n",
        "Key steps included:\n",
        "\n",
        "- **Data acquisition** â€” pulled long-term daily OHLCV for the equity universe, hedges, and context symbols, plus quarterly fundamentals from FMP.\n",
        "- **Feature engineering** â€” created lagged returns/volatility, momentum metrics, trend filters, ATR, volatility-adjusted momentum, and value/quality factor composites. Fundamentals were forward-filled to daily frequency.\n",
        "- **Leakage control & scaling** â€” shifted predictive features by one day, winsorized extreme values, and cross-sectionally z-scored each feature per date.\n",
        "- **Missing data handling** â€” conservative imputation for fundamentals and binary masks to record missingness.\n",
        "- **Quality control** â€” removed low-coverage dates, early warmup period, constant/empty columns, and duplicate rows; generated QC reports and metadata.\n",
        "\n",
        "**Outcome:** A clean, consistent, and statistically robust `features_filtered.parquet` file â€” ready for direct use in **Section 2 (Regime Modeling)** without recomputing or re-fetching any raw data.\n",
        "\n",
        "</details>\n"
      ],
      "metadata": {
        "id": "vOLQATza111V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary> Variables to reuse â€” Section 1 (Data & Universe) </summary>\n",
        "**Status:** Done. Artifacts are written; QC checks passed; ready to start **Section 2 (Regime Modeling)** using the saved files and globals below.\n",
        "\n",
        "---\n",
        "\n",
        "## Canonical Artifacts (reuse, donâ€™t recompute)\n",
        "- `universe.csv` â€“ S&P 500 tickers (Yahoo-style), excludes hedges/context.\n",
        "- `raw_prices.parquet` â€“ OHLCV + `adj_close` for equities + hedges + `^VIX` (long format).\n",
        "- `features.parquet` â€“ lagged, winsorized, cross-sectionally z-scored features (+ *_is_missing masks).\n",
        "- `features_filtered.parquet` â€“ modeling-ready view (warmup & low-coverage dates removed; empty/constant cols dropped).\n",
        "- `funda_quarterly.parquet`, `funda_daily.parquet` â€“ fundamentals at quarterly/daily granularity.\n",
        "- `meta.yaml` â€“ machine-readable metadata (sources, lookback, QC guidance).\n",
        "- QC reports: `qc_summary.json`, `qc_missing_by_feature.csv`, `qc_coverage_by_date.csv`, `qc_constant_cols.csv`, `qc_empty_cols.csv`, `qc_outlier_rate.csv`, `qc_skew_kurtosis.csv`, `qc_drift.csv`, `qc_recommendations.csv`.\n",
        "\n",
        "---\n",
        "\n",
        "## Reusable Globals (organized)\n",
        "> These exist (or are trivially reloadable) after Section 1. Prefer these over re-deriving.\n",
        "\n",
        "### Dates / Ranges\n",
        "- `START_DATE = \"2006-01-01\"`  \n",
        "- `END_DATE = datetime.today().strftime(\"%Y-%m-%d\")`\n",
        "\n",
        "### Universe & Symbols\n",
        "- `sp500_url` â€“ Wikipedia source for constituents.\n",
        "- `tickers_raw` â†’ raw symbols from Wikipedia.\n",
        "- `tickers` â†’ Yahoo-normalized tickers (periods â†’ dashes).\n",
        "- `hedges` â†’ `[\"SPY\",\"XLY\",\"XLF\",\"XLV\",\"XLK\",\"XLI\",\"XLE\",\"XLP\",\"XLB\",\"XLU\",\"XLRE\"]`\n",
        "- `context_symbols` â†’ `[\"^VIX\"]`  *(later used as `{\"^VIX\"}` set in 1.2)*\n",
        "- `universe` â†’ sorted unique S&P tickers.\n",
        "- `universe_all` â†’ `universe + hedges + context_symbols`\n",
        "- `universe_full` â†’ list from `universe.csv` (canonical equities universe for downstream code).\n",
        "\n",
        "### DataFrames (load-once, reuse)\n",
        "- `prices` â†’ long OHLCV for `universe_all` (saved as `raw_prices.parquet`).\n",
        "- `features` â†’ merged technical + context + fundamentals (post-shift, winsorize, z-score) (saved).\n",
        "- `vix` â†’ `^VIX` close series; `spy` â†’ SPY prices with `spy_ret`, `spy_rv_20`.\n",
        "- `ctx` â†’ market context by date: `[\"spy_rv_20\",\"vix_close\",\"breadth\"]`.\n",
        "- `px_daily_all` â†’ `[\"date\",\"ticker\",\"adj_close\"]` for equities universe.\n",
        "- `dates_all` â†’ unique trading dates.\n",
        "- `funda_q` â†’ quarterly fundamentals by ticker (saved).\n",
        "- `funda_daily` â†’ daily forward-filled fundamentals (saved).\n",
        "\n",
        "### Feature Engineering Toggles / Windows\n",
        "- `COMPUTE_SLOPE = True`\n",
        "- `SLOPE_WINDOW = 20`\n",
        "- `RV_WIN = 20`\n",
        "- `ATR_WIN = 14`\n",
        "\n",
        "### Provider / API / Caching\n",
        "- `PROVIDER = \"fmp\"`\n",
        "- `FMP_API_KEY` â€“ from env or prompt (in-memory only).\n",
        "- `CACHE_DIR = \"cache\"`\n",
        "- `CHUNK_TICKERS = 100`, `START_AT = 0`, `SKIP_IF_CACHED = True`\n",
        "- `MAX_WORKERS = 4`, `RETRY_ATTEMPTS = 5`, `BATCH_SLEEP = (0.2, 0.6)`\n",
        "\n",
        "### Useful Function Handles\n",
        "- `to_fmp_symbol(sym)` â€“ Yahoo â€œ-â€ â†” FMP â€œ.â€ class ticker mapping.\n",
        "- `is_index_like(sym)` â€“ identifies index symbols (e.g., `^VIX`).\n",
        "- `compute_atr(df, window=ATR_WIN)`\n",
        "- `vectorized_rolling_slope(y, window=SLOPE_WINDOW)`\n",
        "- `mom_over_n(adj_close, n)`\n",
        "- `_tidy_quarterly_df(df)`, `_coalesce_cols(df, cols, default)`\n",
        "- `_fetch_quarterly_funda_fmp(ticker)` â€“ pulls BS/IS/CF, coalesces variants.\n",
        "- `fetch_or_load_cached_quarterly(ticker)` â€“ cached loader for fundamentals.\n",
        "- `cs_standardize_fast(df, cols, lo=0.01, hi=0.99)` â€“ per-date winsorize+z-score.\n",
        "\n",
        "### Column Sets / Masks (downstream-friendly)\n",
        "- `non_feature_cols = {\"date\",\"ticker\",\"open\",\"high\",\"low\",\"close\",\"adj_close\",\"volume\"}`\n",
        "- `cols_to_shift` â€“ all predictive feature columns actually shifted by 1 bar.\n",
        "- `cs_cols` â€“ features standardized cross-sectionally (lags, vol, mom, fundamentals, etc.).\n",
        "- `context_keep_raw = [\"spy_rv_20\",\"vix_close\",\"breadth\"]`\n",
        "- *(QC section)*\n",
        "  - `FEATURES_PATH = \"features.parquet\"`, `UNIVERSE_PATH = \"universe.csv\"`\n",
        "  - `COVERAGE_MIN = 300`\n",
        "  - `APPLY_FILTERS = True`\n",
        "  - `Z_OUTLIER = 5.0`, `EARLY_YEARS = 5`, `RECENT_YEARS = 5`\n",
        "  - `warmup_cutoff` â€“ computed from SPY date series (â‰ˆ273 trading-day warmup).\n",
        "  - `keep_mask` â€“ dates â‰¥ `warmup_cutoff` and with coverage â‰¥ `COVERAGE_MIN`.\n",
        "  - *(Note: `features_filtered.parquet` is written using `keep_mask` and pruned columns.)*\n",
        "\n",
        "---\n",
        "\n",
        "## What this means for Section 2 (Regimes)\n",
        "- **Use** `features_filtered.parquet` (or reload `features` and apply `keep_mask`) to build HMM inputs.\n",
        "- Inputs available out of the box: `spy_rv_20`, `vix_close`, `breadth`, and per-asset returns (`ret_1d`), plus everything in `cs_cols`.\n",
        "- **No duplicate `(date, ticker)` rows**, **no monotonic issues**; early sparse periods removed by `warmup_cutoff`/`keep_mask`.\n",
        "\n",
        "---\n",
        "\n",
        "## Sanity Questions (short answers)\n",
        "- **â€œAre we good to go?â€** Yes â€” Section 1 is complete and validated; proceed to regime modeling.\n",
        "- **â€œEmpty rows?â€** Raw OHLCV rows with all NaNs were dropped; the modeling file (`features_filtered.parquet`) is filtered to warmup/coverage and prunes empty/constant columns. Row-level all-NaN feature cases should not remain after these filters.\n",
        "- **â€œAdd the assertions?â€** Already present and passing in QC (`qc_summary.json`). No need to add them again unless you change the pipeline.\n",
        "</details>"
      ],
      "metadata": {
        "id": "JGcOoZHEdNC4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Regime Modeling"
      ],
      "metadata": {
        "id": "u5YS_Mluwz_v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details> <summary>\n",
        "Outline (HMM â†’ Regime Labels & Probabilities)</summary>\n",
        "\n",
        "# 2) Regime Modeling â€” Updated Outline (HMM â†’ Regime Labels & Probabilities)\n",
        "\n",
        "## 2.0 Scope & Interfaces\n",
        "- **Goal:** Assign a daily market regime (Risk-On, Risk-Off, Transition) with posterior probabilities to drive regime-aware weighting, turnover caps, and risk targets in Sections 3â€“5.\n",
        "- **Inputs (from Section 1):**\n",
        "  - `features_filtered.parquet` with **raw** `spy_rv_20`, `vix_close`, `breadth`, and SPY `adj_close` for return computation.\n",
        "  - Trading calendar (aligned daily business days).\n",
        "- **Outputs (artifacts):**\n",
        "  - `regime_labels.parquet`: `date, state_id, p0..pK, regime_label`\n",
        "  - `regime_labels.csv` (plot-friendly)\n",
        "  - `regime_plot.png` (timeline with shading), `state_profiles.csv` (state stats)\n",
        "  - `regime_hmm.pkl` (bundle: scaler + HMM per walk-forward window)\n",
        "  - `regime_meta.json` (config, stateâ†’label map, scaler params, transition matrix, diagnostics)\n",
        "  - `regime_sensitivity.json` (K/feature/era stability tests)\n",
        "- **Pass/Fail gates:**\n",
        "  - Interpretable state profiles (return/vol ordering aligns with labels)\n",
        "  - Reasonable persistence (median run length > 5â€“10 days; no chattering)\n",
        "  - Stable mapping across walk-forward windows (low semantic flip rate)\n",
        "  - No leakage (all inputs at t known at t)\n",
        "\n",
        "---\n",
        "\n",
        "## 2.1 Data Assembly (Market Panel)\n",
        "- **Series:**\n",
        "  - SPY **log return** at t (computed from `adj_close`, shifted to avoid leakage if needed).\n",
        "  - SPY realized volatility (20-day) â€” from raw `spy_rv_20`.\n",
        "  - VIX **level** (`vix_close`) and optionally **daily Î”** (t âˆ’ t-1).\n",
        "  - Market breadth (% advancers in S&P, known at t).\n",
        "- **IMPORTANT:** Use **raw** context series from Section 1 (`spy_rv_20`, `vix_close`, `breadth`), **not** cross-sectional z-scored features.\n",
        "- **Breadth timing:** Confirm that `breadth` reflects t-1 data available at t; if not, shift by 1.\n",
        "- **Alignment:** Daily business days; merge by `date`; forward-fill only for indicators known at t; drop rows with missing core inputs.\n",
        "- **Standardization:** Fit `StandardScaler` **per train window** on the raw context features; persist scaler per window (stored in `regime_hmm.pkl`).\n",
        "- **Sanity checks:**\n",
        "  - Stationarity proxy (mean/var drift over eras).\n",
        "  - Outlier handling: no winsorization needed for HMM since we scale raw series per window.\n",
        "  - Coverage check: ensure no missing dates in test stitching.\n",
        "\n",
        "---\n",
        "\n",
        "## 2.2 Model Choice & Configuration\n",
        "- **Primary:** Gaussian HMM with `covariance_type=\"full\"`; components K âˆˆ {2,3} (default 3).\n",
        "- **Alternative (optional):** Student-t HMM, GMM-HMM, Markov-Switching VAR, or Bayesian HMM with sticky priors.\n",
        "- **Hyperparameters:**\n",
        "  - `n_components`, `covariance_type`, `n_iter`, `random_state`.\n",
        "  - Dirichlet priors / sticky transitions to enforce regime persistence.\n",
        "- **Training protocol:**\n",
        "  - Train on standardized features in the train window.\n",
        "  - Multiple random restarts; choose model with highest log-likelihood.\n",
        "  - If applying **finance recency weighting rule**: optionally weight log-likelihood so recent data has more influence (can be implemented here if desired).\n",
        "\n",
        "---\n",
        "\n",
        "## 2.3 State Labeling & Semantics\n",
        "- **Profile each state:**\n",
        "  - Mean and vol of SPY returns.\n",
        "  - Mean VIX level, mean Î”VIX.\n",
        "  - Mean breadth, tail metrics (5% quantile returns).\n",
        "- **Label rules:**\n",
        "  - Highest mean return & lowest vol â†’ **Risk-On**\n",
        "  - Highest vol & lowest return â†’ **Risk-Off**\n",
        "  - Remaining state â†’ **Transition**\n",
        "- **Tie-breakers:** breadth, VIX changes, downside tails.\n",
        "- **Persist mapping:** Save `state_id â†’ regime_label` per window in `regime_meta.json` so semantics donâ€™t silently drift across walk-forward windows.\n",
        "\n",
        "---\n",
        "\n",
        "## 2.4 Smoothing, Persistence & Debounce\n",
        "- **Posterior smoothing:** Option to use Viterbi most-likely path vs. raw posterior argmax.\n",
        "- **Debounce parameters:** `MIN_DWELL_DAYS` and `POSTERIOR_THRESH` from `config.yaml`.\n",
        "- **Gap handling:** Holidays/missing days inherit last known regime; no forward-looking fill.\n",
        "\n",
        "---\n",
        "\n",
        "## 2.5 Robustness & Sensitivity\n",
        "- **K sensitivity:** Run K=2 and K=3; prefer K with clearest separation (return/vol) and healthy dwell-time.\n",
        "- **Feature sensitivity:** Drop-one/add-one tests (remove VIX, remove breadth, etc.) to check label stability.\n",
        "- **Era stability:** Compare state profiles and transition matrices pre/post-2015 and during crisis years (e.g., 2020).\n",
        "- **Bootstrap:** Block bootstrap re-fit; produce confusion matrix for label stability across samples.\n",
        "\n",
        "---\n",
        "\n",
        "## 2.6 Diagnostics & QA\n",
        "- **Plots:**\n",
        "  - Timeline with regime shading over SPY price & drawdown.\n",
        "  - Posterior probabilities (stacked area).\n",
        "  - State return histograms, QQ plots.\n",
        "  - Transition matrix heatmap, dwell-time distribution.\n",
        "- **Tables:**\n",
        "  - State profiles (returns, vol, VIX, breadth, tails).\n",
        "  - Transition matrix & steady-state distribution.\n",
        "  - Switch frequency and chattering metrics.\n",
        "- **Alerts:**\n",
        "  - Flag if any state has inconsistent semantics (positive mean but top-2 vol, dwell-time < 3 days, mapping flips).\n",
        "\n",
        "---\n",
        "\n",
        "## 2.7 Regime-Aware Policy Hooks (Interfaces to Sections 3â€“5)\n",
        "- **Weights & turnover caps:** JSON map per regime (e.g., throttle momentum in Risk-Off, upweight quality).\n",
        "- **Risk targets:** Per-regime vol targets (e.g., 10%/8%/6% for On/Trans/Off).\n",
        "- **Hedge intensity:** Baseline hedge ratios per regime; pass to RL policy as defaults.\n",
        "- **Confidence proxy:** Use max posterior or entropy to scale aggressiveness.\n",
        "\n",
        "---\n",
        "\n",
        "## 2.8 Walk-Forward Integration\n",
        "- **Windows:** Match Section 6 (rolling/expanding).\n",
        "- **Per window:**\n",
        "  - Fit scaler + HMM on train subset.\n",
        "  - Apply to test subset only.\n",
        "  - Save artifacts: `regime_labels_<winid>.parquet`, `regime_hmm.pkl`, `regime_meta.json`.\n",
        "- **Stitching:** Concatenate per-window outputs into one continuous timeline for backtests.\n",
        "- **Label stability:** Use saved stateâ†’label mapping to avoid regime meaning drift.\n",
        "\n",
        "---\n",
        "\n",
        "## 2.9 Forward (Shadow) Mode\n",
        "- **Daily update:** Apply persisted scaler + HMM to latest t; append to `regime_labels.parquet`.\n",
        "- **Retrain cadence:** Weekly/bi-weekly.\n",
        "- **Logging:** Save model hash, posterior, chosen label, features vector.\n",
        "- **Alerts:** If mapping flips or dwell-time anomaly detected.\n",
        "\n",
        "---\n",
        "\n",
        "## 2.10 Configuration & Reproducibility\n",
        "- **Config keys (`config.yaml`):**\n",
        "  - Features list for HMM.\n",
        "  - `n_components`, `MIN_DWELL_DAYS`, `POSTERIOR_THRESH`.\n",
        "  - Finance recency weighting toggle & decay parameter (if implemented here).\n",
        "  - Random seed, plot toggles.\n",
        "- **Serialization:**\n",
        "  - joblib for model + scaler.\n",
        "  - JSON for meta (labels, thresholds, diagnostics).\n",
        "- **Tests:**\n",
        "  - Deterministic output with fixed seed.\n",
        "  - No leakage (t-only features).\n",
        "  - Posterior rows sum to 1; dates strictly increasing.\n",
        "  - No gaps after stitching.\n",
        "  - Label semantics test per window.\n",
        "\n",
        "---\n",
        "\n",
        "## 2.11 Deliverables Checklist\n",
        "- `regime_labels.parquet` (+ CSV).\n",
        "- `regime_hmm.pkl` (model + scaler per window).\n",
        "- `regime_meta.json` (stateâ†’label, scaler params, diagnostics).\n",
        "- `regime_timeline.png`, `regime_posteriors.png`, `state_profiles.csv`, `transition_matrix.csv`.\n",
        "- `regime_sensitivity.json` (K/feature/era stability).\n",
        "- `regime_policy_map.json` (interfaces to Sections 3â€“5).\n",
        "\n",
        "\n",
        "---\n",
        "</details>"
      ],
      "metadata": {
        "id": "mo-4MmP4fdjw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Section 2.0 â€” Scope & Interfaces (Regime Modeling bootstrap)\n",
        "# Builds on Section 1 artifacts; defines config, I/O, sanity checks,\n",
        "# and prepares the market-level panel stub used by 2.1+ (no HMM yet).\n",
        "# ============================================================\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import os\n",
        "import json\n",
        "import yaml\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import Dict, Any, List\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# 0) Paths & directories (reuse Section 1 outputs)\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "FEATURES_PATH_DEFAULT = (\n",
        "    \"features_filtered.parquet\"\n",
        "    if os.path.exists(\"features_filtered.parquet\")\n",
        "    else \"features.parquet\"\n",
        ")\n",
        "UNIVERSE_PATH = \"universe.csv\"\n",
        "ARTIFACT_DIR = \"artifacts\"\n",
        "REGIME_DIR = os.path.join(ARTIFACT_DIR, \"regimes\")\n",
        "PLOTS_DIR = os.path.join(REGIME_DIR, \"plots\")\n",
        "\n",
        "os.makedirs(REGIME_DIR, exist_ok=True)\n",
        "os.makedirs(PLOTS_DIR, exist_ok=True)\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# 1) Config â€” defaults + optional override via config.yaml\n",
        "# Keys are intentionally minimal here; 2.1â€“2.10 will read them.\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "@dataclass\n",
        "class RegimeConfig:\n",
        "    # Raw context features for HMM (NOT cross-sectional z-scores)\n",
        "    hmm_features: List[str]\n",
        "    include_dvix: bool                 # add Î”VIX feature to panel\n",
        "    n_components_grid: List[int]       # HMM K sensitivity (e.g., [2,3])\n",
        "    covariance_type: str               # \"full\" by default\n",
        "    random_seed: int\n",
        "    # Debounce (used later in 2.4)\n",
        "    min_dwell_days: int\n",
        "    posterior_thresh: float\n",
        "    # Optional finance rule: give more weight to recent samples during HMM fit\n",
        "    recency_weighting: bool\n",
        "    recency_half_life_days: int\n",
        "    # I/O\n",
        "    plots_enabled: bool\n",
        "    save_csv_alongside_parquet: bool\n",
        "    features_path: str = FEATURES_PATH_DEFAULT\n",
        "    universe_path: str = UNIVERSE_PATH\n",
        "    regime_dir: str = REGIME_DIR\n",
        "    plots_dir: str = PLOTS_DIR\n",
        "\n",
        "DEFAULT_CFG = RegimeConfig(\n",
        "    hmm_features=[\"spy_rv_20\", \"vix_close\", \"breadth\"],  # from Section 1 (raw context)\n",
        "    include_dvix=True,\n",
        "    n_components_grid=[2, 3],\n",
        "    covariance_type=\"full\",\n",
        "    random_seed=42,\n",
        "    min_dwell_days=3,\n",
        "    posterior_thresh=0.55,\n",
        "    recency_weighting=False,           # flip to True if enabling in 2.2\n",
        "    recency_half_life_days=90,\n",
        "    plots_enabled=True,\n",
        "    save_csv_alongside_parquet=True,\n",
        ")\n",
        "\n",
        "CONFIG_FILE = \"config.yaml\"\n",
        "user_cfg = {}\n",
        "if os.path.exists(CONFIG_FILE):\n",
        "    try:\n",
        "        with open(CONFIG_FILE, \"r\") as f:\n",
        "            raw_cfg = yaml.safe_load(f) or {}\n",
        "            if isinstance(raw_cfg, dict):\n",
        "                user_cfg = raw_cfg.get(\"regimes\", {}) or {}\n",
        "    except Exception:\n",
        "        user_cfg = {}\n",
        "\n",
        "def merge_cfg(default: RegimeConfig, override: Dict[str, Any]) -> RegimeConfig:\n",
        "    d = asdict(default)\n",
        "    for k, v in override.items():\n",
        "        if k in d and v is not None:\n",
        "            d[k] = v\n",
        "    return RegimeConfig(**d)\n",
        "\n",
        "CFG = merge_cfg(DEFAULT_CFG, user_cfg)\n",
        "\n",
        "# Persist effective config for traceability\n",
        "with open(os.path.join(REGIME_DIR, \"regime_config_effective.json\"), \"w\") as f:\n",
        "    json.dump(asdict(CFG), f, indent=2)\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# 2) Load Section 1 artifacts and build the market panel stub\n",
        "# NOTE: use RAW context features from Section 1 (no CS-z).\n",
        "# This version auto-detects whether ^VIX exists as a ticker,\n",
        "# or vix_close/breadth/spy_rv_20 are already on every row.\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "assert os.path.exists(CFG.features_path), f\"Missing features file: {CFG.features_path}\"\n",
        "fe = pd.read_parquet(CFG.features_path)\n",
        "fe[\"date\"] = pd.to_datetime(fe[\"date\"], utc=False, errors=\"coerce\")\n",
        "fe = fe.dropna(subset=[\"date\"]).sort_values([\"date\", \"ticker\"])\n",
        "\n",
        "# Required columns present?\n",
        "required_cols = {\"date\", \"ticker\", \"adj_close\", \"spy_rv_20\", \"vix_close\", \"breadth\"}\n",
        "missing = list(required_cols - set(fe.columns))\n",
        "if missing:\n",
        "    raise ValueError(f\"Required columns missing in features file: {sorted(missing)}\")\n",
        "\n",
        "# SPY must exist for returns\n",
        "if not (fe[\"ticker\"] == \"SPY\").any():\n",
        "    raise ValueError(\"SPY rows not found in features file; cannot compute spy_ret.\")\n",
        "\n",
        "# Build SPY returns\n",
        "spy = fe.loc[fe[\"ticker\"] == \"SPY\", [\"date\", \"adj_close\", \"spy_rv_20\"]].copy()\n",
        "spy[\"spy_ret\"] = np.log(spy[\"adj_close\"] / spy[\"adj_close\"].shift(1))\n",
        "\n",
        "# vix_close / breadth / rv_20 may be replicated on every row; prefer unique-by-date view\n",
        "# If ^VIX rows exist, we can still just take unique-by-dateâ€”works for both layouts.\n",
        "vix_by_date = fe[[\"date\", \"vix_close\"]].drop_duplicates(\"date\").copy()\n",
        "breadth_by_date = fe[[\"date\", \"breadth\"]].drop_duplicates(\"date\").copy()\n",
        "rv20_by_date = fe[[\"date\", \"spy_rv_20\"]].drop_duplicates(\"date\").copy()\n",
        "\n",
        "# Merge market panel (date-level)\n",
        "mkt = (\n",
        "    spy[[\"date\", \"spy_ret\"]]                     # SPY returns\n",
        "    .merge(rv20_by_date, on=\"date\", how=\"inner\") # realized vol\n",
        "    .merge(vix_by_date, on=\"date\", how=\"inner\")  # VIX level\n",
        "    .merge(breadth_by_date, on=\"date\", how=\"inner\")  # breadth\n",
        "    .sort_values(\"date\")\n",
        ")\n",
        "\n",
        "# Optional Î”VIX (level change)\n",
        "if CFG.include_dvix:\n",
        "    mkt[\"dvix\"] = mkt[\"vix_close\"].diff()\n",
        "\n",
        "# Breadth timing guard: uncomment if your breadth is same-day and should be known-at-t\n",
        "# mkt[\"breadth\"] = mkt[\"breadth\"].shift(1)\n",
        "\n",
        "# Complete-case rows only (HMM requires no NaNs)\n",
        "core_cols = [\"spy_ret\", \"spy_rv_20\", \"vix_close\", \"breadth\"] + ([\"dvix\"] if CFG.include_dvix else [])\n",
        "mkt = mkt.dropna(subset=core_cols).reset_index(drop=True)\n",
        "\n",
        "# Save panel (consumed by 2.1/2.2)\n",
        "panel_path = os.path.join(CFG.regime_dir, \"market_panel.parquet\")\n",
        "mkt.to_parquet(panel_path, index=False)\n",
        "if CFG.save_csv_alongside_parquet:\n",
        "    mkt.to_csv(os.path.join(CFG.regime_dir, \"market_panel.csv\"), index=False)\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# 5) Finalize & console summary (with robust date handling)\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def _fmt_date(ts):\n",
        "    return None if pd.isna(ts) else pd.Timestamp(ts).strftime(\"%Y-%m-%d\")\n",
        "\n",
        "if mkt.empty:\n",
        "    # Diagnostics to help you decide if breadth shift is needed, etc.\n",
        "    core_for_diag = [\"spy_ret\", \"spy_rv_20\", \"vix_close\", \"breadth\"] + ([\"dvix\"] if CFG.include_dvix else [])\n",
        "    non_null_counts = {c: int(fe[c].notna().sum()) if c in fe.columns else 0 for c in core_for_diag}\n",
        "    spy_src = fe.loc[fe[\"ticker\"] == \"SPY\", [\"date\", \"adj_close\", \"spy_rv_20\"]].assign(\n",
        "        spy_ret=lambda d: np.log(d[\"adj_close\"] / d[\"adj_close\"].shift(1))\n",
        "    )\n",
        "    coverage_diag = {\n",
        "        \"rows_with_spy_ret_and_rv20\": int(spy_src.dropna(subset=[\"spy_ret\", \"spy_rv_20\"]).shape[0]),\n",
        "        \"unique_dates_with_vix\": int(vix_by_date.dropna(subset=[\"vix_close\"]).shape[0]),\n",
        "        \"unique_dates_with_breadth\": int(breadth_by_date.dropna(subset=[\"breadth\"]).shape[0]),\n",
        "    }\n",
        "    raise ValueError(\n",
        "        \"Market panel is empty after merging/dropping NaNs. \"\n",
        "        f\"Non-null counts (in features file): {non_null_counts}. \"\n",
        "        f\"Coverage by component: {coverage_diag}. \"\n",
        "        \"Common fixes: ensure breadth timing (try shifting breadth by 1), \"\n",
        "        \"or check for gaps in SPY/VIX/breadth date alignment.\"\n",
        "    )\n",
        "\n",
        "summary = {\n",
        "    \"features_file\": CFG.features_path,\n",
        "    \"universe_file\": CFG.universe_path,\n",
        "    \"market_panel_rows\": int(mkt.shape[0]),\n",
        "    \"market_panel_cols\": list(mkt.columns),\n",
        "    \"date_min\": _fmt_date(mkt['date'].min()),\n",
        "    \"date_max\": _fmt_date(mkt['date'].max()),\n",
        "    \"config_effective\": os.path.abspath(os.path.join(CFG.regime_dir, \"regime_config_effective.json\")),\n",
        "    \"panel_path\": os.path.abspath(panel_path),\n",
        "    \"meta_path\": os.path.abspath(os.path.join(CFG.regime_dir, 'regime_meta.json')),\n",
        "}\n",
        "print(json.dumps(summary, indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z59_nRdbzRNn",
        "outputId": "1f33343f-ca20-4f76-db95-44a0851ed1a2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"features_file\": \"features_filtered.parquet\",\n",
            "  \"universe_file\": \"universe.csv\",\n",
            "  \"market_panel_rows\": 4657,\n",
            "  \"market_panel_cols\": [\n",
            "    \"date\",\n",
            "    \"spy_ret\",\n",
            "    \"spy_rv_20\",\n",
            "    \"vix_close\",\n",
            "    \"breadth\",\n",
            "    \"dvix\"\n",
            "  ],\n",
            "  \"date_min\": \"2007-02-06\",\n",
            "  \"date_max\": \"2025-08-08\",\n",
            "  \"config_effective\": \"/content/artifacts/regimes/regime_config_effective.json\",\n",
            "  \"panel_path\": \"/content/artifacts/regimes/market_panel.parquet\",\n",
            "  \"meta_path\": \"/content/artifacts/regimes/regime_meta.json\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Section 2.1 â€” Data Assembly (windowed extraction + scaling)\n",
        "# Uses artifacts from 2.0; prepares X_train/X_test for HMM.\n",
        "# ============================================================\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import os\n",
        "import json\n",
        "from dataclasses import asdict\n",
        "from typing import Dict, Any, Tuple, List, Optional\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import joblib\n",
        "\n",
        "# Reuse CFG, paths from 2.0\n",
        "REGIME_DIR = CFG.regime_dir\n",
        "PLOTS_DIR = CFG.plots_dir\n",
        "PANEL_PATH = os.path.join(REGIME_DIR, \"market_panel.parquet\")\n",
        "META_PATH = os.path.join(REGIME_DIR, \"regime_meta.json\")\n",
        "\n",
        "assert os.path.exists(PANEL_PATH), f\"Missing market panel: {PANEL_PATH}\"\n",
        "mkt = pd.read_parquet(PANEL_PATH)\n",
        "mkt[\"date\"] = pd.to_datetime(mkt[\"date\"])\n",
        "mkt = mkt.sort_values(\"date\").reset_index(drop=True)\n",
        "\n",
        "# Choose feature list (raw context features only; dvix optional)\n",
        "hmm_feat_cols = list(CFG.hmm_features)\n",
        "if CFG.include_dvix and \"dvix\" not in hmm_feat_cols:\n",
        "    hmm_feat_cols.append(\"dvix\")\n",
        "\n",
        "# Safety: ensure columns exist\n",
        "missing_cols = [c for c in hmm_feat_cols + [\"spy_ret\"] if c not in mkt.columns]\n",
        "if missing_cols:\n",
        "    raise ValueError(f\"Missing required columns in market panel: {missing_cols}\")\n",
        "\n",
        "def make_window_masks(df: pd.DataFrame,\n",
        "                      train_start: str,\n",
        "                      train_end: str,\n",
        "                      test_start: str,\n",
        "                      test_end: str) -> Tuple[pd.Series, pd.Series]:\n",
        "    d = df[\"date\"]\n",
        "    train_mask = (d >= pd.to_datetime(train_start)) & (d <= pd.to_datetime(train_end))\n",
        "    test_mask  = (d >= pd.to_datetime(test_start))  & (d <= pd.to_datetime(test_end))\n",
        "    return train_mask, test_mask\n",
        "\n",
        "def build_hmm_matrices(df: pd.DataFrame,\n",
        "                       features: List[str],\n",
        "                       train_start: str,\n",
        "                       train_end: str,\n",
        "                       test_start: str,\n",
        "                       test_end: str,\n",
        "                       scaler_out_path: Optional[str] = None,\n",
        "                       breadth_shift_days: int = 0) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      {\n",
        "        'X_train': np.ndarray,\n",
        "        'X_test': np.ndarray,\n",
        "        'dates_train': pd.DatetimeIndex,\n",
        "        'dates_test': pd.DatetimeIndex,\n",
        "        'scaler_path': str,\n",
        "        'scaler_mean': list,\n",
        "        'scaler_scale': list,\n",
        "        'qc': dict\n",
        "      }\n",
        "    \"\"\"\n",
        "    dfw = df.copy()\n",
        "\n",
        "    # Optional breadth shift (if you decide breadth should be known-at-t from t-1)\n",
        "    if breadth_shift_days != 0 and \"breadth\" in features:\n",
        "        dfw[\"breadth\"] = dfw[\"breadth\"].shift(breadth_shift_days)\n",
        "\n",
        "    # Drop rows with missing features\n",
        "    dfw = dfw.dropna(subset=features).reset_index(drop=True)\n",
        "\n",
        "    # Window masks\n",
        "    tr_mask, te_mask = make_window_masks(dfw, train_start, train_end, test_start, test_end)\n",
        "\n",
        "    # Slice\n",
        "    train_df = dfw.loc[tr_mask, [\"date\"] + features].dropna()\n",
        "    test_df  = dfw.loc[te_mask, [\"date\"] + features].dropna()\n",
        "\n",
        "    if train_df.empty or test_df.empty:\n",
        "        raise ValueError(\n",
        "            f\"Empty train/test after slicing: \"\n",
        "            f\"train({train_start}â†’{train_end}) rows={train_df.shape[0]}, \"\n",
        "            f\"test({test_start}â†’{test_end}) rows={test_df.shape[0]}. \"\n",
        "            f\"Consider adjusting dates or breadth_shift_days.\"\n",
        "        )\n",
        "\n",
        "    # Standardize on TRAIN ONLY; transform TEST with same scaler\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(train_df[features].to_numpy(dtype=float))\n",
        "    X_test  = scaler.transform(test_df[features].to_numpy(dtype=float))\n",
        "\n",
        "    # Persist scaler per window\n",
        "    if scaler_out_path is None:\n",
        "        win_tag = f\"{train_start}_{train_end}__{test_start}_{test_end}\".replace(\"-\", \"\")\n",
        "        scaler_out_path = os.path.join(REGIME_DIR, f\"scaler_{win_tag}.joblib\")\n",
        "    joblib.dump(scaler, scaler_out_path)\n",
        "\n",
        "    # Quick QC: mean/var drift (train vs test) and feature coverage\n",
        "    qc = {\n",
        "        \"train_rows\": int(train_df.shape[0]),\n",
        "        \"test_rows\": int(test_df.shape[0]),\n",
        "        \"features\": features,\n",
        "        \"train_means\": dict(zip(features, np.mean(X_train, axis=0).round(6).tolist())),\n",
        "        \"train_stds\": dict(zip(features, np.std(X_train, axis=0, ddof=0).round(6).tolist())),\n",
        "        \"test_means\": dict(zip(features, np.mean(X_test, axis=0).round(6).tolist())),\n",
        "        \"test_stds\": dict(zip(features, np.std(X_test, axis=0, ddof=0).round(6).tolist())),\n",
        "    }\n",
        "\n",
        "    # Save a tiny per-window QC file\n",
        "    win_qc_path = scaler_out_path.replace(\".joblib\", \"_qc.json\")\n",
        "    with open(win_qc_path, \"w\") as f:\n",
        "        json.dump(qc, f, indent=2)\n",
        "\n",
        "    return {\n",
        "        \"X_train\": X_train,\n",
        "        \"X_test\": X_test,\n",
        "        \"dates_train\": train_df[\"date\"].to_list(),\n",
        "        \"dates_test\": test_df[\"date\"].to_list(),\n",
        "        \"scaler_path\": scaler_out_path,\n",
        "        \"scaler_mean\": scaler.mean_.round(12).tolist(),\n",
        "        \"scaler_scale\": scaler.scale_.round(12).tolist(),\n",
        "        \"qc\": qc,\n",
        "    }\n",
        "\n",
        "# Example: pick a first walk-forward split anchored to your warmup_cutoff\n",
        "# You can replace these with your Section 6 generator later.\n",
        "train_start = \"2007-02-06\"  # day after warmup_cutoff in your QC\n",
        "train_end   = \"2016-12-30\"\n",
        "test_start  = \"2017-01-03\"\n",
        "test_end    = mkt[\"date\"].max().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "window = build_hmm_matrices(\n",
        "    df=mkt,\n",
        "    features=hmm_feat_cols,\n",
        "    train_start=train_start,\n",
        "    train_end=train_end,\n",
        "    test_start=test_start,\n",
        "    test_end=test_end,\n",
        "    scaler_out_path=None,\n",
        "    breadth_shift_days=0,  # set to 1 if you confirm breadth must be known-at-t from t-1\n",
        ")\n",
        "\n",
        "# Persist a small window manifest so later steps (2.2+) can load it\n",
        "manifest = {\n",
        "    \"window\": {\n",
        "        \"train_start\": train_start,\n",
        "        \"train_end\": train_end,\n",
        "        \"test_start\": test_start,\n",
        "        \"test_end\": test_end,\n",
        "    },\n",
        "    \"features\": hmm_feat_cols,\n",
        "    \"scaler_path\": window[\"scaler_path\"],\n",
        "    \"n_train\": len(window[\"dates_train\"]),\n",
        "    \"n_test\": len(window[\"dates_test\"]),\n",
        "}\n",
        "with open(os.path.join(REGIME_DIR, \"window_manifest.json\"), \"w\") as f:\n",
        "    json.dump(manifest, f, indent=2)\n",
        "\n",
        "print(json.dumps({\n",
        "    \"status\": \"2.1 ready\",\n",
        "    \"features_used\": hmm_feat_cols,\n",
        "    \"scaler_saved\": window[\"scaler_path\"],\n",
        "    \"train_rows\": manifest[\"n_train\"],\n",
        "    \"test_rows\": manifest[\"n_test\"],\n",
        "}, indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfBUiQIZvMAS",
        "outputId": "9602d347-e5b1-472f-8713-5d53fe002555"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"status\": \"2.1 ready\",\n",
            "  \"features_used\": [\n",
            "    \"spy_rv_20\",\n",
            "    \"vix_close\",\n",
            "    \"breadth\",\n",
            "    \"dvix\"\n",
            "  ],\n",
            "  \"scaler_saved\": \"artifacts/regimes/scaler_20070206_20161230__20170103_20250808.joblib\",\n",
            "  \"train_rows\": 2495,\n",
            "  \"test_rows\": 2162\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hmmlearn --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQf0An0h0TWS",
        "outputId": "292a7bd9-6726-4691-c91b-e3a085555e75"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/165.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m122.9/165.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m165.9/165.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Section 2.2 â€” Model Choice & Configuration (Gaussian HMM)\n",
        "# Primary: GaussianHMM (full covariance), K in {2,3}\n",
        "# - Multiple restarts; pick best train log-likelihood\n",
        "# - Sticky transitions (Dirichlet-like persistence) via diagonal bias\n",
        "# - Finance recency weighting: time-decayed sub-sequences (ENABLED)\n",
        "# Reuses:\n",
        "#   - artifacts/regimes/market_panel.parquet (from 2.0)\n",
        "#   - artifacts/regimes/window_manifest.json (from 2.1)\n",
        "#   - scaler_*.joblib (from 2.1)\n",
        "# Outputs:\n",
        "#   - artifacts/regimes/regime_hmm.pkl (joblib bundle: model + meta)\n",
        "#   - artifacts/regimes/hmm_kgrid.json (scores by K)\n",
        "# ============================================================\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import os\n",
        "import json\n",
        "from typing import Dict, Any, List, Tuple\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "from hmmlearn.hmm import GaussianHMM\n",
        "\n",
        "# Reuse config and paths from 2.0 / 2.1\n",
        "REGIME_DIR = CFG.regime_dir\n",
        "PANEL_PATH = os.path.join(REGIME_DIR, \"market_panel.parquet\")\n",
        "MANIFEST_PATH = os.path.join(REGIME_DIR, \"window_manifest.json\")\n",
        "META_PATH = os.path.join(REGIME_DIR, \"regime_meta.json\")\n",
        "\n",
        "assert os.path.exists(PANEL_PATH), f\"Missing market panel: {PANEL_PATH}\"\n",
        "assert os.path.exists(MANIFEST_PATH), f\"Missing window manifest: {MANIFEST_PATH}\"\n",
        "\n",
        "with open(MANIFEST_PATH, \"r\") as f:\n",
        "    MAN = json.load(f)\n",
        "\n",
        "mkt = pd.read_parquet(PANEL_PATH).sort_values(\"date\").reset_index(drop=True)\n",
        "mkt[\"date\"] = pd.to_datetime(mkt[\"date\"])\n",
        "\n",
        "scaler = joblib.load(MAN[\"scaler_path\"])\n",
        "features = MAN[\"features\"]\n",
        "assert all(c in mkt.columns for c in features), f\"Panel missing features: {set(features) - set(mkt.columns)}\"\n",
        "\n",
        "# Train/test windows\n",
        "train_start = pd.to_datetime(MAN[\"window\"][\"train_start\"])\n",
        "train_end   = pd.to_datetime(MAN[\"window\"][\"train_end\"])\n",
        "test_start  = pd.to_datetime(MAN[\"window\"][\"test_start\"])\n",
        "test_end    = pd.to_datetime(MAN[\"window\"][\"test_end\"])\n",
        "\n",
        "train_df = mkt[(mkt[\"date\"] >= train_start) & (mkt[\"date\"] <= train_end)][[\"date\"] + features].dropna().reset_index(drop=True)\n",
        "test_df  = mkt[(mkt[\"date\"] >= test_start)  & (mkt[\"date\"] <= test_end)][[\"date\"] + features].dropna().reset_index(drop=True)\n",
        "\n",
        "X_train = scaler.transform(train_df[features].to_numpy(dtype=float))\n",
        "X_test  = scaler.transform(test_df[features].to_numpy(dtype=float))\n",
        "dates_train = train_df[\"date\"].to_numpy()\n",
        "dates_test  = test_df[\"date\"].to_numpy()\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Hyperparameters â€” Test run now, bump for real run (marked TOCHANGE)\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "N_COMPONENTS_GRID = [2]   # TOCHANGE: [3] or [2,3] for real run\n",
        "N_ITER = 200              # TOCHANGE: 1000 for real run\n",
        "N_INIT = 2                # TOCHANGE: 10 for real run\n",
        "RANDOM_STATE = 42\n",
        "COVARIANCE_TYPE = \"full\"\n",
        "TOL = 1e-3                # TOCHANGE: 1e-4 for real run\n",
        "\n",
        "# Sticky transitions strength (Dirichlet-like, diagonal blend post-fit)\n",
        "# Larger -> stickier regimes (longer dwell times)\n",
        "LAMBDA_STICK = 0.15       # TOCHANGE: 0.30â€“0.50 for real run\n",
        "\n",
        "# Finance recency weighting â€” ENABLED\n",
        "APPLY_RECENCY = True\n",
        "HALF_LIFE_DAYS = 756      # ~3 years; keeps 2008 meaningful\n",
        "# TOCHANGE: try 504 (~2y, more recency), 756 (~3y, balanced), 1260 (~5y, less recency)\n",
        "\n",
        "EPSILON_FLOOR = 0.10      # ensures old episodes never get <10% of peak weight\n",
        "# TOCHANGE: 0.05â€“0.15 depending on how protective you want to be\n",
        "\n",
        "# For recency sampler (still lightweight in test; scale for real run)\n",
        "SEG_LEN = 60              # TOCHANGE: 90â€“120 for real run\n",
        "N_SEGMENTS = 80           # TOCHANGE: 200â€“400 for real run\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Utilities\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def _diag_sticky_blend(transmat: np.ndarray, lam: float) -> np.ndarray:\n",
        "    k = transmat.shape[0]\n",
        "    T = (1.0 - lam) * transmat + lam * np.eye(k)\n",
        "    T = T / T.sum(axis=1, keepdims=True)\n",
        "    return T\n",
        "\n",
        "def _build_time_decay_weights(dates: np.ndarray, half_life_days: int) -> np.ndarray:\n",
        "    t = np.array([pd.Timestamp(d).toordinal() for d in dates], dtype=float)\n",
        "    age = (t.max() - t)  # newer dates -> smaller age\n",
        "    decay = np.log(2) / max(1, half_life_days)\n",
        "    w = np.exp(-decay * age)\n",
        "    return w / (w.sum() + 1e-12)\n",
        "\n",
        "def _sample_time_weighted_subsequences(\n",
        "    X: np.ndarray,\n",
        "    dates: np.ndarray,\n",
        "    seg_len: int,\n",
        "    n_segments: int,\n",
        "    half_life_days: int,\n",
        "    random_state: int,\n",
        ") -> Tuple[np.ndarray, List[int]]:\n",
        "    rng = np.random.RandomState(random_state)\n",
        "    n = X.shape[0]\n",
        "    if n < seg_len:\n",
        "        return X.copy(), [n]\n",
        "    ends = np.arange(seg_len - 1, n)\n",
        "    p = _build_time_decay_weights(dates[ends], half_life_days)\n",
        "    p = np.maximum(p, EPSILON_FLOOR * p.max())\n",
        "    p = p / p.sum()\n",
        "\n",
        "    chosen = rng.choice(ends, size=min(n_segments, len(ends)), replace=True, p=p)\n",
        "    lengths, chunks = [], []\n",
        "    for e in chosen:\n",
        "        s = e - (seg_len - 1)\n",
        "        chunk = X[s:e+1]\n",
        "        chunks.append(chunk)\n",
        "        lengths.append(len(chunk))\n",
        "    X_concat = np.vstack(chunks)\n",
        "    return X_concat, lengths\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Train HMMs; pick best by train log-likelihood\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "best = {\"score\": -np.inf, \"model\": None, \"k\": None, \"seed\": None, \"train_lengths\": None, \"fit_mode\": None}\n",
        "results = []\n",
        "\n",
        "for k in N_COMPONENTS_GRID:\n",
        "    for r in range(N_INIT):\n",
        "        seed = RANDOM_STATE + r\n",
        "\n",
        "        if APPLY_RECENCY:\n",
        "            X_fit, lengths = _sample_time_weighted_subsequences(\n",
        "                X_train, dates_train,\n",
        "                seg_len=SEG_LEN,\n",
        "                n_segments=N_SEGMENTS,\n",
        "                half_life_days=HALF_LIFE_DAYS,\n",
        "                random_state=seed,\n",
        "            )\n",
        "            fit_mode = \"recency\"\n",
        "        else:\n",
        "            X_fit, lengths = X_train, [len(X_train)]\n",
        "            fit_mode = \"plain\"\n",
        "\n",
        "        # Init model\n",
        "        model = GaussianHMM(\n",
        "            n_components=k,\n",
        "            covariance_type=COVARIANCE_TYPE,\n",
        "            n_iter=N_ITER,\n",
        "            tol=TOL,\n",
        "            random_state=seed,\n",
        "            verbose=False,\n",
        "            # IMPORTANT: do not include 's' or 't' here, otherwise your custom\n",
        "            # startprob_/transmat_ get overwritten on init.\n",
        "            init_params=\"mc\",      # means, covars only\n",
        "            params=\"stmc\",         # learn startprob, transmat, means, covars\n",
        "        )\n",
        "\n",
        "        # Sticky-biased initialization (near-diagonal)\n",
        "        trans0 = np.full((k, k), (1.0 - 0.90) / max(1, k - 1))\n",
        "        np.fill_diagonal(trans0, 0.90)\n",
        "        model.transmat_ = trans0\n",
        "\n",
        "        # Uniform start probabilities\n",
        "        model.startprob_ = np.full(k, 1.0 / k)\n",
        "\n",
        "        # Fit\n",
        "        model.fit(X_fit, lengths=lengths)\n",
        "\n",
        "        # Post-fit sticky blend (Dirichlet-like)\n",
        "        model.transmat_ = _diag_sticky_blend(model.transmat_, LAMBDA_STICK)\n",
        "\n",
        "        score = model.score(X_train)  # comparable scoring on original train sequence\n",
        "\n",
        "        results.append({\n",
        "            \"k\": k,\n",
        "            \"seed\": seed,\n",
        "            \"score\": float(score),\n",
        "            \"fit_mode\": fit_mode,\n",
        "            \"transmat\": model.transmat_.tolist(),\n",
        "        })\n",
        "\n",
        "        if score > best[\"score\"]:\n",
        "            best.update({\"score\": score, \"model\": model, \"k\": k, \"seed\": seed, \"train_lengths\": lengths, \"fit_mode\": fit_mode})\n",
        "\n",
        "# Save grid scores\n",
        "with open(os.path.join(REGIME_DIR, \"hmm_kgrid.json\"), \"w\") as f:\n",
        "    json.dump({\"results\": results, \"chosen\": {\"k\": best[\"k\"], \"seed\": best[\"seed\"], \"score\": float(best[\"score\"]), \"fit_mode\": best[\"fit_mode\"]}}, f, indent=2)\n",
        "\n",
        "# Persist best model bundle\n",
        "bundle = {\n",
        "    \"model\": best[\"model\"],\n",
        "    \"k\": best[\"k\"],\n",
        "    \"random_state\": best[\"seed\"],\n",
        "    \"features\": features,\n",
        "    \"scaler_path\": MAN[\"scaler_path\"],\n",
        "    \"train_dates\": [str(d) for d in dates_train],\n",
        "    \"test_dates\": [str(d) for d in dates_test],\n",
        "    \"fit_mode\": best[\"fit_mode\"],\n",
        "    \"sticky_lambda\": LAMBDA_STICK,\n",
        "    \"n_iter\": N_ITER,\n",
        "    \"n_init\": N_INIT,\n",
        "    \"tol\": TOL,\n",
        "    \"covariance_type\": COVARIANCE_TYPE,\n",
        "    \"recency_weighting\": True,  # enabled\n",
        "    \"recency_half_life_days\": HALF_LIFE_DAYS,\n",
        "    \"recency_seg_len\": SEG_LEN,         # TOCHANGE: 90â€“120\n",
        "    \"recency_n_segments\": N_SEGMENTS,   # TOCHANGE: 200â€“400\n",
        "    \"created_at\": datetime.utcnow().isoformat() + \"Z\",\n",
        "    \"recency_epsilon_floor\": EPSILON_FLOOR\n",
        "}\n",
        "joblib.dump(bundle, os.path.join(REGIME_DIR, \"regime_hmm.pkl\"))\n",
        "\n",
        "print(json.dumps({\n",
        "    \"status\": \"2.2 trained\",\n",
        "    \"chosen_k\": best[\"k\"],\n",
        "    \"fit_mode\": best[\"fit_mode\"],\n",
        "    \"train_score\": float(best[\"score\"]),\n",
        "    \"n_iter\": N_ITER,\n",
        "    \"n_init\": N_INIT,\n",
        "    \"sticky_lambda\": LAMBDA_STICK,\n",
        "    \"recency_weighting\": True,\n",
        "    \"half_life_days\": HALF_LIFE_DAYS,\n",
        "    \"seg_len\": SEG_LEN,\n",
        "    \"n_segments\": N_SEGMENTS,\n",
        "}, indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3ekvERhym7N",
        "outputId": "d3b9edf4-80b9-44c3-9e22-1af59764ec5d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"status\": \"2.2 trained\",\n",
            "  \"chosen_k\": 2,\n",
            "  \"fit_mode\": \"recency\",\n",
            "  \"train_score\": -8178.567260217938,\n",
            "  \"n_iter\": 200,\n",
            "  \"n_init\": 2,\n",
            "  \"sticky_lambda\": 0.15,\n",
            "  \"recency_weighting\": true,\n",
            "  \"half_life_days\": 756,\n",
            "  \"seg_len\": 60,\n",
            "  \"n_segments\": 80\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick peek at effective sampling weights (fixed timedelta math)\n",
        "ends = np.arange(SEG_LEN - 1, len(dates_train))\n",
        "end_dates = pd.to_datetime(dates_train[ends])\n",
        "\n",
        "pp = _build_time_decay_weights(end_dates, HALF_LIFE_DAYS)\n",
        "pp = np.maximum(pp, EPSILON_FLOOR * pp.max())\n",
        "pp = pp / pp.sum()\n",
        "\n",
        "# Age in *days* relative to the most recent end_date\n",
        "max_date = end_dates.max()\n",
        "ages_days = (max_date - end_dates) / np.timedelta64(1, \"D\")  # float days\n",
        "\n",
        "# Weighted mean age (how \"old\" the typical sampled segment end is)\n",
        "w_mean_age_days = float(np.sum(ages_days * pp))\n",
        "\n",
        "# 95% weight age: age threshold below which 95% of weight lies\n",
        "order = np.argsort(ages_days)                  # youngest â†’ oldest\n",
        "cumw = np.cumsum(pp[order])\n",
        "w95_idx = np.searchsorted(cumw, 0.95)\n",
        "w95_age_days = float(ages_days[order][min(w95_idx, len(ages_days)-1)])\n",
        "\n",
        "print({\n",
        "    \"weights_min\": float(pp.min()),\n",
        "    \"weights_max\": float(pp.max()),\n",
        "    \"weighted_mean_age_days\": w_mean_age_days,\n",
        "    \"weighted_p95_age_days\": w95_age_days,\n",
        "})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKJAsmdi85Ek",
        "outputId": "c7a8d8b3-12ee-46cf-ac7e-f8ed02ceecd1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'weights_min': 0.0001335955016895934, 'weights_max': 0.001335955016895934, 'weighted_mean_age_days': 1017.4276078929489, 'weighted_p95_age_days': 2990.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Section 2.3 â€” State Labeling & Semantics\n",
        "# - Score posteriors for all dates\n",
        "# - Profile states on TRAIN window only (no peeking)\n",
        "# - Label states: Risk-On (â†‘ret, â†“vol), Risk-Off (â†“ret, â†‘vol), Transition (rest)\n",
        "# - Persist labels, posteriors, and profiles\n",
        "# Outputs:\n",
        "#   artifacts/regimes/regime_labels.parquet (date, state_id, p0..pK, regime_label)\n",
        "#   artifacts/regimes/state_profiles.csv\n",
        "#   artifacts/regimes/regime_meta.json (updated label map)\n",
        "# ============================================================\n",
        "\n",
        "from __future__ import annotations\n",
        "import os, json\n",
        "from typing import Dict, Any\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from datetime import datetime\n",
        "\n",
        "REGIME_DIR = CFG.regime_dir\n",
        "PANEL_PATH = os.path.join(REGIME_DIR, \"market_panel.parquet\")\n",
        "MANIFEST_PATH = os.path.join(REGIME_DIR, \"window_manifest.json\")\n",
        "BUNDLE_PATH = os.path.join(REGIME_DIR, \"regime_hmm.pkl\")\n",
        "META_PATH = os.path.join(REGIME_DIR, \"regime_meta.json\")\n",
        "\n",
        "# Load artifacts\n",
        "mkt = pd.read_parquet(PANEL_PATH).sort_values(\"date\").reset_index(drop=True)\n",
        "mkt[\"date\"] = pd.to_datetime(mkt[\"date\"])\n",
        "with open(MANIFEST_PATH, \"r\") as f:\n",
        "    MAN = json.load(f)\n",
        "bundle = joblib.load(BUNDLE_PATH)\n",
        "model = bundle[\"model\"]\n",
        "features = bundle[\"features\"]\n",
        "scaler = joblib.load(bundle[\"scaler_path\"])\n",
        "\n",
        "# Prepare matrices for ALL dates (but label semantics computed on TRAIN ONLY)\n",
        "X_all = scaler.transform(mkt[features].to_numpy(dtype=float))\n",
        "dates_all = mkt[\"date\"].to_numpy()\n",
        "\n",
        "# Score posteriors\n",
        "post = model.predict_proba(X_all)  # shape: (T, K)\n",
        "states_argmax = post.argmax(axis=1)\n",
        "K = post.shape[1]\n",
        "\n",
        "# Train/test masks\n",
        "train_start = pd.to_datetime(MAN[\"window\"][\"train_start\"])\n",
        "train_end   = pd.to_datetime(MAN[\"window\"][\"train_end\"])\n",
        "test_start  = pd.to_datetime(MAN[\"window\"][\"test_start\"])\n",
        "test_end    = pd.to_datetime(MAN[\"window\"][\"test_end\"])\n",
        "train_mask = (mkt[\"date\"] >= train_start) & (mkt[\"date\"] <= train_end)\n",
        "test_mask  = (mkt[\"date\"] >= test_start)  & (mkt[\"date\"] <= test_end)\n",
        "\n",
        "# Helper: posterior-weighted stats on TRAIN window\n",
        "def weighted_mean(x, w):\n",
        "    w = np.asarray(w, dtype=float)\n",
        "    x = np.asarray(x, dtype=float)\n",
        "    s = w.sum()\n",
        "    return float((x * w).sum() / s) if s > 0 else np.nan\n",
        "\n",
        "def weighted_std(x, w):\n",
        "    mu = weighted_mean(x, w)\n",
        "    w = np.asarray(w, dtype=float)\n",
        "    x = np.asarray(x, dtype=float)\n",
        "    s = w.sum()\n",
        "    if s <= 1: return np.nan\n",
        "    var = ((w * (x - mu)**2).sum()) / s\n",
        "    return float(np.sqrt(max(var, 0.0)))\n",
        "\n",
        "def weighted_quantile(x, w, q=0.05):\n",
        "    x = np.asarray(x, dtype=float)\n",
        "    w = np.asarray(w, dtype=float)\n",
        "    order = np.argsort(x)\n",
        "    x_sorted, w_sorted = x[order], w[order]\n",
        "    cw = np.cumsum(w_sorted)\n",
        "    if cw[-1] == 0: return np.nan\n",
        "    return float(x_sorted[np.searchsorted(cw, q * cw[-1])])\n",
        "\n",
        "# Compute per-state profiles on TRAIN\n",
        "train_ix = np.where(train_mask.values)[0]\n",
        "has_dvix = \"dvix\" in mkt.columns\n",
        "profiles = []\n",
        "for s in range(K):\n",
        "    w = post[train_ix, s]\n",
        "    if w.sum() == 0:\n",
        "        mu_ret = mu_vol = mu_vix = mu_brd = q05 = np.nan\n",
        "        sd_ret = np.nan\n",
        "    else:\n",
        "        mu_ret = weighted_mean(mkt.loc[train_mask, \"spy_ret\"].values, w)\n",
        "        sd_ret = weighted_std(mkt.loc[train_mask, \"spy_ret\"].values, w)\n",
        "        mu_vol = weighted_mean(mkt.loc[train_mask, \"spy_rv_20\"].values, w)\n",
        "        mu_vix = weighted_mean(mkt.loc[train_mask, \"vix_close\"].values, w)\n",
        "        mu_brd = weighted_mean(mkt.loc[train_mask, \"breadth\"].values, w)\n",
        "        mu_dvix = weighted_mean(mkt.loc[train_mask, \"dvix\"].values, w) if has_dvix else np.nan\n",
        "        q05    = weighted_quantile(mkt.loc[train_mask, \"spy_ret\"].values, w, q=0.05)\n",
        "\n",
        "    profiles.append({\n",
        "        \"state_id\": s,\n",
        "        \"ret_mean\": mu_ret,\n",
        "        \"ret_std\": sd_ret,\n",
        "        \"rv20_mean\": mu_vol,\n",
        "        \"vix_mean\": mu_vix,\n",
        "        \"dvix_mean\": mu_dvix if has_dvix else np.nan,\n",
        "        \"breadth_mean\": mu_brd,\n",
        "        \"ret_q05\": q05,\n",
        "    })\n",
        "\n",
        "prof_df = pd.DataFrame(profiles)\n",
        "\n",
        "# Labeling rules (train window only, no peeking):\n",
        "#  - Risk-On: highest mean return, lowest vol (tie-breakers help if ambiguous)\n",
        "#  - Risk-Off: highest vol, lowest mean return (tie-breakers help if ambiguous)\n",
        "#  - Transition: whichever state is not assigned above\n",
        "# Primary ranks\n",
        "ret_rank = prof_df[\"ret_mean\"].rank(method=\"dense\")                        # higher = better\n",
        "# For clarity: choose Risk-Off by *highest* rv20 (vol spike)\n",
        "risk_off_id = int(prof_df[\"rv20_mean\"].idxmax())                           # highest vol\n",
        "risk_on_id  = int(ret_rank.idxmax())                                       # highest return\n",
        "\n",
        "# Tie-breaker refinement (only if they collide or look ambiguous)\n",
        "# Risk-On tie-breakers: breadthâ†‘, VIXâ†“, tail q05â†‘\n",
        "# Risk-Off tie-breakers: volâ†‘, retâ†“, Î”VIXâ†‘, breadthâ†“\n",
        "def _best_risk_on_row(df: pd.DataFrame) -> int:\n",
        "    score = (\n",
        "        df[\"breadth_mean\"].fillna(-1.0).rank(method=\"dense\", ascending=False) +\n",
        "        df[\"vix_mean\"].fillna(np.inf).rank(method=\"dense\", ascending=True) +\n",
        "        df[\"ret_q05\"].fillna(-np.inf).rank(method=\"dense\", ascending=False)\n",
        "    )\n",
        "    return int(score.idxmax())\n",
        "\n",
        "def _best_risk_off_row(df: pd.DataFrame) -> int:\n",
        "    score = (\n",
        "        df[\"rv20_mean\"].fillna(-np.inf).rank(method=\"dense\", ascending=False) +\n",
        "        df[\"ret_mean\"].fillna(np.inf).rank(method=\"dense\", ascending=True) +\n",
        "        (df[\"dvix_mean\"] if \"dvix_mean\" in df.columns else pd.Series(0.0, index=df.index)).fillna(0.0).rank(method=\"dense\", ascending=False) +\n",
        "        df[\"breadth_mean\"].fillna(np.inf).rank(method=\"dense\", ascending=True)\n",
        "    )\n",
        "    return int(score.idxmax())\n",
        "\n",
        "if risk_on_id == risk_off_id:\n",
        "    risk_on_id  = _best_risk_on_row(prof_df)\n",
        "    risk_off_id = _best_risk_off_row(prof_df)\n",
        "    # Safety: if still colliding (extremely rare), force Risk-Off = highest vol, Risk-On = highest return\n",
        "    if risk_on_id == risk_off_id:\n",
        "        risk_off_id = int(prof_df[\"rv20_mean\"].idxmax())\n",
        "        risk_on_id  = int(prof_df[\"ret_mean\"].idxmax())\n",
        "\n",
        "# Final label map\n",
        "label_map = {risk_on_id: \"Risk-On\", risk_off_id: \"Risk-Off\"}\n",
        "for s in range(K):\n",
        "    if s not in label_map:\n",
        "        label_map[s] = \"Transition\"\n",
        "\n",
        "# ---------- Build outputs USING the FINAL label_map ----------\n",
        "out = mkt[[\"date\"]].copy()\n",
        "out[\"state_id\"] = post.argmax(axis=1)\n",
        "for s in range(K):\n",
        "    out[f\"p{s}\"] = post[:, s]\n",
        "out[\"regime_label\"] = out[\"state_id\"].map(label_map)\n",
        "\n",
        "out_path = os.path.join(REGIME_DIR, \"regime_labels.parquet\")\n",
        "out.to_parquet(out_path, index=False)\n",
        "out.to_csv(os.path.join(REGIME_DIR, \"regime_labels.csv\"), index=False)\n",
        "\n",
        "prof_df.to_csv(os.path.join(REGIME_DIR, \"state_profiles.csv\"), index=False)\n",
        "\n",
        "# ---------- Update meta WITH the FINAL label_map ----------\n",
        "if os.path.exists(META_PATH):\n",
        "    with open(META_PATH, \"r\") as f:\n",
        "        meta = json.load(f)\n",
        "else:\n",
        "    meta = {}\n",
        "\n",
        "meta.setdefault(\"created_at\", datetime.utcnow().isoformat() + \"Z\")\n",
        "meta.setdefault(\"config\", {})\n",
        "meta.setdefault(\"diagnostics\", {})\n",
        "meta[\"diagnostics\"][\"state_profiles_train\"] = prof_df.to_dict(orient=\"records\")\n",
        "meta[\"state_label_map\"] = {int(k): v for k, v in label_map.items()}\n",
        "meta.setdefault(\"features_used\", features)\n",
        "meta[\"notes\"] = meta.get(\"notes\", []) + [\n",
        "    \"State labeling computed on train window only (no peeking).\",\n",
        "    \"Risk-On: highest mean ret & lowest vol; Risk-Off: highest vol & lowest ret; else Transition.\",\n",
        "    \"Tie-breakers: breadthâ†‘, VIXâ†“, tail q05â†‘ (Risk-On); volâ†‘, retâ†“, Î”VIXâ†‘, breadthâ†“ (Risk-Off).\",\n",
        "]\n",
        "\n",
        "with open(META_PATH, \"w\") as f:\n",
        "    json.dump(meta, f, indent=2)\n",
        "\n",
        "print(json.dumps({\n",
        "    \"status\": \"2.3 labeled\",\n",
        "    \"k\": K,\n",
        "    \"label_map\": label_map,\n",
        "    \"profiles_path\": os.path.join(REGIME_DIR, \"state_profiles.csv\"),\n",
        "    \"labels_path\": out_path,\n",
        "}, indent=2))\n",
        "\n",
        "# Posteriors sanity\n",
        "assert np.allclose(post.sum(axis=1), 1.0, atol=1e-6), \"Posterior rows must sum to 1.\"\n",
        "assert not pd.isna(out[\"regime_label\"]).any(), \"All states must map to a regime label.\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1sBNFT58pPr",
        "outputId": "e92eb316-0c43-4840-ecd5-9027c01ce9c3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"status\": \"2.3 labeled\",\n",
            "  \"k\": 2,\n",
            "  \"label_map\": {\n",
            "    \"0\": \"Risk-On\",\n",
            "    \"1\": \"Risk-Off\"\n",
            "  },\n",
            "  \"profiles_path\": \"artifacts/regimes/state_profiles.csv\",\n",
            "  \"labels_path\": \"artifacts/regimes/regime_labels.parquet\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Section 2.4 â€” Smoothing, Persistence & Debounce\n",
        "# - Option: Viterbi most-likely path vs. posterior argmax\n",
        "# - Debounce: POSTERIOR_THRESH and MIN_DWELL_DAYS from config\n",
        "# - Gap handling: inherit last known regime (dates already market-days)\n",
        "# Reuses:\n",
        "#   - artifacts/regimes/market_panel.parquet (2.0)\n",
        "#   - artifacts/regimes/window_manifest.json (2.1)\n",
        "#   - artifacts/regimes/regime_hmm.pkl (2.2)\n",
        "#   - artifacts/regimes/regime_labels.parquet (2.3)\n",
        "# Outputs:\n",
        "#   - artifacts/regimes/regime_labels.parquet (updated with *_smoothed cols)\n",
        "#   - artifacts/regimes/regime_meta.json (updated diagnostics)\n",
        "#   - console summary of dwell-time stats\n",
        "# ============================================================\n",
        "\n",
        "from __future__ import annotations\n",
        "import os, json\n",
        "from typing import List, Dict, Any, Tuple\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "REGIME_DIR     = CFG.regime_dir\n",
        "PANEL_PATH     = os.path.join(REGIME_DIR, \"market_panel.parquet\")\n",
        "LABELS_PATH    = os.path.join(REGIME_DIR, \"regime_labels.parquet\")\n",
        "META_PATH      = os.path.join(REGIME_DIR, \"regime_meta.json\")\n",
        "BUNDLE_PATH    = os.path.join(REGIME_DIR, \"regime_hmm.pkl\")\n",
        "\n",
        "assert os.path.exists(PANEL_PATH),  f\"Missing market panel: {PANEL_PATH}\"\n",
        "assert os.path.exists(LABELS_PATH), f\"Missing labels from 2.3: {LABELS_PATH}\"\n",
        "assert os.path.exists(BUNDLE_PATH), f\"Missing HMM bundle: {BUNDLE_PATH}\"\n",
        "\n",
        "# --- Config knobs (extend 2.0 config if not present) ---\n",
        "P_THRESH   = getattr(CFG, \"posterior_thresh\", 0.55) # TOCHANGE: consider 0.60â€“0.65 for a stricter switch confirmation.\n",
        "MIN_DWELL  = getattr(CFG, \"min_dwell_days\", 3) #  # TOCHANGE: consider 5â€“10 to further reduce chattering.\n",
        "SMOOTH_MTH = getattr(CFG, \"smoothing_method\", \"posterior\")  # \"posterior\" | \"viterbi\" # TOCHANGE: try \"viterbi\" for the real run and compare dwell-time stats and chattering.\n",
        "\n",
        "# --- Load artifacts ---\n",
        "labels = pd.read_parquet(LABELS_PATH).sort_values(\"date\").reset_index(drop=True)\n",
        "bundle = joblib.load(BUNDLE_PATH)\n",
        "model  = bundle[\"model\"]\n",
        "features = bundle[\"features\"]\n",
        "scaler  = joblib.load(bundle[\"scaler_path\"])\n",
        "\n",
        "mkt = pd.read_parquet(PANEL_PATH).sort_values(\"date\").reset_index(drop=True)\n",
        "mkt[\"date\"] = pd.to_datetime(mkt[\"date\"])\n",
        "\n",
        "# sanity\n",
        "assert np.array_equal(labels[\"date\"].values, mkt[\"date\"].values), \"Date alignment mismatch between labels and market panel.\"\n",
        "\n",
        "# --- Choose base path: Viterbi vs posterior argmax ---\n",
        "# We need posteriors for thresholding either way; for Viterbi we re-score X_all.\n",
        "X_all = scaler.transform(mkt[features].to_numpy(dtype=float))\n",
        "post  = model.predict_proba(X_all)  # (T, K)\n",
        "K     = post.shape[1]\n",
        "\n",
        "if SMOOTH_MTH.lower() == \"viterbi\":\n",
        "    base_states = model.predict(X_all)     # most-likely state path\n",
        "else:\n",
        "    base_states = post.argmax(axis=1)      # raw posterior argmax (already in 2.3)\n",
        "\n",
        "# --- Debounce step 1: posterior threshold gating (no switch if low confidence) ---\n",
        "maxp = post.max(axis=1)\n",
        "debounce_states = np.array(base_states, dtype=int)\n",
        "for i in range(1, len(debounce_states)):\n",
        "    if debounce_states[i] != debounce_states[i-1]:\n",
        "        # require sufficient posterior confidence on the *new* state\n",
        "        if maxp[i] < P_THRESH:\n",
        "            debounce_states[i] = debounce_states[i-1]\n",
        "\n",
        "# --- Debounce step 2: enforce minimum dwell time by collapsing short runs ---\n",
        "def _runs(state_series: np.ndarray) -> List[Tuple[int,int,int]]:\n",
        "    \"\"\"Return list of (start_idx, end_idx_inclusive, state) runs.\"\"\"\n",
        "    out = []\n",
        "    s = 0\n",
        "    cur = state_series[0]\n",
        "    for i in range(1, len(state_series)):\n",
        "        if state_series[i] != cur:\n",
        "            out.append((s, i-1, cur))\n",
        "            s = i\n",
        "            cur = state_series[i]\n",
        "    out.append((s, len(state_series)-1, cur))\n",
        "    return out\n",
        "\n",
        "def _collapse_short_runs(states: np.ndarray, min_len: int, post: np.ndarray) -> np.ndarray:\n",
        "    arr = states.copy()\n",
        "    changed = True\n",
        "    # iterate until stable (collapsing can merge adjacent runs)\n",
        "    while changed:\n",
        "        changed = False\n",
        "        runs = _runs(arr)\n",
        "        for (s, e, st) in runs:\n",
        "            run_len = e - s + 1\n",
        "            if run_len < min_len:\n",
        "                # Candidate neighbors: previous and next, choose higher avg posterior over this segment\n",
        "                prev_state = runs[runs.index((s, e, st))-1][2] if runs.index((s, e, st)) > 0 else None\n",
        "                next_state = runs[runs.index((s, e, st))+1][2] if runs.index((s, e, st)) < len(runs)-1 else None\n",
        "\n",
        "                # If no neighbors (degenerate), skip\n",
        "                if prev_state is None and next_state is None:\n",
        "                    continue\n",
        "\n",
        "                # Compute average posterior for neighbors over the short segment\n",
        "                best_neighbor = None\n",
        "                best_score = -np.inf\n",
        "                for cand in [prev_state, next_state]:\n",
        "                    if cand is None:\n",
        "                        continue\n",
        "                    score = float(post[s:e+1, cand].mean())\n",
        "                    if score > best_score:\n",
        "                        best_score = score\n",
        "                        best_neighbor = cand\n",
        "                # Relabel the short run to best neighbor\n",
        "                arr[s:e+1] = best_neighbor\n",
        "                changed = True\n",
        "                break  # restart since runs have changed\n",
        "    return arr\n",
        "\n",
        "smoothed_states = _collapse_short_runs(debounce_states, MIN_DWELL, post)\n",
        "\n",
        "# --- Map to labels using the semantics from 2.3 (state_label_map) ---\n",
        "# read label map from meta\n",
        "if os.path.exists(META_PATH):\n",
        "    with open(META_PATH, \"r\") as f:\n",
        "        meta = json.load(f)\n",
        "else:\n",
        "    meta = {}\n",
        "\n",
        "state_label_map = meta.get(\"state_label_map\", None)\n",
        "if state_label_map is None:\n",
        "    # fallback to identity names if meta missing (shouldn't happen)\n",
        "    state_label_map = {int(s): f\"State{s}\" for s in range(K)}\n",
        "\n",
        "# Update labels DataFrame with smoothed outputs\n",
        "labels[\"state_id_smoothed\"] = smoothed_states\n",
        "for s in range(K):\n",
        "    # keep original p0..pK as-is from 2.3; they reflect the raw model posteriors\n",
        "    if f\"p{s}\" not in labels.columns:\n",
        "        labels[f\"p{s}\"] = post[:, s]\n",
        "\n",
        "labels[\"regime_label_smoothed\"] = labels[\"state_id_smoothed\"].map({int(k): v for k, v in state_label_map.items()})\n",
        "\n",
        "# --- Dwell-time diagnostics ---\n",
        "def _dwell_stats(states: np.ndarray) -> pd.DataFrame:\n",
        "    rr = _runs(states)\n",
        "    return pd.DataFrame({\n",
        "        \"state_id\": [st for (_,_,st) in rr],\n",
        "        \"run_len\":  [e - s + 1 for (s,e,_) in rr],\n",
        "    }).groupby(\"state_id\").agg(\n",
        "        median_run_length=(\"run_len\", \"median\"),\n",
        "        mean_run_length  =(\"run_len\", \"mean\"),\n",
        "        n_runs           =(\"run_len\", \"count\"),\n",
        "        max_run_length   =(\"run_len\", \"max\"),\n",
        "    ).reset_index()\n",
        "\n",
        "dwell_df = _dwell_stats(labels[\"state_id_smoothed\"].to_numpy())\n",
        "\n",
        "# --- Save updated labels back to disk ---\n",
        "labels.to_parquet(LABELS_PATH, index=False)\n",
        "labels.to_csv(LABELS_PATH.replace(\".parquet\", \".csv\"), index=False)\n",
        "\n",
        "# --- Update regime_meta.json diagnostics & config snapshot ---\n",
        "meta.setdefault(\"diagnostics\", {})\n",
        "meta[\"diagnostics\"][\"smoothing\"] = {\n",
        "    \"method\": SMOOTH_MTH,\n",
        "    \"posterior_thresh\": P_THRESH,\n",
        "    \"min_dwell_days\": MIN_DWELL,\n",
        "    \"dwell_stats\": dwell_df.to_dict(orient=\"records\"),\n",
        "}\n",
        "meta.setdefault(\"notes\", [])\n",
        "meta[\"notes\"] += [\n",
        "    \"2.4 smoothing applied with debounce (posterior threshold + min dwell).\",\n",
        "    \"If method='viterbi', base path is Viterbi; else posterior argmax.\",\n",
        "]\n",
        "# de-dup notes\n",
        "meta[\"notes\"] = list(dict.fromkeys(meta[\"notes\"]))\n",
        "\n",
        "with open(META_PATH, \"w\") as f:\n",
        "    json.dump(meta, f, indent=2)\n",
        "\n",
        "print(json.dumps({\n",
        "    \"status\": \"2.4 smoothed\",\n",
        "    \"method\": SMOOTH_MTH,\n",
        "    \"posterior_thresh\": P_THRESH,\n",
        "    \"min_dwell_days\": MIN_DWELL,\n",
        "    \"k\": K,\n",
        "    \"median_dwell_by_state\": {\n",
        "        int(r[\"state_id\"]): float(r[\"median_run_length\"]) for r in dwell_df.to_dict(orient=\"records\")\n",
        "    },\n",
        "    \"labels_path\": LABELS_PATH\n",
        "}, indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eo9ZowDyKJ-f",
        "outputId": "ac481b5b-84a1-473d-fb31-182772efe937"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"status\": \"2.4 smoothed\",\n",
            "  \"method\": \"posterior\",\n",
            "  \"posterior_thresh\": 0.55,\n",
            "  \"min_dwell_days\": 3,\n",
            "  \"k\": 2,\n",
            "  \"median_dwell_by_state\": {\n",
            "    \"0\": 33.5,\n",
            "    \"1\": 12.0\n",
            "  },\n",
            "  \"labels_path\": \"artifacts/regimes/regime_labels.parquet\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Section 2.5 â€” Robustness & Sensitivity\n",
        "# - K sensitivity: K âˆˆ {2,3}\n",
        "# - Feature sensitivity: drop-one/add-one variants\n",
        "# - Era stability: pre/post-2015 and 2020 crisis\n",
        "# - Bootstrap: block bootstrap label stability\n",
        "# Reuses:\n",
        "#   - artifacts/regimes/market_panel.parquet (2.0)\n",
        "#   - artifacts/regimes/window_manifest.json (2.1)\n",
        "#   - scaler_*.joblib (2.1)\n",
        "#   - artifacts/regimes/regime_hmm.pkl (2.2 baseline)\n",
        "#   - artifacts/regimes/regime_labels.parquet (2.3 baseline labels)\n",
        "# Outputs:\n",
        "#   - artifacts/regimes/regime_sensitivity.json\n",
        "# Notes:\n",
        "#   This is a light test pass; heavier settings are tagged with # TOCHANGE\n",
        "# ============================================================\n",
        "\n",
        "from __future__ import annotations\n",
        "import os, json\n",
        "from typing import Dict, Any, List, Tuple\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from hmmlearn.hmm import GaussianHMM\n",
        "\n",
        "REGIME_DIR  = CFG.regime_dir\n",
        "PANEL_PATH  = os.path.join(REGIME_DIR, \"market_panel.parquet\")\n",
        "MAN_PATH    = os.path.join(REGIME_DIR, \"window_manifest.json\")\n",
        "BUNDLE_PATH = os.path.join(REGIME_DIR, \"regime_hmm.pkl\")\n",
        "LABELS_PATH = os.path.join(REGIME_DIR, \"regime_labels.parquet\")\n",
        "OUT_PATH    = os.path.join(REGIME_DIR, \"regime_sensitivity.json\")\n",
        "\n",
        "assert os.path.exists(PANEL_PATH) and os.path.exists(MAN_PATH) and os.path.exists(BUNDLE_PATH)\n",
        "mkt = pd.read_parquet(PANEL_PATH).sort_values(\"date\").reset_index(drop=True)\n",
        "mkt[\"date\"] = pd.to_datetime(mkt[\"date\"])\n",
        "\n",
        "with open(MAN_PATH, \"r\") as f:\n",
        "    MAN = json.load(f)\n",
        "\n",
        "bundle   = joblib.load(BUNDLE_PATH)\n",
        "features_base = bundle[\"features\"]\n",
        "scaler   = joblib.load(bundle[\"scaler_path\"])\n",
        "k_base   = int(bundle[\"k\"])\n",
        "recency  = bool(bundle.get(\"recency_weighting\", True))\n",
        "hl_days  = int(bundle.get(\"recency_half_life_days\", 756))\n",
        "seg_len  = int(bundle.get(\"recency_seg_len\", 60))\n",
        "n_segs   = int(bundle.get(\"recency_n_segments\", 80))\n",
        "tol      = float(bundle.get(\"tol\", 1e-3))\n",
        "n_iter   = int(bundle.get(\"n_iter\", 200))         # TOCHANGE: 1000 for real run\n",
        "n_init   = int(bundle.get(\"n_init\", 2))           # TOCHANGE: 10 for real run\n",
        "covtype  = bundle.get(\"covariance_type\", \"full\")\n",
        "lam_stick= float(bundle.get(\"sticky_lambda\", 0.15))  # TOCHANGE: 0.30â€“0.50 for real run\n",
        "rand0    = int(bundle.get(\"random_state\", 42))\n",
        "\n",
        "train_start = pd.to_datetime(MAN[\"window\"][\"train_start\"])\n",
        "train_end   = pd.to_datetime(MAN[\"window\"][\"train_end\"])\n",
        "test_start  = pd.to_datetime(MAN[\"window\"][\"test_start\"])\n",
        "test_end    = pd.to_datetime(MAN[\"window\"][\"test_end\"])\n",
        "\n",
        "mask_train = (mkt[\"date\"] >= train_start) & (mkt[\"date\"] <= train_end)\n",
        "mask_test  = (mkt[\"date\"] >= test_start)  & (mkt[\"date\"] <= test_end)\n",
        "\n",
        "# â¬‡ï¸ ADD: tiny helper to fit a local scaler on the train (or era) subset\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def _fit_local_scaler(feats: List[str], subset_mask: pd.Series) -> StandardScaler:\n",
        "    df = mkt.loc[subset_mask, feats].dropna()\n",
        "    scaler_local = StandardScaler()\n",
        "    scaler_local.fit(df.to_numpy(dtype=float))\n",
        "    return scaler_local\n",
        "\n",
        "def _diag_sticky_blend(T: np.ndarray, lam: float) -> np.ndarray:\n",
        "    k = T.shape[0]\n",
        "    out = (1.0 - lam) * T + lam * np.eye(k)\n",
        "    return out / out.sum(axis=1, keepdims=True)\n",
        "\n",
        "def _build_time_decay_weights(dates: np.ndarray, half_life_days: int) -> np.ndarray:\n",
        "    t = np.array([pd.Timestamp(d).toordinal() for d in dates], dtype=float)\n",
        "    age = (t.max() - t)\n",
        "    decay = np.log(2) / max(1, half_life_days)\n",
        "    w = np.exp(-decay * age)\n",
        "    return w / (w.sum() + 1e-12)\n",
        "\n",
        "# Canonical recency-sampling params (align with 2.2 bundle keys)\n",
        "REC_SEG_LEN    = int(bundle.get(\"recency_seg_len\", 60))\n",
        "REC_N_SEGMENTS = int(bundle.get(\"recency_n_segments\", 80))\n",
        "REC_HALF_LIFE  = int(bundle.get(\"recency_half_life_days\", 756))\n",
        "REC_EPS        = float(bundle.get(\"recency_epsilon_floor\", 0.10))  # matches 2.2 key\n",
        "\n",
        "def _sample_time_weighted_subsequences(\n",
        "    X: np.ndarray,\n",
        "    dates: np.ndarray,\n",
        "    seg_len: int = REC_SEG_LEN,\n",
        "    n_segments: int = REC_N_SEGMENTS,\n",
        "    half_life_days: int = REC_HALF_LIFE,\n",
        "    seed: int = 42,\n",
        "):\n",
        "    rng = np.random.RandomState(seed)\n",
        "    n = X.shape[0]\n",
        "    if n < seg_len:\n",
        "        return X.copy(), [n]\n",
        "    ends = np.arange(seg_len - 1, n)\n",
        "    p = _build_time_decay_weights(dates[ends], half_life_days)\n",
        "    p = np.maximum(p, REC_EPS * p.max())\n",
        "    p = p / p.sum()\n",
        "    chosen = rng.choice(ends, size=min(n_segments, len(ends)), replace=True, p=p)\n",
        "    chunks, lengths = [], []\n",
        "    for e in chosen:\n",
        "        s = e - (seg_len - 1)\n",
        "        chunks.append(X[s:e+1])\n",
        "        lengths.append(seg_len)\n",
        "    return np.vstack(chunks), lengths\n",
        "\n",
        "# â¬‡ï¸ MODIFY: _fit_hmm_for_features now fits and returns a local scaler,\n",
        "# and uses it for both training and scoring.\n",
        "def _fit_hmm_for_features(feats: List[str], k: int, rs: int, subset_mask: pd.Series) -> Dict[str, Any]:\n",
        "    # Fit local scaler on the subset (train or era) to avoid feature-count mismatch\n",
        "    scaler_local = _fit_local_scaler(feats, subset_mask)\n",
        "\n",
        "    df = mkt.loc[subset_mask, [\"date\"] + feats].dropna().reset_index(drop=True)\n",
        "    dates = df[\"date\"].to_numpy()\n",
        "    X = scaler_local.transform(df[feats].to_numpy(dtype=float))\n",
        "\n",
        "    if recency:\n",
        "        X_fit, lengths = _sample_time_weighted_subsequences(\n",
        "            X, dates, seg_len=seg_len, n_segments=n_segs, half_life_days=hl_days, seed=rs\n",
        "        )\n",
        "    else:\n",
        "        X_fit, lengths = X, [len(X)]\n",
        "\n",
        "    model = GaussianHMM(\n",
        "        n_components=k,\n",
        "        covariance_type=covtype,\n",
        "        n_iter=n_iter,\n",
        "        tol=tol,\n",
        "        random_state=rs,\n",
        "        verbose=False,\n",
        "        init_params=\"mc\",   # means, covars\n",
        "        params=\"stmc\",      # learn startprob/transmat as well\n",
        "    )\n",
        "    # sticky-ish init\n",
        "    T0 = np.full((k, k), (1.0 - 0.90) / max(1, k - 1)); np.fill_diagonal(T0, 0.90)\n",
        "    model.transmat_ = T0\n",
        "    model.startprob_ = np.full(k, 1.0 / k)\n",
        "\n",
        "    model.fit(X_fit, lengths=lengths)\n",
        "    model.transmat_ = _diag_sticky_blend(model.transmat_, lam_stick)\n",
        "\n",
        "    # score on original (non-sampled) sequence\n",
        "    score = float(model.score(X))\n",
        "\n",
        "    return {\n",
        "        \"model\": model,\n",
        "        \"score\": score,\n",
        "        \"dates\": dates,\n",
        "        \"feats\": feats,\n",
        "        \"scaler\": scaler_local  # â¬…ï¸ return it\n",
        "    }\n",
        "\n",
        "# â¬‡ï¸ MODIFY: _profile_and_label takes the local scaler we fit above\n",
        "def _profile_and_label(model, feats: List[str], scaler_local: StandardScaler) -> Dict[str, Any]:\n",
        "    X_all = scaler_local.transform(mkt[feats].to_numpy(dtype=float))\n",
        "    post  = model.predict_proba(X_all)\n",
        "    K = post.shape[1]\n",
        "\n",
        "    # compute profiles on TRAIN ONLY (no peeking)\n",
        "    train_ix = np.where(mask_train.values)[0]\n",
        "\n",
        "    def wmean(x, w):\n",
        "        w = np.asarray(w); x = np.asarray(x); s = w.sum()\n",
        "        return float((x*w).sum()/s) if s>0 else np.nan\n",
        "    def wstd(x, w):\n",
        "        mu = wmean(x, w); w = np.asarray(w); x = np.asarray(x); s=w.sum()\n",
        "        if s<=1: return np.nan\n",
        "        return float(np.sqrt(max(((w*(x-mu)**2).sum()/s), 0.0)))\n",
        "    def wq05(x, w):\n",
        "        x=np.asarray(x); w=np.asarray(w); o=np.argsort(x); xs,ws=x[o],w[o]; cw=np.cumsum(ws)\n",
        "        return float(xs[np.searchsorted(cw, 0.05*cw[-1])]) if cw[-1]>0 else np.nan\n",
        "\n",
        "    prof = []\n",
        "    for s in range(K):\n",
        "        w = post[train_ix, s]\n",
        "        prof.append({\n",
        "            \"state_id\": s,\n",
        "            \"ret_mean\": wmean(mkt.loc[mask_train,\"spy_ret\"].values, w),\n",
        "            \"ret_std\":  wstd (mkt.loc[mask_train,\"spy_ret\"].values, w),\n",
        "            \"rv20_mean\": wmean(mkt.loc[mask_train,\"spy_rv_20\"].values, w),\n",
        "            \"vix_mean\":  wmean(mkt.loc[mask_train,\"vix_close\"].values, w),\n",
        "            \"dvix_mean\": wmean(mkt.loc[mask_train,\"dvix\"].values, w) if \"dvix\" in mkt.columns else np.nan,\n",
        "            \"breadth_mean\": wmean(mkt.loc[mask_train,\"breadth\"].values, w),\n",
        "            \"ret_q05\":  wq05 (mkt.loc[mask_train,\"spy_ret\"].values, w),\n",
        "        })\n",
        "    prof_df = pd.DataFrame(prof)\n",
        "\n",
        "    risk_off_id = int(prof_df[\"rv20_mean\"].idxmax())\n",
        "    risk_on_id  = int(prof_df[\"ret_mean\"].idxmax())\n",
        "    label_map = {risk_on_id: \"Risk-On\", risk_off_id: \"Risk-Off\"}\n",
        "    for s in range(K):\n",
        "        if s not in label_map:\n",
        "            label_map[s] = \"Transition\"\n",
        "\n",
        "    return {\n",
        "        \"profiles\": prof_df.to_dict(orient=\"records\"),\n",
        "        \"label_map\": {int(k): v for k,v in label_map.items()},\n",
        "        \"posteriors_shape\": list(post.shape),\n",
        "        \"transmat\": model.transmat_.tolist(),\n",
        "    }\n",
        "\n",
        "def _agreement_vs_baseline(new_states: np.ndarray, baseline_states: np.ndarray) -> float:\n",
        "    # simple percent agreement\n",
        "    if len(new_states) != len(baseline_states):\n",
        "        return np.nan\n",
        "    return float((new_states == baseline_states).mean())\n",
        "\n",
        "# --- Load baseline label sequence (we'll compare to *smoothed* if present) ---\n",
        "base_labels = pd.read_parquet(LABELS_PATH).sort_values(\"date\")\n",
        "base_labels[\"date\"] = pd.to_datetime(base_labels[\"date\"])  # <- add this\n",
        "\n",
        "if \"state_id_smoothed\" in base_labels.columns:\n",
        "    base_states = base_labels[\"state_id_smoothed\"].to_numpy()\n",
        "else:\n",
        "    base_states = base_labels[\"state_id\"].to_numpy()\n",
        "\n",
        "results: Dict[str, Any] = {\"k_sensitivity\": [], \"feature_sensitivity\": [], \"era_stability\": [], \"bootstrap\": {}}\n",
        "\n",
        "def _score_states_on_valid_dates(model, feats, scaler_local):\n",
        "    full_df = mkt[[\"date\"] + feats].dropna().reset_index(drop=True)\n",
        "    Xa = scaler_local.transform(full_df[feats].to_numpy(dtype=float))\n",
        "    states = model.predict_proba(Xa).argmax(axis=1)\n",
        "    return full_df[\"date\"].to_numpy(), states\n",
        "\n",
        "def _agreement_on_intersection(dates_new, states_new, base_labels_df) -> float:\n",
        "    df_new = pd.DataFrame({\"date\": dates_new, \"state_new\": states_new})\n",
        "    df_join = df_new.merge(\n",
        "        base_labels_df[[\"date\", \"state_id_smoothed\" if \"state_id_smoothed\" in base_labels_df.columns else \"state_id\"]]\n",
        "        .rename(columns={\"state_id_smoothed\":\"state_base\",\"state_id\":\"state_base\"}),\n",
        "        on=\"date\", how=\"inner\"\n",
        "    )\n",
        "    if len(df_join) == 0:\n",
        "        return np.nan\n",
        "    return float((df_join[\"state_new\"].to_numpy() == df_join[\"state_base\"].to_numpy()).mean())\n",
        "\n",
        "# 1) K sensitivity ------------------------------------------------------------\n",
        "K_GRID = [2, 3]  # TOCHANGE: can expand to [2,3] in real run if currently narrowed\n",
        "for k in K_GRID:\n",
        "    best = {\"score\": -np.inf, \"meta\": None}\n",
        "    for r in range(n_init):\n",
        "        rs = rand0 + r\n",
        "        fit = _fit_hmm_for_features(features_base, k, rs, mask_train)\n",
        "        meta = _profile_and_label(fit[\"model\"], features_base, fit[\"scaler\"])\n",
        "        dates_scored, states_all = _score_states_on_valid_dates(fit[\"model\"], features_base, fit[\"scaler\"])\n",
        "        agree = _agreement_on_intersection(dates_scored, states_all, base_labels)\n",
        "\n",
        "        entry = {\n",
        "            \"k\": k, \"seed\": rs, \"score\": fit[\"score\"], \"agreement_vs_baseline\": agree,\n",
        "            \"profiles\": meta[\"profiles\"], \"label_map\": meta[\"label_map\"],\n",
        "            \"transmat\": meta[\"transmat\"]\n",
        "        }\n",
        "        if fit[\"score\"] > best[\"score\"]:\n",
        "            best = {\"score\": fit[\"score\"], \"meta\": entry}\n",
        "    results[\"k_sensitivity\"].append(best[\"meta\"])\n",
        "\n",
        "# 2) Feature sensitivity -------------------------------------------------------\n",
        "# Define variants relative to baseline features\n",
        "fsets = []\n",
        "fsets.append((\"baseline\", features_base))\n",
        "if \"vix_close\" in features_base: fsets.append((\"no_vix\", [f for f in features_base if f!=\"vix_close\"]))\n",
        "if \"breadth\"   in features_base: fsets.append((\"no_breadth\", [f for f in features_base if f!=\"breadth\"]))\n",
        "if \"dvix\"      in features_base: fsets.append((\"no_dvix\", [f for f in features_base if f!=\"dvix\"]))\n",
        "# minimal core set\n",
        "core = [f for f in [\"spy_rv_20\",\"vix_close\"] if f in mkt.columns]\n",
        "if core: fsets.append((\"core_rv_vix\", core))\n",
        "\n",
        "for name, feats in fsets:\n",
        "    k = k_base\n",
        "    best = {\"score\": -np.inf, \"meta\": None}\n",
        "    for r in range(n_init):\n",
        "        rs = rand0 + 100 + r\n",
        "        fit = _fit_hmm_for_features(feats, k, rs, mask_train)\n",
        "        meta = _profile_and_label(fit[\"model\"], feats, fit[\"scaler\"])\n",
        "        dates_scored, states_all = _score_states_on_valid_dates(fit[\"model\"], feats, fit[\"scaler\"])\n",
        "        agree = _agreement_on_intersection(dates_scored, states_all, base_labels)\n",
        "        entry = {\n",
        "            \"feature_set\": name, \"k\": k, \"seed\": rs, \"feats\": feats,\n",
        "            \"score\": fit[\"score\"], \"agreement_vs_baseline\": agree,\n",
        "            \"profiles\": meta[\"profiles\"], \"label_map\": meta[\"label_map\"],\n",
        "            \"transmat\": meta[\"transmat\"]\n",
        "        }\n",
        "        if fit[\"score\"] > best[\"score\"]:\n",
        "            best = {\"score\": fit[\"score\"], \"meta\": entry}\n",
        "    results[\"feature_sensitivity\"].append(best[\"meta\"])\n",
        "\n",
        "\n",
        "# 3) Era stability -------------------------------------------------------------\n",
        "def _fit_on_era(start: str, end: str, k: int, seed: int, feats: List[str], name: str) -> Dict[str, Any]:\n",
        "    mask = (mkt[\"date\"] >= pd.to_datetime(start)) & (mkt[\"date\"] <= pd.to_datetime(end))\n",
        "    fit  = _fit_hmm_for_features(feats, k, seed, mask)\n",
        "    meta = _profile_and_label(fit[\"model\"], feats, fit[\"scaler\"])\n",
        "    return {\n",
        "        \"era\": name, \"k\": k, \"seed\": seed, \"score\": fit[\"score\"],\n",
        "        \"profiles\": meta[\"profiles\"], \"label_map\": meta[\"label_map\"],\n",
        "        \"transmat\": meta[\"transmat\"], \"start\": str(start), \"end\": str(end)\n",
        "    }\n",
        "\n",
        "\n",
        "# â¬‡ï¸ OPTIONAL TIDY-UP â€” Bootstrap: use a local scaler for baseline features too\n",
        "df_tr = mkt.loc[mask_train, [\"date\"] + features_base].dropna().reset_index(drop=True)\n",
        "scaler_base_local = StandardScaler().fit(df_tr[features_base].to_numpy(dtype=float))  # local\n",
        "X_tr  = scaler_base_local.transform(df_tr[features_base].to_numpy(dtype=float))\n",
        "dt_tr = df_tr[\"date\"].to_numpy()\n",
        "\n",
        "eras = [\n",
        "    (\"pre_2015\",  \"2007-02-06\", \"2014-12-31\"),\n",
        "    (\"post_2015\", \"2015-01-01\", str(train_end.date())),\n",
        "    (\"crisis_2020\",\"2020-02-15\",\"2020-12-31\"),\n",
        "]\n",
        "for name, s, e in eras:\n",
        "    rs = rand0 + hash(name) % 1000\n",
        "    results[\"era_stability\"].append(_fit_on_era(s, e, k_base, rs, features_base, name))\n",
        "\n",
        "\n",
        "# 4) Bootstrap (block) ---------------------------------------------------------\n",
        "def _block_bootstrap_indices(n: int, block: int, n_blocks: int, rng: np.random.RandomState):\n",
        "    starts = rng.randint(0, n, size=n_blocks)\n",
        "    idx = []\n",
        "    for st in starts:\n",
        "        idx.extend([(st + j) % n for j in range(block)])\n",
        "    return np.array(idx[:n])\n",
        "\n",
        "BOOT_REPS   = 5    # TOCHANGE: 100â€“300 for real run\n",
        "BLOCK_DAYS  = 20   # TOCHANGE: 20â€“60 depending on serial corr\n",
        "rng = np.random.RandomState(rand0+999)\n",
        "\n",
        "# Build train matrix for bootstrap using local scaler\n",
        "df_tr = mkt.loc[mask_train, [\"date\"] + features_base].dropna().reset_index(drop=True)\n",
        "scaler_base_local = StandardScaler().fit(df_tr[features_base].to_numpy(dtype=float))\n",
        "X_tr  = scaler_base_local.transform(df_tr[features_base].to_numpy(dtype=float))\n",
        "dt_tr = df_tr[\"date\"].to_numpy()\n",
        "\n",
        "boot_summ = {\"k\": k_base, \"reps\": BOOT_REPS, \"block_days\": BLOCK_DAYS, \"agreement_vs_baseline\": []}\n",
        "for b in range(BOOT_REPS):\n",
        "    idx = _block_bootstrap_indices(len(dt_tr), BLOCK_DAYS, max(1, len(dt_tr)//BLOCK_DAYS), rng)\n",
        "    Xb  = X_tr[idx]\n",
        "\n",
        "    model = GaussianHMM(\n",
        "        n_components=k_base, covariance_type=covtype, n_iter=n_iter, tol=tol,\n",
        "        random_state=rand0 + 500 + b, verbose=False, init_params=\"mc\", params=\"stmc\"\n",
        "    )\n",
        "    T0 = np.full((k_base, k_base), (1.0 - 0.90) / max(1, k_base - 1)); np.fill_diagonal(T0, 0.90)\n",
        "    model.transmat_ = T0; model.startprob_ = np.full(k_base, 1.0 / k_base)\n",
        "    model.fit(Xb, lengths=[len(Xb)])\n",
        "    model.transmat_ = _diag_sticky_blend(model.transmat_, lam_stick)\n",
        "\n",
        "    # Score on valid dates & align to baseline\n",
        "    dates_scored, states_all = _score_states_on_valid_dates(model, features_base, scaler_base_local)\n",
        "    agree = _agreement_on_intersection(dates_scored, states_all, base_labels)\n",
        "    boot_summ[\"agreement_vs_baseline\"].append(agree)\n",
        "\n",
        "boot_summ[\"agreement_mean\"] = float(np.mean(boot_summ[\"agreement_vs_baseline\"]))\n",
        "boot_summ[\"agreement_std\"]  = float(np.std (boot_summ[\"agreement_vs_baseline\"]))\n",
        "results[\"bootstrap\"] = boot_summ\n",
        "\n",
        "# Save results\n",
        "with open(OUT_PATH, \"w\") as f:\n",
        "    json.dump({\n",
        "        \"created_at\": datetime.utcnow().isoformat() + \"Z\",\n",
        "        \"inputs\": {\n",
        "            \"features_base\": features_base,\n",
        "            \"k_base\": k_base,\n",
        "            \"recency_weighting\": recency,\n",
        "            \"half_life_days\": hl_days,\n",
        "            \"lam_stick\": lam_stick,\n",
        "            \"n_iter\": n_iter, \"n_init\": n_init, \"tol\": tol, \"covariance_type\": covtype\n",
        "        },\n",
        "        \"results\": results\n",
        "    }, f, indent=2)\n",
        "\n",
        "print(json.dumps({\n",
        "    \"status\": \"2.5 done\",\n",
        "    \"out\": OUT_PATH,\n",
        "    \"k_choices\": [2,3],\n",
        "    \"feature_sets_tested\": [fs[0] for fs in fsets],\n",
        "    \"bootstrap\": {\"reps\": BOOT_REPS, \"agreement_mean\": boot_summ[\"agreement_mean\"], \"agreement_std\": boot_summ[\"agreement_std\"]},\n",
        "}, indent=2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGWBeVKSMGWt",
        "outputId": "6622e670-2eef-4d7b-dc71-f673f46c02c7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"status\": \"2.5 done\",\n",
            "  \"out\": \"artifacts/regimes/regime_sensitivity.json\",\n",
            "  \"k_choices\": [\n",
            "    2,\n",
            "    3\n",
            "  ],\n",
            "  \"feature_sets_tested\": [\n",
            "    \"baseline\",\n",
            "    \"no_vix\",\n",
            "    \"no_breadth\",\n",
            "    \"no_dvix\",\n",
            "    \"core_rv_vix\"\n",
            "  ],\n",
            "  \"bootstrap\": {\n",
            "    \"reps\": 5,\n",
            "    \"agreement_mean\": 0.53115739746618,\n",
            "    \"agreement_std\": 0.2478116873422969\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Section 2.6 â€” Diagnostics & QA\n",
        "# Plots:\n",
        "#   â€¢ Timeline with regime shading over SPY price (rebased) & drawdown\n",
        "#   â€¢ Posterior probabilities (stacked area)\n",
        "#   â€¢ State return histograms, QQ plots\n",
        "#   â€¢ Transition matrix heatmap, dwell-time distribution\n",
        "# Tables:\n",
        "#   â€¢ State profiles (load from 2.3), transition matrix & steady-state\n",
        "#   â€¢ Switch frequency & chattering metrics\n",
        "# Alerts:\n",
        "#   â€¢ Inconsistent semantics (e.g., positive mean but highest vol)\n",
        "#   â€¢ Very short dwell (median < 3d)\n",
        "#   â€¢ Mapping flips / excessive chattering\n",
        "# Reuses (do not recompute):\n",
        "#   - artifacts/regimes/market_panel.parquet (2.0)\n",
        "#   - artifacts/regimes/window_manifest.json (2.1)\n",
        "#   - artifacts/regimes/regime_hmm.pkl (2.2)\n",
        "#   - artifacts/regimes/regime_labels.parquet (2.3/2.4)\n",
        "#   - artifacts/regimes/state_profiles.csv (2.3)\n",
        "#   - artifacts/regimes/regime_meta.json (2.3/2.4)\n",
        "# Outputs:\n",
        "#   - artifacts/regimes/diagnostics/*.png\n",
        "#   - artifacts/regimes/diagnostics/*.csv / *.json\n",
        "#   - console summary + alerts\n",
        "# Notes:\n",
        "#   - #TOCHANGE marks spots to bump for real run (heavier plots/points)\n",
        "# ============================================================\n",
        "\n",
        "from __future__ import annotations\n",
        "import os, json\n",
        "from typing import Dict, Any, List, Tuple\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "REGIME_DIR  = CFG.regime_dir\n",
        "DIAG_DIR    = os.path.join(REGIME_DIR, \"diagnostics\")\n",
        "os.makedirs(DIAG_DIR, exist_ok=True)\n",
        "\n",
        "PANEL_PATH  = os.path.join(REGIME_DIR, \"market_panel.parquet\")\n",
        "MAN_PATH    = os.path.join(REGIME_DIR, \"window_manifest.json\")\n",
        "BUNDLE_PATH = os.path.join(REGIME_DIR, \"regime_hmm.pkl\")\n",
        "LABELS_PATH = os.path.join(REGIME_DIR, \"regime_labels.parquet\")\n",
        "META_PATH   = os.path.join(REGIME_DIR, \"regime_meta.json\")\n",
        "PROF_PATH   = os.path.join(REGIME_DIR, \"state_profiles.csv\")\n",
        "\n",
        "# --- Load artifacts\n",
        "assert os.path.exists(PANEL_PATH) and os.path.exists(MAN_PATH) and os.path.exists(BUNDLE_PATH) and os.path.exists(LABELS_PATH)\n",
        "\n",
        "mkt = pd.read_parquet(PANEL_PATH).sort_values(\"date\").reset_index(drop=True)\n",
        "mkt[\"date\"] = pd.to_datetime(mkt[\"date\"])\n",
        "labels = pd.read_parquet(LABELS_PATH).sort_values(\"date\").reset_index(drop=True)\n",
        "labels[\"date\"] = pd.to_datetime(labels[\"date\"])\n",
        "\n",
        "with open(MAN_PATH, \"r\") as f:\n",
        "    MAN = json.load(f)\n",
        "\n",
        "bundle = joblib.load(BUNDLE_PATH)\n",
        "features = bundle[\"features\"]\n",
        "k = int(bundle[\"k\"])\n",
        "\n",
        "meta = {}\n",
        "if os.path.exists(META_PATH):\n",
        "    with open(META_PATH, \"r\") as f:\n",
        "        meta = json.load(f)\n",
        "\n",
        "# --- Derived / convenience\n",
        "date_equal = np.array_equal(mkt[\"date\"].values, labels[\"date\"].values)\n",
        "if not date_equal:\n",
        "    # Align by inner-join on date (some rows may be dropped if any side had NA)\n",
        "    labels = labels.merge(mkt[[\"date\"]], on=\"date\", how=\"inner\").sort_values(\"date\").reset_index(drop=True)\n",
        "    mkt    = mkt.merge(labels[[\"date\"]], on=\"date\", how=\"inner\").sort_values(\"date\").reset_index(drop=True)\n",
        "\n",
        "# Prefer smoothed series if present\n",
        "state_col  = \"state_id_smoothed\" if \"state_id_smoothed\" in labels.columns else \"state_id\"\n",
        "label_col  = \"regime_label_smoothed\" if \"regime_label_smoothed\" in labels.columns else \"regime_label\"\n",
        "\n",
        "# K from posterior columns p0..pK-1 (robust to re-fits)\n",
        "p_cols = [c for c in labels.columns if c.startswith(\"p\")]\n",
        "K = len(p_cols) if len(p_cols) > 0 else k\n",
        "\n",
        "# --- Helpers\n",
        "def _runs(series: np.ndarray) -> List[Tuple[int,int,int]]:\n",
        "    \"\"\"Return (start_idx, end_idx, value) runs for integer state series.\"\"\"\n",
        "    out = []\n",
        "    s = 0; cur = series[0]\n",
        "    for i in range(1, len(series)):\n",
        "        if series[i] != cur:\n",
        "            out.append((s, i-1, int(cur)))\n",
        "            s = i; cur = series[i]\n",
        "    out.append((s, len(series)-1, int(cur)))\n",
        "    return out\n",
        "\n",
        "def _transition_matrix(states: np.ndarray, K: int) -> np.ndarray:\n",
        "    T = np.zeros((K, K), dtype=float)\n",
        "    for i in range(len(states)-1):\n",
        "        T[states[i], states[i+1]] += 1.0\n",
        "    row_sums = T.sum(axis=1, keepdims=True)\n",
        "    row_sums[row_sums==0] = 1.0\n",
        "    return T / row_sums\n",
        "\n",
        "def _steady_state(T: np.ndarray) -> np.ndarray:\n",
        "    # Empirical steady-state as left eigenvector (or fallback to state freq)\n",
        "    try:\n",
        "        vals, vecs = np.linalg.eig(T.T)\n",
        "        i = np.argmin(np.abs(vals - 1.0))\n",
        "        v = np.real(vecs[:, i]); v = np.maximum(v, 0)\n",
        "        if v.sum() == 0: raise ValueError\n",
        "        return v / v.sum()\n",
        "    except Exception:\n",
        "        return np.ones(T.shape[0]) / T.shape[0]\n",
        "\n",
        "def _rebase_price_from_returns(rets: np.ndarray, start=100.0) -> np.ndarray:\n",
        "    # Assumes rets are simple daily returns (e.g., spy_ret). If logrets, replace with exp(cumsum).\n",
        "    out = np.empty_like(rets, dtype=float); out[0] = start * (1.0 + np.nan_to_num(rets[0], nan=0.0))\n",
        "    for i in range(1, len(rets)):\n",
        "        out[i] = out[i-1] * (1.0 + np.nan_to_num(rets[i], nan=0.0))\n",
        "    return out\n",
        "\n",
        "def _drawdown(price: np.ndarray) -> np.ndarray:\n",
        "    cummax = np.maximum.accumulate(price)\n",
        "    dd = price / np.where(cummax==0, 1.0, cummax) - 1.0\n",
        "    return dd\n",
        "\n",
        "# --- Compute core diagnostics\n",
        "states = labels[state_col].to_numpy(dtype=int)\n",
        "Tmat   = _transition_matrix(states, K)\n",
        "ss_emp = _steady_state(Tmat)\n",
        "runs   = _runs(states)\n",
        "dwell  = pd.DataFrame({\n",
        "    \"state_id\": [st for (s,e,st) in runs],\n",
        "    \"run_len\":  [e-s+1 for (s,e,st) in runs],\n",
        "})\n",
        "\n",
        "# --- Switch/chattering metrics\n",
        "switches = (states[1:] != states[:-1]).sum()\n",
        "switch_rate = switches / max(1, len(states)-1)\n",
        "one_day_runs = (dwell[\"run_len\"] == 1).mean()  # fraction single-day\n",
        "lt3_runs = (dwell[\"run_len\"] < 3).mean()\n",
        "\n",
        "# --- Load state profiles (2.3) if present, else compute from TRAIN weights\n",
        "profiles_df = None\n",
        "if os.path.exists(PROF_PATH):\n",
        "    profiles_df = pd.read_csv(PROF_PATH)\n",
        "else:\n",
        "    # Fallback: rough unweighted per-state profiles on all data (not ideal, but avoids recompute)\n",
        "    tmp = []\n",
        "    for s in range(K):\n",
        "        mask = (states == s)\n",
        "        tmp.append({\n",
        "            \"state_id\": s,\n",
        "            \"ret_mean\": float(np.nanmean(mkt.loc[mask,\"spy_ret\"])),\n",
        "            \"ret_std\":  float(np.nanstd (mkt.loc[mask,\"spy_ret\"])),\n",
        "            \"rv20_mean\": float(np.nanmean(mkt.loc[mask,\"spy_rv_20\"])),\n",
        "            \"vix_mean\":  float(np.nanmean(mkt.loc[mask,\"vix_close\"])),\n",
        "            \"dvix_mean\": float(np.nanmean(mkt.loc[mask,\"dvix\"])) if \"dvix\" in mkt.columns else np.nan,\n",
        "            \"breadth_mean\": float(np.nanmean(mkt.loc[mask,\"breadth\"])),\n",
        "            \"ret_q05\":  float(np.nanquantile(mkt.loc[mask,\"spy_ret\"], 0.05)),\n",
        "        })\n",
        "    profiles_df = pd.DataFrame(tmp)\n",
        "profiles_df.to_csv(os.path.join(DIAG_DIR, \"state_profiles_table.csv\"), index=False)\n",
        "\n",
        "# --- Transition matrix & steady-state tables\n",
        "pd.DataFrame(Tmat, columns=[f\"to_{i}\" for i in range(K)], index=[f\"from_{i}\" for i in range(K)]) \\\n",
        "  .to_csv(os.path.join(DIAG_DIR, \"transition_matrix.csv\"))\n",
        "pd.DataFrame({\"state_id\": list(range(K)), \"steady_state_prob\": ss_emp}) \\\n",
        "  .to_csv(os.path.join(DIAG_DIR, \"steady_state.csv\"), index=False)\n",
        "\n",
        "# --- Switch frequency table (yearly)\n",
        "lab = labels[[\"date\", state_col]].copy()\n",
        "lab[\"year\"] = lab[\"date\"].dt.year\n",
        "lab[\"sw\"] = (lab[state_col].shift(-1) != lab[state_col]).astype(int)\n",
        "switch_by_year = lab.groupby(\"year\")[\"sw\"].sum().reset_index().rename(columns={\"sw\":\"n_switches\"})\n",
        "switch_by_year.to_csv(os.path.join(DIAG_DIR, \"switches_by_year.csv\"), index=False)\n",
        "\n",
        "# ============================================================\n",
        "# PLOTS\n",
        "# ============================================================\n",
        "\n",
        "# 1) Price timeline with regime shading & drawdown\n",
        "spy_price = _rebase_price_from_returns(mkt[\"spy_ret\"].to_numpy(dtype=float), start=100.0)\n",
        "spy_dd    = _drawdown(spy_price)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 5))\n",
        "ax.plot(mkt[\"date\"], spy_price, lw=1.25)\n",
        "# Shade by regime\n",
        "for (s,e,st) in runs:\n",
        "    ax.axvspan(mkt[\"date\"].iloc[s], mkt[\"date\"].iloc[e], alpha=0.15, label=f\"State {st}\" if s==runs[0][0] else None)\n",
        "ax.set_title(\"SPY (rebased) with Regime Shading\")\n",
        "ax.set_xlabel(\"Date\"); ax.set_ylabel(\"Rebased Price\")\n",
        "fig.tight_layout()\n",
        "fig.savefig(os.path.join(DIAG_DIR, \"regime_timeline.png\"), dpi=150)\n",
        "plt.close(fig)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 3))\n",
        "ax.plot(mkt[\"date\"], spy_dd, lw=1.0)\n",
        "ax.set_title(\"SPY Drawdown\")\n",
        "ax.set_xlabel(\"Date\"); ax.set_ylabel(\"Drawdown\")\n",
        "fig.tight_layout()\n",
        "fig.savefig(os.path.join(DIAG_DIR, \"timeline_drawdown.png\"), dpi=150)\n",
        "plt.close(fig)\n",
        "\n",
        "# 2) Posterior probabilities (stacked area)\n",
        "if len(p_cols) == K:\n",
        "    fig, ax = plt.subplots(figsize=(12, 4))\n",
        "    ax.stackplot(labels[\"date\"], *(labels[c].to_numpy() for c in p_cols))\n",
        "    ax.set_ylim(0,1); ax.set_title(\"Posterior Probabilities (Stacked)\")\n",
        "    ax.set_xlabel(\"Date\"); ax.set_ylabel(\"Probability\")\n",
        "    fig.tight_layout()\n",
        "    fig.savefig(os.path.join(DIAG_DIR, \"regime_posteriors.png\"), dpi=150)\n",
        "    plt.close(fig)\n",
        "\n",
        "\n",
        "# (erfinv helper without SciPy)\n",
        "def erfinv(y):\n",
        "    # Approximation (Winitzki) good enough for QQ visual; replace with SciPy in real run\n",
        "    a = 0.147\n",
        "    sign = np.sign(y)\n",
        "    x = np.clip(y, -0.999999, 0.999999)\n",
        "    ln = np.log(1 - x**2)\n",
        "    first = 2/(np.pi*a) + ln/2\n",
        "    return sign * np.sqrt( np.sqrt(first**2 - ln/a) - first )\n",
        "\n",
        "# 3) State return histograms + QQ plots\n",
        "# TOCHANGE: bump N_QQ_POINTS to 1000 for real run\n",
        "N_QQ_POINTS = 200\n",
        "qs = np.linspace(0.01, 0.99, N_QQ_POINTS)\n",
        "for s in range(K):\n",
        "    mask = (states == s)\n",
        "    r = mkt.loc[mask, \"spy_ret\"].dropna().to_numpy()\n",
        "    if len(r) == 0:\n",
        "        continue\n",
        "\n",
        "    # Histogram\n",
        "    fig, ax = plt.subplots(figsize=(5,3))\n",
        "    ax.hist(r, bins=40, alpha=0.8)  # #TOCHANGE: 80 bins for real run\n",
        "    ax.set_title(f\"State {s} return histogram\")\n",
        "    fig.tight_layout()\n",
        "    fig.savefig(os.path.join(DIAG_DIR, f\"state_{s}_ret_hist.png\"), dpi=150)\n",
        "    plt.close(fig)\n",
        "\n",
        "    # QQ vs normal\n",
        "    mu, sd = float(np.mean(r)), float(np.std(r, ddof=0))\n",
        "    if sd <= 0:\n",
        "        sd = 1e-9\n",
        "    emp_q = np.quantile(r, qs)\n",
        "    nor_q = mu + sd * np.sqrt(2) * erfinv(2*qs - 1)  # inverse CDF via erfinv\n",
        "    fig, ax = plt.subplots(figsize=(5,3))\n",
        "    ax.scatter(nor_q, emp_q, s=6, alpha=0.7)\n",
        "    lims = [min(nor_q.min(), emp_q.min()), max(nor_q.max(), emp_q.max())]\n",
        "    ax.plot(lims, lims, lw=1.0)\n",
        "    ax.set_title(f\"State {s} QQ vs Normal\")\n",
        "    ax.set_xlabel(\"Theoretical quantiles\"); ax.set_ylabel(\"Empirical quantiles\")\n",
        "    fig.tight_layout()\n",
        "    fig.savefig(os.path.join(DIAG_DIR, f\"state_{s}_qq.png\"), dpi=150)\n",
        "    plt.close(fig)\n",
        "\n",
        "\n",
        "# 4) Transition heatmap\n",
        "fig, ax = plt.subplots(figsize=(5,4))\n",
        "im = ax.imshow(Tmat, aspect=\"auto\", vmin=0, vmax=np.max(Tmat))\n",
        "ax.set_title(\"Transition Matrix\")\n",
        "ax.set_xlabel(\"to\"); ax.set_ylabel(\"from\")\n",
        "ax.set_xticks(range(K)); ax.set_yticks(range(K))\n",
        "fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
        "fig.tight_layout()\n",
        "fig.savefig(os.path.join(DIAG_DIR, \"transition_matrix_heatmap.png\"), dpi=150)\n",
        "plt.close(fig)\n",
        "\n",
        "# 5) Dwell-time distribution per state\n",
        "fig, ax = plt.subplots(figsize=(6,4))\n",
        "for s in range(K):\n",
        "    ax.hist(dwell.loc[dwell[\"state_id\"]==s, \"run_len\"], bins=range(1,51), alpha=0.6, label=f\"State {s}\")\n",
        "ax.legend()\n",
        "ax.set_title(\"Dwell-time distribution (days)\")\n",
        "ax.set_xlabel(\"Run length (days)\"); ax.set_ylabel(\"Count\")\n",
        "fig.tight_layout()\n",
        "fig.savefig(os.path.join(DIAG_DIR, \"dwell_time_distribution.png\"), dpi=150)\n",
        "plt.close(fig)\n",
        "\n",
        "# ============================================================\n",
        "# ALERTS\n",
        "# ============================================================\n",
        "alerts = []\n",
        "\n",
        "# Semantics: positive mean but highest vol -> suspicious \"Risk-On\"\n",
        "if profiles_df is not None and len(profiles_df) >= K:\n",
        "    vol_rank = profiles_df[\"rv20_mean\"].rank(ascending=True)  # 1 = lowest vol\n",
        "    hi_vol_state = int(profiles_df.loc[profiles_df[\"rv20_mean\"].idxmax(), \"state_id\"])\n",
        "    pos_mean_states = profiles_df.loc[profiles_df[\"ret_mean\"] > 0, \"state_id\"].astype(int).tolist()\n",
        "    if hi_vol_state in pos_mean_states and K >= 2:\n",
        "        alerts.append(f\"State {hi_vol_state}: positive mean return but highest realized vol (check semantics).\")\n",
        "\n",
        "# Dwell-time < 3 days median\n",
        "dwell_median = dwell.groupby(\"state_id\")[\"run_len\"].median()\n",
        "for s, med in dwell_median.items():\n",
        "    if med < 3:\n",
        "        alerts.append(f\"State {s}: median dwell {med}d < 3 (too chatty).\")\n",
        "\n",
        "# Chattering: high switch rate or many single-day runs\n",
        "if switch_rate > 0.15:  # #TOCHANGE: tighten to 0.10 for real run\n",
        "    alerts.append(f\"High switch rate: {switch_rate:.2%}\")\n",
        "if one_day_runs > 0.10:  # #TOCHANGE: tighten to 0.05 for real run\n",
        "    alerts.append(f\"Single-day run fraction elevated: {one_day_runs:.2%}\")\n",
        "\n",
        "# Mapping flips heuristic: compare label continuity around major drawdowns\n",
        "# (simple heuristic: if label changes >3 times within any 20-day window)\n",
        "# #TOCHANGE: widen window to 60 days for real run\n",
        "WINDOW = 20\n",
        "roll_switch = pd.Series((states[1:] != states[:-1]).astype(int)).rolling(WINDOW).sum().fillna(0)\n",
        "if (roll_switch > 3).any():\n",
        "    alerts.append(\"Frequent label flips in short windows (potential mapping instability).\")\n",
        "\n",
        "# Save alerts\n",
        "with open(os.path.join(DIAG_DIR, \"alerts.json\"), \"w\") as f:\n",
        "    json.dump({\"alerts\": alerts}, f, indent=2)\n",
        "\n",
        "# Save a compact summary CSV\n",
        "pd.DataFrame({\n",
        "    \"metric\": [\"K\", \"switches\", \"switch_rate\", \"one_day_runs_frac\", \"lt3_runs_frac\"],\n",
        "    \"value\": [K, switches, switch_rate, one_day_runs, lt3_runs]\n",
        "}).to_csv(os.path.join(DIAG_DIR, \"summary_metrics.csv\"), index=False)\n",
        "\n",
        "print(json.dumps({\n",
        "    \"status\": \"2.6 diagnostics complete\",\n",
        "    \"plots_dir\": DIAG_DIR,\n",
        "    \"alerts_count\": len(alerts),\n",
        "    \"notes\": [\n",
        "        \"State profiles loaded from 2.3 if available; else quick fallback was used.\",\n",
        "        \"Semantics checks are heuristics; confirm with 2.3 profiles and 2.5 sensitivity.\"\n",
        "    ]\n",
        "}, indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWuaoM4eZ57C",
        "outputId": "247d9753-f3d8-4304-e765-8eb6d739d761"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"status\": \"2.6 diagnostics complete\",\n",
            "  \"plots_dir\": \"artifacts/regimes/diagnostics\",\n",
            "  \"alerts_count\": 1,\n",
            "  \"notes\": [\n",
            "    \"State profiles loaded from 2.3 if available; else quick fallback was used.\",\n",
            "    \"Semantics checks are heuristics; confirm with 2.3 profiles and 2.5 sensitivity.\"\n",
            "  ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Section 2.7 â€” Regime-Aware Policy Hooks (Interfaces to Sec 3â€“5)\n",
        "# Reuses:\n",
        "#   - artifacts/regimes/regime_labels.parquet (2.3/2.4)\n",
        "#   - artifacts/regimes/regime_meta.json (2.3/2.4)\n",
        "#   - artifacts/regimes/regime_hmm.pkl (2.2)  [fallback if p-cols missing]\n",
        "#   - artifacts/regimes/window_manifest.json (2.1)\n",
        "#   - artifacts/regimes/market_panel.parquet (2.0)  [fallback scoring]\n",
        "# Outputs:\n",
        "#   - artifacts/regimes/regime_policy_map.json\n",
        "# Notes:\n",
        "#   - This file is the single interface consumed by Sections 3â€“5.\n",
        "#   - #TOCHANGE marks values to tune for the real run.\n",
        "# ============================================================\n",
        "\n",
        "from __future__ import annotations\n",
        "import os, json, hashlib\n",
        "from typing import Dict, Any\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "REGIME_DIR  = CFG.regime_dir\n",
        "PANEL_PATH  = os.path.join(REGIME_DIR, \"market_panel.parquet\")\n",
        "LABELS_PATH = os.path.join(REGIME_DIR, \"regime_labels.parquet\")\n",
        "META_PATH   = os.path.join(REGIME_DIR, \"regime_meta.json\")\n",
        "MAN_PATH    = os.path.join(REGIME_DIR, \"window_manifest.json\")\n",
        "BUNDLE_PATH = os.path.join(REGIME_DIR, \"regime_hmm.pkl\")\n",
        "OUT_PATH    = os.path.join(REGIME_DIR, \"regime_policy_map.json\")\n",
        "\n",
        "# --- Load essentials\n",
        "labels = pd.read_parquet(LABELS_PATH).sort_values(\"date\").reset_index(drop=True)\n",
        "labels[\"date\"] = pd.to_datetime(labels[\"date\"])\n",
        "with open(MAN_PATH, \"r\") as f: MAN = json.load(f)\n",
        "bundle = joblib.load(BUNDLE_PATH)\n",
        "\n",
        "# stateâ†’label semantics\n",
        "state_label_map = None\n",
        "if os.path.exists(META_PATH):\n",
        "    with open(META_PATH, \"r\") as f:\n",
        "        meta = json.load(f)\n",
        "    state_label_map = meta.get(\"state_label_map\", None)\n",
        "else:\n",
        "    meta = {}\n",
        "\n",
        "# infer K and get posteriors\n",
        "p_cols = [c for c in labels.columns if c.startswith(\"p\")]\n",
        "K = len(p_cols) if p_cols else int(bundle[\"k\"])\n",
        "\n",
        "# fallback: if no p-cols, score from model on all dates\n",
        "if not p_cols:\n",
        "    feats = bundle[\"features\"]\n",
        "    scaler = joblib.load(bundle[\"scaler_path\"])\n",
        "    mkt = pd.read_parquet(PANEL_PATH).sort_values(\"date\").reset_index(drop=True)\n",
        "    X_all = scaler.transform(mkt[feats].to_numpy(dtype=float))\n",
        "    post = bundle[\"model\"].predict_proba(X_all)\n",
        "    for s in range(post.shape[1]):\n",
        "        labels[f\"p{s}\"] = post[:, s]\n",
        "    p_cols = [f\"p{s}\" for s in range(K)]\n",
        "\n",
        "# choose smoothed ids/labels if available\n",
        "state_col = \"state_id_smoothed\" if \"state_id_smoothed\" in labels.columns else \"state_id\"\n",
        "label_col = \"regime_label_smoothed\" if \"regime_label_smoothed\" in labels.columns else \"regime_label\"\n",
        "\n",
        "# if meta has mapping but label_col missing, map on the fly\n",
        "if label_col not in labels.columns and state_label_map is not None:\n",
        "    labels[label_col] = labels[state_col].map({int(k): v for k, v in state_label_map.items()})\n",
        "\n",
        "# --- Confidence proxies\n",
        "def entropy(p: np.ndarray) -> float:\n",
        "    p = np.clip(p, 1e-12, 1.0)\n",
        "    return float(-(p*np.log(p)).sum() / np.log(len(p)))  # normalized to [0,1]\n",
        "\n",
        "def aggressiveness_from_confidence(p: np.ndarray) -> Dict[str, float]:\n",
        "    # Proxy 1: max posterior\n",
        "    c_max = float(p.max())\n",
        "    # Proxy 2: 1 - normalized entropy (higher -> more certain)\n",
        "    c_ent = 1.0 - entropy(p)\n",
        "    # Combine (simple mean)  #TOCHANGE: use weighted combo or monotone spline\n",
        "    c = 0.5 * (c_max + c_ent)\n",
        "    # Map to aggressiveness scalar g âˆˆ [g_min, g_max]\n",
        "    g_min, g_max = 0.35, 1.00     #TOCHANGE: (0.25,1.00) if you want deeper throttling\n",
        "    g = g_min + (g_max - g_min) * c\n",
        "    return {\"c_max\": c_max, \"c_entropy\": c_ent, \"c\": c, \"g\": g}\n",
        "\n",
        "# --- Latest regime & confidence (optionally smooth over last N days)\n",
        "#TOCHANGE: set N_SMOOTH=5â€“10 for prod; 1 for fast test\n",
        "N_SMOOTH = 3\n",
        "tail = labels.tail(N_SMOOTH)\n",
        "p_tail = tail[p_cols].to_numpy(dtype=float)\n",
        "p_mean = p_tail.mean(axis=0)\n",
        "latest_row = labels.iloc[-1]\n",
        "latest_label = str(latest_row[label_col]) if label_col in labels.columns else f\"State{int(latest_row[state_col])}\"\n",
        "conf = aggressiveness_from_confidence(p_mean)\n",
        "\n",
        "# --- Per-regime policy defaults (edit for your stack)\n",
        "# Use intuitive names; downstream can match by these labels\n",
        "# Turnover caps are relative (e.g., fraction of portfolio eligible to trade)\n",
        "policy_by_regime: Dict[str, Dict[str, Any]] = {\n",
        "    \"Risk-On\": {\n",
        "        \"weights_multipliers\": {          #TOCHANGE: tailor to your factors\n",
        "            \"momentum\": 1.20,\n",
        "            \"quality\":  1.00,\n",
        "            \"value\":    1.00,\n",
        "            \"low_vol\":  0.85,\n",
        "        },\n",
        "        \"turnover_cap\": 0.20,             #TOCHANGE: 0.25\n",
        "        \"risk_target_vol_annual\": 0.10,   # 10%\n",
        "        \"hedge_intensity\": 0.0,           # baseline hedge ratio\n",
        "    },\n",
        "    \"Transition\": {\n",
        "        \"weights_multipliers\": {\n",
        "            \"momentum\": 0.95,\n",
        "            \"quality\":  1.05,\n",
        "            \"value\":    1.05,\n",
        "            \"low_vol\":  1.05,\n",
        "        },\n",
        "        \"turnover_cap\": 0.15,\n",
        "        \"risk_target_vol_annual\": 0.08,   # 8%\n",
        "        \"hedge_intensity\": 0.15,\n",
        "    },\n",
        "    \"Risk-Off\": {\n",
        "        \"weights_multipliers\": {\n",
        "            \"momentum\": 0.70,             # throttle momo\n",
        "            \"quality\":  1.15,             # upweight quality/defensive\n",
        "            \"value\":    1.05,\n",
        "            \"low_vol\":  1.25,\n",
        "        },\n",
        "        \"turnover_cap\": 0.10,\n",
        "        \"risk_target_vol_annual\": 0.06,   # 6%\n",
        "        \"hedge_intensity\": 0.35,\n",
        "    },\n",
        "}\n",
        "\n",
        "# --- If our label universe differs (e.g., only 2 states), coerce keys\n",
        "present_labels = set(labels[label_col].dropna().astype(str).unique()) if label_col in labels.columns else set()\n",
        "for lbl in list(policy_by_regime.keys()):\n",
        "    if lbl not in present_labels and present_labels:\n",
        "        # map missing labels to a reasonable fallback  #TOCHANGE: make explicit mapping per run\n",
        "        del policy_by_regime[lbl]\n",
        "# If states are only numeric (no semantic labels), synthesize keys\n",
        "if not policy_by_regime and state_label_map is None:\n",
        "    unique_states = sorted(labels[state_col].unique())\n",
        "    for s in unique_states:\n",
        "        policy_by_regime[f\"State{s}\"] = {\n",
        "            \"weights_multipliers\": {\"momentum\":1.0,\"quality\":1.0,\"value\":1.0,\"low_vol\":1.0},\n",
        "            \"turnover_cap\": 0.15, \"risk_target_vol_annual\": 0.08, \"hedge_intensity\": 0.15,\n",
        "        }\n",
        "\n",
        "# --- Global scaling by confidence g (downstream can apply this linearly)\n",
        "# We expose both the raw confidence and recommend common scalings.\n",
        "scaling = {\n",
        "    \"aggressiveness_scalar_g\": conf[\"g\"],\n",
        "    \"confidence\": conf,                           # contains c_max, c_entropy, c (combined)\n",
        "    \"recommendations\": {\n",
        "        # Downstream usage suggestions\n",
        "        \"scale_position_sizes_by_g\": True,\n",
        "        \"scale_turnover_cap_by_g\": True,\n",
        "        \"scale_hedge_intensity_by_(1-g)\": True\n",
        "    }\n",
        "}\n",
        "\n",
        "# --- Package the full map\n",
        "out = {\n",
        "    \"created_at\": pd.Timestamp.utcnow().isoformat() + \"Z\",\n",
        "    \"latest_date\": str(latest_row[\"date\"].date()),\n",
        "    \"k\": int(K),\n",
        "    \"latest_regime_label\": latest_label,\n",
        "    \"latest_state_id\": int(latest_row[state_col]),\n",
        "    \"latest_posteriors\": {f\"p{s}\": float(latest_row.get(f\"p{s}\", np.nan)) for s in range(K)},\n",
        "    \"confidence\": scaling,\n",
        "    \"policy_by_regime\": policy_by_regime,\n",
        "    \"inputs\": {\n",
        "        \"labels_path\": LABELS_PATH,\n",
        "        \"meta_path\": META_PATH,\n",
        "        \"bundle_path\": BUNDLE_PATH,\n",
        "        \"scaler_path\": MAN[\"scaler_path\"],\n",
        "        \"features\": bundle[\"features\"],\n",
        "        \"window\": MAN.get(\"window\", {}),\n",
        "        \"smoothing_window_days\": N_SMOOTH,  #TOCHANGE\n",
        "    },\n",
        "}\n",
        "\n",
        "# include sensitivity & diagnostics pointers if present\n",
        "sens_path = os.path.join(REGIME_DIR, \"regime_sensitivity.json\")\n",
        "diag_dir  = os.path.join(REGIME_DIR, \"diagnostics\")\n",
        "if os.path.exists(sens_path):\n",
        "    out[\"inputs\"][\"sensitivity_path\"] = sens_path\n",
        "if os.path.isdir(diag_dir):\n",
        "    out[\"inputs\"][\"diagnostics_dir\"] = diag_dir\n",
        "\n",
        "# hash a minimal signature (useful for caching / auditing)\n",
        "sig = hashlib.sha256(json.dumps({\n",
        "    \"features\": out[\"inputs\"][\"features\"],\n",
        "    \"window\": out[\"inputs\"][\"window\"],\n",
        "    \"k\": out[\"k\"]\n",
        "}, sort_keys=True).encode()).hexdigest()\n",
        "out[\"signature\"] = sig\n",
        "\n",
        "with open(OUT_PATH, \"w\") as f:\n",
        "    json.dump(out, f, indent=2)\n",
        "\n",
        "print(json.dumps({\n",
        "    \"status\": \"2.7 policy hooks exported\",\n",
        "    \"out\": OUT_PATH,\n",
        "    \"latest_label\": latest_label,\n",
        "    \"g_scalar\": round(out[\"confidence\"][\"aggressiveness_scalar_g\"], 4),\n",
        "}, indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLOwjNqprmtB",
        "outputId": "5cfe08b3-8c9b-441d-8b02-4e8df80c1fc6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"status\": \"2.7 policy hooks exported\",\n",
            "  \"out\": \"artifacts/regimes/regime_policy_map.json\",\n",
            "  \"latest_label\": \"Risk-On\",\n",
            "  \"g_scalar\": 0.8719\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Section 2.8 â€” Walk-Forward Integration\n",
        "# - Rolling/expanding windows (match Section 6 when available)\n",
        "# - Fit scaler+HMM on TRAIN; score/label TEST ONLY\n",
        "# - Save per-window artifacts; stitch into a continuous timeline\n",
        "# - Preserve stateâ†’label semantics per window (no drift)\n",
        "# Reuses:\n",
        "#   - CFG.regime_dir (from 2.0)\n",
        "#   - artifacts/regimes/market_panel.parquet (2.0)\n",
        "#   - artifacts/regimes/window_manifest.json (2.1; single-window fallback)\n",
        "#   - artifacts/regimes/windows_manifest.json (preferred multi-window; else autogen)\n",
        "#   - artifacts/regimes/regime_sensitivity.json (2.5, optional pointer)\n",
        "#   - artifacts/regimes/diagnostics/* (2.6, optional pointer)\n",
        "# Outputs:\n",
        "#   - artifacts/regimes/windowed/regime_labels_<winid>.parquet (+ .csv)\n",
        "#   - artifacts/regimes/windowed/regime_hmm_<winid>.pkl\n",
        "#   - artifacts/regimes/windowed/regime_meta_<winid>.json\n",
        "#   - artifacts/regimes/regime_labels.parquet (+ .csv)  [stitched TEST]\n",
        "# Notes:\n",
        "#   - #TOCHANGE marks heavier settings for real run.\n",
        "# ============================================================\n",
        "\n",
        "from __future__ import annotations\n",
        "import os, json\n",
        "from typing import Dict, Any, List, Tuple\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from hmmlearn.hmm import GaussianHMM\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "REGIME_DIR   = CFG.regime_dir\n",
        "PANEL_PATH   = os.path.join(REGIME_DIR, \"market_panel.parquet\")\n",
        "MAN_SINGLE   = os.path.join(REGIME_DIR, \"window_manifest.json\")        # from 2.1 (single window)\n",
        "MAN_WINDOWS  = os.path.join(REGIME_DIR, \"windows_manifest.json\")       # preferred (multi)\n",
        "OUT_DIR_WIN  = os.path.join(REGIME_DIR, \"windowed\")\n",
        "os.makedirs(OUT_DIR_WIN, exist_ok=True)\n",
        "\n",
        "assert os.path.exists(PANEL_PATH), f\"Missing market panel: {PANEL_PATH}\"\n",
        "mkt = pd.read_parquet(PANEL_PATH).sort_values(\"date\").reset_index(drop=True)\n",
        "mkt[\"date\"] = pd.to_datetime(mkt[\"date\"])\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# 0) Windows manifest: prefer multi-window; else autogen a LIGHT test manifest (#TOCHANGE)\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def _autogen_windows(dates: pd.Series) -> List[Dict[str, Any]]:\n",
        "    # LIGHT test plan: 2 small rolling windows (#TOCHANGE for real run)\n",
        "    # Real run suggestion (#TOCHANGE): expanding train of ~5â€“8y; test 6â€“12m; stride 6m\n",
        "    dmin, dmax = dates.min(), dates.max()\n",
        "    # crude splits for quick smoke test: last ~4y span\n",
        "    cuts = [\n",
        "        {\"win_id\":\"W1\", \"train_start\":str((dmin + pd.Timedelta(days=365*3)).date()),\n",
        "         \"train_end\":  str((dmin + pd.Timedelta(days=365*9)).date()),\n",
        "         \"test_start\": str((dmin + pd.Timedelta(days=365*9)+pd.Timedelta(days=1)).date()),\n",
        "         \"test_end\":   str((dmin + pd.Timedelta(days=365*10)).date())},\n",
        "        {\"win_id\":\"W2\", \"train_start\":str((dmin + pd.Timedelta(days=365*4)).date()),\n",
        "         \"train_end\":  str((dmin + pd.Timedelta(days=365*10)).date()),\n",
        "         \"test_start\": str((dmin + pd.Timedelta(days=365*10)+pd.Timedelta(days=1)).date()),\n",
        "         \"test_end\":   str(dmax.date())},\n",
        "    ]\n",
        "    return cuts\n",
        "\n",
        "if os.path.exists(MAN_WINDOWS):\n",
        "    with open(MAN_WINDOWS, \"r\") as f:\n",
        "        WINS = json.load(f)\n",
        "elif os.path.exists(MAN_SINGLE):\n",
        "    # Wrap the single window as a one-window WF run\n",
        "    with open(MAN_SINGLE, \"r\") as f:\n",
        "        man = json.load(f)\n",
        "    WINS = [{\n",
        "        \"win_id\":\"W0\",\n",
        "        \"train_start\": man[\"window\"][\"train_start\"],\n",
        "        \"train_end\":   man[\"window\"][\"train_end\"],\n",
        "        \"test_start\":  man[\"window\"][\"test_start\"],\n",
        "        \"test_end\":    man[\"window\"][\"test_end\"],\n",
        "    }]\n",
        "else:\n",
        "    # autogen LIGHT test manifest (#TOCHANGE)\n",
        "    WINS = _autogen_windows(mkt[\"date\"])\n",
        "    with open(MAN_WINDOWS, \"w\") as f:\n",
        "        json.dump(WINS, f, indent=2)\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# 1) HMM hyperparams â€” reuse Section 2.2 defaults; mark heavier real-run values\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "N_ITER = 200            # TOCHANGE: 1000 for real run\n",
        "N_INIT = 2              # TOCHANGE: 10 for real run\n",
        "COVTYPE = \"full\"\n",
        "TOL = 1e-3              # TOCHANGE: 1e-4 for real run\n",
        "RANDOM_STATE = 42\n",
        "LAMBDA_STICK = 0.15     # TOCHANGE: 0.30â€“0.50 for real run\n",
        "K = 3                   # TOCHANGE: choose from sensitivity (2 or 3); default 3\n",
        "\n",
        "APPLY_RECENCY   = True  # reuse the finance recency rule from 2.2\n",
        "HALF_LIFE_DAYS  = 756   # TOCHANGE: try 504/756/1260\n",
        "SEG_LEN         = 60    # TOCHANGE: 90â€“120\n",
        "N_SEGMENTS      = 80    # TOCHANGE: 200â€“400\n",
        "EPSILON_FLOOR   = 0.10  # TOCHANGE: 0.05â€“0.15\n",
        "\n",
        "FEATURES = CFG.hmm_features.copy()\n",
        "if getattr(CFG, \"include_dvix\", False) and \"dvix\" in mkt.columns and \"dvix\" not in FEATURES:\n",
        "    FEATURES.append(\"dvix\")\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# 2) Utilities reused from 2.2/2.3/2.4\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def _diag_sticky_blend(T: np.ndarray, lam: float) -> np.ndarray:\n",
        "    k = T.shape[0]\n",
        "    out = (1.0 - lam) * T + lam * np.eye(k)\n",
        "    return out / out.sum(axis=1, keepdims=True)\n",
        "\n",
        "def _build_time_decay_weights(dates: np.ndarray, half_life_days: int) -> np.ndarray:\n",
        "    t = np.array([pd.Timestamp(d).toordinal() for d in dates], dtype=float)\n",
        "    age = (t.max() - t)\n",
        "    decay = np.log(2) / max(1, half_life_days)\n",
        "    w = np.exp(-decay * age)\n",
        "    return w / (w.sum() + 1e-12)\n",
        "\n",
        "def _sample_time_weighted_subsequences(\n",
        "    X: np.ndarray, dates: np.ndarray,\n",
        "    seg_len: int=SEG_LEN, n_segments: int=N_SEGMENTS, half_life_days: int=HALF_LIFE_DAYS, seed: int=RANDOM_STATE\n",
        ") -> Tuple[np.ndarray, List[int]]:\n",
        "    rng = np.random.RandomState(seed)\n",
        "    n = X.shape[0]\n",
        "    if n < seg_len:\n",
        "        return X.copy(), [n]\n",
        "    ends = np.arange(seg_len - 1, n)\n",
        "    p = _build_time_decay_weights(dates[ends], half_life_days)\n",
        "    p = np.maximum(p, EPSILON_FLOOR * p.max()); p = p / p.sum()\n",
        "    chosen = rng.choice(ends, size=min(n_segments, len(ends)), replace=True, p=p)\n",
        "    chunks, lengths = [], []\n",
        "    for e in chosen:\n",
        "        s = e - (seg_len - 1)\n",
        "        chunks.append(X[s:e+1]); lengths.append(seg_len)\n",
        "    return np.vstack(chunks), lengths\n",
        "\n",
        "def _fit_hmm(X_train: np.ndarray, dates_train: np.ndarray, k: int, seed: int) -> GaussianHMM:\n",
        "    if APPLY_RECENCY:\n",
        "        X_fit, lengths = _sample_time_weighted_subsequences(X_train, dates_train, seg_len=SEG_LEN, n_segments=N_SEGMENTS, half_life_days=HALF_LIFE_DAYS, seed=seed)\n",
        "    else:\n",
        "        X_fit, lengths = X_train, [len(X_train)]\n",
        "    model = GaussianHMM(\n",
        "        n_components=k, covariance_type=COVTYPE, n_iter=N_ITER, tol=TOL,\n",
        "        random_state=seed, verbose=False, init_params=\"mc\", params=\"stmc\"\n",
        "    )\n",
        "    T0 = np.full((k, k), (1.0 - 0.90) / max(1, k - 1)); np.fill_diagonal(T0, 0.90)\n",
        "    model.transmat_ = T0; model.startprob_ = np.full(k, 1.0 / k)\n",
        "    model.fit(X_fit, lengths=lengths)\n",
        "    model.transmat_ = _diag_sticky_blend(model.transmat_, LAMBDA_STICK)\n",
        "    return model\n",
        "\n",
        "def _label_states(df_tr: pd.DataFrame, st_train: np.ndarray) -> Dict[int, str]:\n",
        "    # df_tr is TRAIN-only and aligned to st_train\n",
        "    assert len(df_tr) == len(st_train), \"Train data and train states length mismatch\"\n",
        "    tmp = []\n",
        "    states_unique = sorted(np.unique(st_train))\n",
        "    for s in states_unique:\n",
        "        mask = (st_train == s)\n",
        "        def wmean(x, w):\n",
        "            w = np.asarray(w, float); x = np.asarray(x, float)\n",
        "            z = w.sum()\n",
        "            return float((x*w).sum()/z) if z > 0 else np.nan\n",
        "        tmp.append({\n",
        "            \"state_id\":  s,\n",
        "            \"ret_mean\":  wmean(df_tr[\"spy_ret\"].values, mask),\n",
        "            \"rv20_mean\": wmean(df_tr[\"spy_rv_20\"].values, mask) if \"spy_rv_20\" in df_tr else np.nan,\n",
        "            \"vix_mean\":  wmean(df_tr[\"vix_close\"].values, mask) if \"vix_close\" in df_tr else np.nan,\n",
        "            \"breadth_mean\": wmean(df_tr[\"breadth\"].values, mask) if \"breadth\" in df_tr else np.nan,\n",
        "        })\n",
        "    prof = pd.DataFrame(tmp)\n",
        "    risk_off_id = int(prof[\"rv20_mean\"].idxmax())\n",
        "    risk_on_id  = int(prof[\"ret_mean\"].idxmax())\n",
        "    mapping = {risk_on_id: \"Risk-On\", risk_off_id: \"Risk-Off\"}\n",
        "    for s in states_unique:\n",
        "        if s not in mapping:\n",
        "            mapping[s] = \"Transition\"\n",
        "    return mapping\n",
        "\n",
        "def _debounce_series(state_ids: np.ndarray, min_dwell_days: int=CFG.min_dwell_days) -> np.ndarray:\n",
        "    # 2.4 minimal debounce: enforce min dwell by suppressing singleton flips\n",
        "    out = state_ids.copy()\n",
        "    i = 1\n",
        "    while i < len(out)-1:\n",
        "        if out[i] != out[i-1] and out[i] != out[i+1]:\n",
        "            out[i] = out[i-1]  # squash 1-day blip\n",
        "            i += 1\n",
        "        i += 1\n",
        "    # NOTE: #TOCHANGE implement full min-run-length >= min_dwell_days if needed\n",
        "    return out\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# 3) Walk-forward loop\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "stitched = []  # list[DataFrame] of TEST chunks with posteriors + labels\n",
        "win_summ = []\n",
        "\n",
        "for w in WINS:\n",
        "    win_id = w.get(\"win_id\", f\"W{len(win_summ)}\")\n",
        "    ts, te = pd.to_datetime(w[\"train_start\"]), pd.to_datetime(w[\"train_end\"])\n",
        "    us, ue = pd.to_datetime(w[\"test_start\"]),  pd.to_datetime(w[\"test_end\"])\n",
        "    m_train = (mkt[\"date\"] >= ts) & (mkt[\"date\"] <= te)\n",
        "    m_test  = (mkt[\"date\"] >= us) & (mkt[\"date\"] <= ue)\n",
        "    df_tr = mkt.loc[m_train, [\"date\", \"spy_ret\"] + FEATURES].dropna().reset_index(drop=True)\n",
        "    df_te = mkt.loc[m_test,  [\"date\", \"spy_ret\"] + FEATURES].dropna().reset_index(drop=True)\n",
        "\n",
        "    if df_tr.empty or df_te.empty:\n",
        "        continue\n",
        "\n",
        "    scaler = StandardScaler().fit(df_tr[FEATURES].to_numpy(dtype=float))\n",
        "    X_tr = scaler.transform(df_tr[FEATURES].to_numpy(dtype=float))\n",
        "    X_te = scaler.transform(df_te[FEATURES].to_numpy(dtype=float))\n",
        "\n",
        "    # Fit HMM (single best run for speed; #TOCHANGE run N_INIT restarts, keep best)\n",
        "    seed = RANDOM_STATE\n",
        "    model = _fit_hmm(X_tr, df_tr[\"date\"].to_numpy(), k=K, seed=seed)\n",
        "\n",
        "    # Score TEST only\n",
        "    post = model.predict_proba(X_te)\n",
        "    hard = post.argmax(axis=1)\n",
        "\n",
        "    # Map semantics using TRAIN (no peeking)\n",
        "    # We need states on TRAIN for profiling; use predict_proba on train too (cheap)\n",
        "    st_train = model.predict_proba(X_tr).argmax(axis=1)\n",
        "    mapping  = _label_states(df_tr, st_train)\n",
        "    lbl_test = pd.Series([mapping.get(int(s), f\"State{int(s)}\") for s in hard], index=df_te.index)\n",
        "\n",
        "    # Debounce (light)\n",
        "    hard_db = _debounce_series(hard, min_dwell_days=CFG.min_dwell_days)\n",
        "\n",
        "    # Package per-window labels\n",
        "    out_cols = {\n",
        "        \"date\": df_te[\"date\"],\n",
        "        \"state_id\": hard,\n",
        "        \"state_id_smoothed\": hard_db,\n",
        "        \"regime_label\": lbl_test.values,\n",
        "        \"regime_label_smoothed\": pd.Series([mapping.get(int(s), f\"State{int(s)}\") for s in hard_db], index=df_te.index).values,\n",
        "    }\n",
        "    # Add posterior columns p0..pK-1\n",
        "    for s in range(post.shape[1]):\n",
        "        out_cols[f\"p{s}\"] = post[:, s]\n",
        "    lab_te = pd.DataFrame(out_cols).sort_values(\"date\").reset_index(drop=True)\n",
        "\n",
        "    # Save per-window artifacts\n",
        "    # Bundle: model + scaler + meta knobs for traceability\n",
        "    bundle = {\n",
        "        \"model\": model,\n",
        "        \"k\": K,\n",
        "        \"features\": FEATURES,\n",
        "        \"scaler\": scaler,  # keep in-bundle object; also persist path below if desired\n",
        "        \"random_state\": seed,\n",
        "        \"n_iter\": N_ITER, \"n_init\": N_INIT, \"tol\": TOL, \"covariance_type\": COVTYPE,\n",
        "        \"recency_weighting\": APPLY_RECENCY, \"recency_half_life_days\": HALF_LIFE_DAYS,\n",
        "        \"recency_seg_len\": SEG_LEN, \"recency_n_segments\": N_SEGMENTS, \"recency_epsilon_floor\": EPSILON_FLOOR,\n",
        "        \"sticky_lambda\": LAMBDA_STICK,\n",
        "        \"train_dates\": [str(d) for d in df_tr[\"date\"].to_numpy()],\n",
        "        \"test_dates\":  [str(d) for d in df_te[\"date\"].to_numpy()],\n",
        "        \"created_at\": datetime.utcnow().isoformat() + \"Z\",\n",
        "        \"fit_mode\": \"recency\" if APPLY_RECENCY else \"plain\",\n",
        "    }\n",
        "    bpath = os.path.join(OUT_DIR_WIN, f\"regime_hmm_{win_id}.pkl\")\n",
        "    joblib.dump(bundle, bpath)\n",
        "\n",
        "    meta = {\n",
        "        \"win_id\": win_id,\n",
        "        \"window\": {\"train_start\": str(ts.date()), \"train_end\": str(te.date()), \"test_start\": str(us.date()), \"test_end\": str(ue.date())},\n",
        "        \"features\": FEATURES,\n",
        "        \"k\": K,\n",
        "        \"state_label_map\": {int(k): v for k, v in mapping.items()},\n",
        "        \"sticky_lambda\": LAMBDA_STICK,\n",
        "        \"recency\": {\"enabled\": APPLY_RECENCY, \"half_life_days\": HALF_LIFE_DAYS, \"seg_len\": SEG_LEN, \"n_segments\": N_SEGMENTS, \"epsilon_floor\": EPSILON_FLOOR},\n",
        "        \"bundle_path\": bpath,\n",
        "        \"panel_path\": PANEL_PATH,\n",
        "        \"sensitivity_path\": os.path.join(REGIME_DIR, \"regime_sensitivity.json\") if os.path.exists(os.path.join(REGIME_DIR, \"regime_sensitivity.json\")) else None,\n",
        "        \"diagnostics_dir\": os.path.join(REGIME_DIR, \"diagnostics\") if os.path.isdir(os.path.join(REGIME_DIR, \"diagnostics\")) else None,\n",
        "        \"created_at\": datetime.utcnow().isoformat() + \"Z\",\n",
        "    }\n",
        "    mpath = os.path.join(OUT_DIR_WIN, f\"regime_meta_{win_id}.json\")\n",
        "    with open(mpath, \"w\") as f:\n",
        "        json.dump(meta, f, indent=2)\n",
        "\n",
        "    # Always write CSV alongside parquet  #TOCHANGE: keep in prod when many windows\n",
        "    lpath = os.path.join(OUT_DIR_WIN, f\"regime_labels_{win_id}.parquet\")\n",
        "    lab_te.to_parquet(lpath, index=False)\n",
        "    try:\n",
        "        lab_te.to_csv(lpath.replace(\".parquet\", \".csv\"), index=False)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    stitched.append(lab_te)\n",
        "    win_summ.append({\n",
        "        \"win_id\": win_id,\n",
        "        \"train_start\": str(ts.date()),\n",
        "        \"train_end\":   str(te.date()),\n",
        "        \"test_start\":  str(us.date()),\n",
        "        \"test_end\":    str(ue.date()),\n",
        "        \"n_train\": int(len(df_tr)),\n",
        "        \"n_test\": int(len(df_te)),\n",
        "        \"bundle_path\": bpath,\n",
        "        \"labels_path\": lpath,\n",
        "        \"meta_path\": mpath,\n",
        "    })\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# 4) Save a windows index (QoL)\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "idx_json = os.path.join(OUT_DIR_WIN, \"windows_index.json\")\n",
        "idx_csv  = os.path.join(OUT_DIR_WIN, \"windows_index.csv\")\n",
        "with open(idx_json, \"w\") as f:\n",
        "    json.dump(win_summ, f, indent=2)\n",
        "try:\n",
        "    pd.DataFrame(win_summ).to_csv(idx_csv, index=False)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# 5) Stitch all TEST chunks into one continuous timeline\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "if len(stitched) == 0:\n",
        "    raise RuntimeError(\"No window produced test labels; check window coverage.\")\n",
        "\n",
        "lab_all = pd.concat(stitched, axis=0, ignore_index=True).sort_values(\"date\").reset_index(drop=True)\n",
        "lab_all.to_parquet(os.path.join(REGIME_DIR, \"regime_labels.parquet\"), index=False)\n",
        "try:\n",
        "    lab_all.to_csv(os.path.join(REGIME_DIR, \"regime_labels.csv\"), index=False)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "print(json.dumps({\n",
        "    \"status\": \"2.8 walk-forward complete\",\n",
        "    \"n_windows\": len(win_summ),\n",
        "    \"windows\": win_summ,\n",
        "    \"stitched_out\": {\n",
        "        \"parquet\": os.path.join(REGIME_DIR, \"regime_labels.parquet\"),\n",
        "        \"csv\": os.path.join(REGIME_DIR, \"regime_labels.csv\"),\n",
        "    },\n",
        "    \"windows_index\": {\n",
        "    \"json\": idx_json,\n",
        "    \"csv\": idx_csv\n",
        "    },\n",
        "    \"notes\": [\n",
        "        \"Per-window scaler fitted on TRAIN only; TEST scored out-of-sample.\",\n",
        "        \"Stateâ†’label semantics are saved per window and applied to the test chunk.\",\n",
        "        \"For real run: increase N_ITER/N_INIT and recency sampler size; align windows with Section 6.\"\n",
        "    ]\n",
        "}, indent=2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FcgDjXn_sub",
        "outputId": "b5bebce4-6066-4a5f-90db-82d8d5182c99"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"status\": \"2.8 walk-forward complete\",\n",
            "  \"n_windows\": 1,\n",
            "  \"windows\": [\n",
            "    {\n",
            "      \"win_id\": \"W0\",\n",
            "      \"train_start\": \"2007-02-06\",\n",
            "      \"train_end\": \"2016-12-30\",\n",
            "      \"test_start\": \"2017-01-03\",\n",
            "      \"test_end\": \"2025-08-08\",\n",
            "      \"n_train\": 2495,\n",
            "      \"n_test\": 2162,\n",
            "      \"bundle_path\": \"artifacts/regimes/windowed/regime_hmm_W0.pkl\",\n",
            "      \"labels_path\": \"artifacts/regimes/windowed/regime_labels_W0.parquet\",\n",
            "      \"meta_path\": \"artifacts/regimes/windowed/regime_meta_W0.json\"\n",
            "    }\n",
            "  ],\n",
            "  \"stitched_out\": {\n",
            "    \"parquet\": \"artifacts/regimes/regime_labels.parquet\",\n",
            "    \"csv\": \"artifacts/regimes/regime_labels.csv\"\n",
            "  },\n",
            "  \"windows_index\": {\n",
            "    \"json\": \"artifacts/regimes/windowed/windows_index.json\",\n",
            "    \"csv\": \"artifacts/regimes/windowed/windows_index.csv\"\n",
            "  },\n",
            "  \"notes\": [\n",
            "    \"Per-window scaler fitted on TRAIN only; TEST scored out-of-sample.\",\n",
            "    \"State\\u2192label semantics are saved per window and applied to the test chunk.\",\n",
            "    \"For real run: increase N_ITER/N_INIT and recency sampler size; align windows with Section 6.\"\n",
            "  ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Section 2.9 â€” Forward (Shadow) Mode\n",
        "# Daily update:\n",
        "#   â€¢ Load latest window bundle (model+scaler+features) and meta\n",
        "#   â€¢ Read newest feature rows (from Section 1 products via market_panel.parquet)\n",
        "#   â€¢ Transform â†’ predict_proba â†’ state_id â†’ regime_label (via saved mapping)\n",
        "#   â€¢ Append to regime_labels.parquet (+ CSV), no backfilling\n",
        "# Retrain cadence: weekly/bi-weekly (#TOCHANGE)\n",
        "# Logging: JSONL with model hash/date/posteriors/label\n",
        "# Alerts: simple chattering/dwell anomaly (rolling window)\n",
        "# Reuses:\n",
        "#   - CFG.regime_dir (2.0)\n",
        "#   - artifacts/regimes/windowed/windows_index.json (2.8)\n",
        "#   - artifacts/regimes/windowed/regime_meta_<winid>.json (2.8)\n",
        "#   - artifacts/regimes/windowed/regime_hmm_<winid>.pkl (2.8)\n",
        "#   - artifacts/regimes/market_panel.parquet (2.0)\n",
        "#   - artifacts/regimes/regime_labels.parquet (2.8 stitched history)\n",
        "#   - artifacts/regimes/regime_policy_map.json (2.7; optional refresh)\n",
        "# Notes:\n",
        "#   - #TOCHANGE marks production choices (smoothing, policy refresh cadence, thresholds).\n",
        "# ============================================================\n",
        "\n",
        "from __future__ import annotations\n",
        "import os, json, hashlib\n",
        "from typing import Dict, Any, List\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Paths (reuse CFG from 2.0)\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "REGIME_DIR   = CFG.regime_dir\n",
        "PANEL_PATH   = os.path.join(REGIME_DIR, \"market_panel.parquet\")\n",
        "LAB_PATH_PQ  = os.path.join(REGIME_DIR, \"regime_labels.parquet\")\n",
        "LAB_PATH_CSV = os.path.join(REGIME_DIR, \"regime_labels.csv\")\n",
        "WIN_DIR      = os.path.join(REGIME_DIR, \"windowed\")\n",
        "WIN_INDEX    = os.path.join(WIN_DIR, \"windows_index.json\")\n",
        "\n",
        "# Optional: Section 1 global (won't fail if missing)\n",
        "START_DATE = globals().get(\"START_DATE\", None)\n",
        "\n",
        "# Optional policy refresh (2.7-lite)\n",
        "UPDATE_POLICY_MAP = True   # TOCHANGE: set True for prod daily refresh, False for fast tests\n",
        "POLICY_OUT        = os.path.join(REGIME_DIR, \"regime_policy_map.json\")\n",
        "\n",
        "# Forward log & alerts\n",
        "FWD_LOG   = os.path.join(REGIME_DIR, \"regime_forward_log.jsonl\")\n",
        "ALERTS_FP = os.path.join(REGIME_DIR, \"forward_alerts.json\")\n",
        "\n",
        "# Smoothing / chattering guardrails\n",
        "FWD_DEBOUNCE    = False    # TOCHANGE: consider True in prod (requires short context window)\n",
        "ROLL_WINDOW_D   = 20       # TOCHANGE: 60 for prod\n",
        "ROLL_MAX_SWITCH = 4        # TOCHANGE: 3 for prod\n",
        "\n",
        "# Confidence tail length for policy scaling\n",
        "N_CONF_TAIL = 3            # TOCHANGE: 5â€“10 for prod\n",
        "\n",
        "os.makedirs(WIN_DIR, exist_ok=True)\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Helpers\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def _load_latest_window_meta() -> Dict[str, Any]:\n",
        "    \"\"\"Pick the window with the latest test_end from windows_index.json; fallback: scan meta files.\"\"\"\n",
        "    if os.path.exists(WIN_INDEX):\n",
        "        with open(WIN_INDEX, \"r\") as f:\n",
        "            idx = json.load(f)\n",
        "        if isinstance(idx, list) and len(idx) > 0:\n",
        "            # sort by test_end\n",
        "            idx_sorted = sorted(idx, key=lambda d: d.get(\"test_end\", \"\"), reverse=True)\n",
        "            return idx_sorted[0]\n",
        "    # Fallback: scan meta files\n",
        "    metas = [p for p in os.listdir(WIN_DIR) if p.startswith(\"regime_meta_\") and p.endswith(\".json\")]\n",
        "    if not metas:\n",
        "        raise FileNotFoundError(\"No window meta files found; run 2.8 first.\")\n",
        "    # choose the lexicographically latest as a fallback heuristic\n",
        "    metas.sort(reverse=True)\n",
        "    with open(os.path.join(WIN_DIR, metas[0]), \"r\") as f:\n",
        "        m = json.load(f)\n",
        "    return {\n",
        "        \"win_id\": m.get(\"win_id\", \"W?\"),\n",
        "        \"bundle_path\": m.get(\"bundle_path\"),\n",
        "        \"meta_path\": os.path.join(WIN_DIR, metas[0]),\n",
        "        \"labels_path\": None,\n",
        "        \"test_end\": m.get(\"window\", {}).get(\"test_end\", \"\"),\n",
        "    }\n",
        "\n",
        "def _model_signature(features: List[str], k: int, window: Dict[str, Any]) -> str:\n",
        "    return hashlib.sha256(json.dumps({\"features\": features, \"k\": k, \"window\": window}, sort_keys=True).encode()).hexdigest()\n",
        "\n",
        "def _entropy(p: np.ndarray) -> float:\n",
        "    p = np.clip(p, 1e-12, 1.0)\n",
        "    return float(-(p*np.log(p)).sum() / np.log(len(p)))\n",
        "\n",
        "def _aggressiveness_from_posterior(p_mean: np.ndarray) -> Dict[str, float]:\n",
        "    c_max = float(p_mean.max())\n",
        "    c_ent = 1.0 - _entropy(p_mean)\n",
        "    c = 0.5 * (c_max + c_ent)\n",
        "    g_min, g_max = 0.35, 1.00  # TOCHANGE\n",
        "    g = g_min + (g_max - g_min) * c\n",
        "    return {\"c_max\": c_max, \"c_entropy\": c_ent, \"c\": c, \"g\": g}\n",
        "\n",
        "def _append_jsonl(path: str, rec: Dict[str, Any]) -> None:\n",
        "    with open(path, \"a\") as f:\n",
        "        f.write(json.dumps(rec) + \"\\n\")\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Load artifacts\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "assert os.path.exists(PANEL_PATH), f\"Missing market panel: {PANEL_PATH}\"\n",
        "panel = pd.read_parquet(PANEL_PATH).sort_values(\"date\").reset_index(drop=True)\n",
        "panel[\"date\"] = pd.to_datetime(panel[\"date\"])\n",
        "\n",
        "meta_idx = _load_latest_window_meta()\n",
        "assert meta_idx.get(\"bundle_path\"), \"Latest window has no bundle_path; re-run 2.8.\"\n",
        "with open(meta_idx.get(\"meta_path\") or os.path.join(WIN_DIR, f\"regime_meta_{meta_idx['win_id']}.json\"), \"r\") as f:\n",
        "    META = json.load(f)\n",
        "\n",
        "BUNDLE = joblib.load(meta_idx[\"bundle_path\"])\n",
        "MODEL  = BUNDLE[\"model\"]\n",
        "SCALER = BUNDLE[\"scaler\"]\n",
        "FEATS  = BUNDLE[\"features\"]\n",
        "K      = int(BUNDLE[\"k\"])\n",
        "\n",
        "# Stateâ†’label semantics\n",
        "state_label_map: Dict[int, str] = META.get(\"state_label_map\", {})\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Determine the forward slice (dates to append)\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "if os.path.exists(LAB_PATH_PQ):\n",
        "    labels_hist = pd.read_parquet(LAB_PATH_PQ).sort_values(\"date\").reset_index(drop=True)\n",
        "    labels_hist[\"date\"] = pd.to_datetime(labels_hist[\"date\"])\n",
        "    last_date_done = labels_hist[\"date\"].max()\n",
        "else:\n",
        "    labels_hist = None\n",
        "    # If no file yet, use START_DATE if provided, else take panel min-1\n",
        "    last_date_done = pd.to_datetime(START_DATE) - pd.Timedelta(days=1) if START_DATE else panel[\"date\"].min() - pd.Timedelta(days=1)\n",
        "\n",
        "new_mask = panel[\"date\"] > last_date_done\n",
        "to_score = panel.loc[new_mask, [\"date\"] + FEATS].dropna().reset_index(drop=True)\n",
        "\n",
        "if to_score.empty:\n",
        "    print(json.dumps({\n",
        "        \"status\": \"2.9 forward: nothing to do\",\n",
        "        \"last_processed\": str(last_date_done.date()) if pd.notnull(last_date_done) else None,\n",
        "        \"note\": \"No new rows in market_panel.parquet beyond current regime_labels.\"\n",
        "    }, indent=2))\n",
        "else:\n",
        "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    # Transform â†’ score â†’ label\n",
        "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    X = SCALER.transform(to_score[FEATS].to_numpy(dtype=float))\n",
        "    post = MODEL.predict_proba(X)\n",
        "    hard = post.argmax(axis=1)\n",
        "\n",
        "    # Map to labels (no re-profiling)\n",
        "    lbl = [state_label_map.get(int(s), f\"State{int(s)}\") for s in hard]\n",
        "\n",
        "    # Optional forward debounce (minimal). For single-day updates this does little.\n",
        "    if FWD_DEBOUNCE and labels_hist is not None and len(labels_hist) > 2:\n",
        "        prev_state = int(labels_hist.iloc[-1][\"state_id\"])\n",
        "        if len(hard) == 1:\n",
        "            # squash a 1-day blip if it differs from the last two states\n",
        "            prev_prev_state = int(labels_hist.iloc[-2][\"state_id\"])\n",
        "            if hard[0] != prev_state and prev_state == prev_prev_state:\n",
        "                hard[0] = prev_state\n",
        "                lbl[0]  = state_label_map.get(prev_state, f\"State{prev_state}\")\n",
        "\n",
        "    # Package new rows\n",
        "    out = pd.DataFrame({\n",
        "        \"date\": to_score[\"date\"],\n",
        "        \"state_id\": hard,\n",
        "        \"regime_label\": lbl,\n",
        "        **{f\"p{s}\": post[:, s] for s in range(K)}\n",
        "    }).sort_values(\"date\").reset_index(drop=True)\n",
        "\n",
        "    # Preserve smoothed columns if they exist by copying raw (forward mode = minimal smoothing)\n",
        "    if labels_hist is not None and \"state_id_smoothed\" in labels_hist.columns:\n",
        "        out[\"state_id_smoothed\"] = out[\"state_id\"].values\n",
        "    if labels_hist is not None and \"regime_label_smoothed\" in labels_hist.columns:\n",
        "        out[\"regime_label_smoothed\"] = out[\"regime_label\"].values\n",
        "\n",
        "    # Append to history (or create)\n",
        "    if labels_hist is not None:\n",
        "        labels_new = pd.concat([labels_hist, out], axis=0, ignore_index=True)\n",
        "    else:\n",
        "        labels_new = out\n",
        "\n",
        "    labels_new = labels_new.drop_duplicates(subset=[\"date\"]).sort_values(\"date\").reset_index(drop=True)\n",
        "    labels_new.to_parquet(LAB_PATH_PQ, index=False)\n",
        "    try:\n",
        "        labels_new.to_csv(LAB_PATH_CSV, index=False)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    # Logging & alerts\n",
        "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    sig = _model_signature(FEATS, K, META.get(\"window\", {}))\n",
        "    for i, r in out.iterrows():\n",
        "        rec = {\n",
        "            \"ts\": datetime.utcnow().isoformat() + \"Z\",\n",
        "            \"date\": str(pd.to_datetime(r[\"date\"]).date()),\n",
        "            \"model_sig\": sig,\n",
        "            \"state_id\": int(r[\"state_id\"]),\n",
        "            \"regime_label\": str(r[\"regime_label\"]),\n",
        "            \"posteriors\": {f\"p{s}\": float(r[f\"p{s}\"]) for s in range(K)}\n",
        "        }\n",
        "        _append_jsonl(FWD_LOG, rec)\n",
        "\n",
        "    # Simple chattering alert (rolling window over recent labels)\n",
        "    alerts = {\"generated_at\": datetime.utcnow().isoformat() + \"Z\", \"alerts\": []}\n",
        "    tail = labels_new.tail(ROLL_WINDOW_D).reset_index(drop=True)\n",
        "    if len(tail) >= 3:\n",
        "        sw = int((tail[\"state_id\"].diff().fillna(0) != 0).sum())\n",
        "        if sw >= ROLL_MAX_SWITCH:\n",
        "            alerts[\"alerts\"].append(\n",
        "                f\"High switch count in last {ROLL_WINDOW_D}d: {sw} (>= {ROLL_MAX_SWITCH})\"\n",
        "            )\n",
        "    if alerts[\"alerts\"]:\n",
        "        with open(ALERTS_FP, \"w\") as f:\n",
        "            json.dump(alerts, f, indent=2)\n",
        "\n",
        "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    # Optional: refresh policy hooks (2.7-lite)\n",
        "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    if UPDATE_POLICY_MAP:\n",
        "        p_cols = [c for c in labels_new.columns if c.startswith(\"p\")]\n",
        "        if len(p_cols) == K:\n",
        "            tail_p = labels_new[p_cols].tail(N_CONF_TAIL).to_numpy(dtype=float)\n",
        "            p_mean = tail_p.mean(axis=0)\n",
        "            conf   = _aggressiveness_from_posterior(p_mean)\n",
        "            latest = labels_new.iloc[-1]\n",
        "            policy = {\n",
        "                \"created_at\": datetime.utcnow().isoformat() + \"Z\",\n",
        "                \"latest_date\": str(pd.to_datetime(latest[\"date\"]).date()),\n",
        "                \"k\": K,\n",
        "                \"latest_regime_label\": str(latest.get(\"regime_label\")),\n",
        "                \"latest_state_id\": int(latest.get(\"state_id\")),\n",
        "                \"latest_posteriors\": {f\"p{s}\": float(latest.get(f\"p{s}\", np.nan)) for s in range(K)},\n",
        "                \"confidence\": {\n",
        "                    \"aggressiveness_scalar_g\": conf[\"g\"],\n",
        "                    \"confidence\": conf,\n",
        "                    \"recommendations\": {\n",
        "                        \"scale_position_sizes_by_g\": True,\n",
        "                        \"scale_turnover_cap_by_g\": True,\n",
        "                        \"scale_hedge_intensity_by_(1-g)\": True\n",
        "                    }\n",
        "                },\n",
        "                \"inputs\": {\n",
        "                    \"bundle_path\": meta_idx[\"bundle_path\"],\n",
        "                    \"meta_path\": meta_idx.get(\"meta_path\"),\n",
        "                    \"labels_path\": LAB_PATH_PQ,\n",
        "                    \"panel_path\": PANEL_PATH,\n",
        "                    \"smoothing_window_days\": N_CONF_TAIL  # TOCHANGE\n",
        "                },\n",
        "                \"signature\": sig\n",
        "            }\n",
        "            with open(POLICY_OUT, \"w\") as f:\n",
        "                json.dump(policy, f, indent=2)\n",
        "\n",
        "    print(json.dumps({\n",
        "        \"status\": \"2.9 forward appended\",\n",
        "        \"n_rows_appended\": int(out.shape[0]),\n",
        "        \"last_appended_date\": str(out[\"date\"].iloc[-1].date()),\n",
        "        \"bundle_used\": meta_idx[\"bundle_path\"],\n",
        "        \"policy_refreshed\": bool(UPDATE_POLICY_MAP),\n",
        "        \"log_path\": FWD_LOG,\n",
        "        \"alerts_written\": os.path.exists(ALERTS_FP),\n",
        "    }, indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZWhCdqFHfJ6",
        "outputId": "eb18c62d-de7c-42e5-f3ba-b2e38c017e95"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"status\": \"2.9 forward: nothing to do\",\n",
            "  \"last_processed\": \"2025-08-08\",\n",
            "  \"note\": \"No new rows in market_panel.parquet beyond current regime_labels.\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Section Forward Smoke Test (Prep + Run)\n",
        "# Goal: simulate a realistic forward append using the *actual*\n",
        "# latest tail of the panel, without touching production files.\n",
        "#\n",
        "# What it does:\n",
        "#   - Truncates current labels by last N days into a smoke copy\n",
        "#   - Loads latest windowed HMM bundle + meta (from 2.8 outputs)\n",
        "#   - Transforms last N feature rows with saved scaler\n",
        "#   - Predicts posteriors/labels using saved state map\n",
        "#   - Appends to smoke labels and writes a smoke policy map\n",
        "#\n",
        "# Reuses:\n",
        "#   - CFG (from 2.0)\n",
        "#   - artifacts/regimes/market_panel.parquet (1.â†’2.0)\n",
        "#   - artifacts/regimes/regime_labels.parquet (2.8 stitched)\n",
        "#   - artifacts/regimes/windowed/windows_index.json (2.8)\n",
        "#   - artifacts/regimes/windowed/regime_hmm_<win>.pkl (2.8)\n",
        "#   - artifacts/regimes/windowed/regime_meta_<win>.json (2.8)\n",
        "#\n",
        "# Outputs (smoke-only; production files untouched):\n",
        "#   - artifacts/regimes/forward_smoketest/regime_labels_smoke_base.parquet\n",
        "#   - artifacts/regimes/forward_smoketest/regime_labels_smoke.parquet (+csv)\n",
        "#   - artifacts/regimes/forward_smoketest/regime_policy_map_smoke.json\n",
        "#\n",
        "# Notes:\n",
        "#   - #TOCHANGE marks heavier/production settings.\n",
        "#   - This section is safe to run multiple times; it overwrites files in the smoke folder.\n",
        "# ============================================================\n",
        "\n",
        "from __future__ import annotations\n",
        "import os, json, hashlib\n",
        "from typing import Dict, Any\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "# ---- Tunables ------------------------------------------------\n",
        "SMOKE_N_DAYS = 3     # #TOCHANGE: 5â€“10 for a beefier smoke\n",
        "REGIME_DIR   = CFG.regime_dir\n",
        "SMOKE_DIR    = os.path.join(REGIME_DIR, \"forward_smoketest\")\n",
        "os.makedirs(SMOKE_DIR, exist_ok=True)\n",
        "\n",
        "# ---- Paths ---------------------------------------------------\n",
        "PANEL_PATH    = os.path.join(REGIME_DIR, \"market_panel.parquet\")\n",
        "LAB_PROD_PATH = os.path.join(REGIME_DIR, \"regime_labels.parquet\")\n",
        "WINDEX_PATH   = os.path.join(REGIME_DIR, \"windowed\", \"windows_index.json\")\n",
        "WIN_DIR       = os.path.join(REGIME_DIR, \"windowed\")\n",
        "\n",
        "# ---- Load panel & prod labels --------------------------------\n",
        "assert os.path.exists(PANEL_PATH), f\"Missing panel: {PANEL_PATH}\"\n",
        "panel = pd.read_parquet(PANEL_PATH).sort_values(\"date\").reset_index(drop=True)\n",
        "panel[\"date\"] = pd.to_datetime(panel[\"date\"])\n",
        "\n",
        "if not os.path.exists(LAB_PROD_PATH):\n",
        "    raise FileNotFoundError(\n",
        "        \"Expected stitched labels at artifacts/regimes/regime_labels.parquet from 2.8. \"\n",
        "        \"Run 2.8 first.\"\n",
        "    )\n",
        "lab_prod = pd.read_parquet(LAB_PROD_PATH).sort_values(\"date\").reset_index(drop=True)\n",
        "lab_prod[\"date\"] = pd.to_datetime(lab_prod[\"date\"])\n",
        "\n",
        "# ---- Choose the latest window meta/bundle --------------------\n",
        "def _pick_latest_window(win_index_json: str) -> Dict[str, Any]:\n",
        "    # Prefer windows_index.json (2.8 QoL). Fallback to single W0.\n",
        "    if os.path.exists(win_index_json):\n",
        "        with open(win_index_json, \"r\") as f:\n",
        "            idx = json.load(f)\n",
        "        if isinstance(idx, list) and len(idx) > 0:\n",
        "            # sort by test_end\n",
        "            idx_sorted = sorted(idx, key=lambda d: pd.to_datetime(d[\"test_end\"]))\n",
        "            return idx_sorted[-1]\n",
        "    # Fallback: try W0 paths\n",
        "    meta_fallback = os.path.join(WIN_DIR, \"regime_meta_W0.json\")\n",
        "    bundle_fallback = os.path.join(WIN_DIR, \"regime_hmm_W0.pkl\")\n",
        "    if os.path.exists(meta_fallback) and os.path.exists(bundle_fallback):\n",
        "        with open(meta_fallback, \"r\") as f:\n",
        "            m = json.load(f)\n",
        "        return {\n",
        "            \"win_id\": \"W0\",\n",
        "            \"train_start\": m[\"window\"][\"train_start\"],\n",
        "            \"train_end\": m[\"window\"][\"train_end\"],\n",
        "            \"test_start\": m[\"window\"][\"test_start\"],\n",
        "            \"test_end\": m[\"window\"][\"test_end\"],\n",
        "            \"bundle_path\": m[\"bundle_path\"],\n",
        "            \"labels_path\": os.path.join(WIN_DIR, f\"regime_labels_W0.parquet\"),\n",
        "            \"meta_path\": meta_fallback,\n",
        "        }\n",
        "    raise FileNotFoundError(\"No windowed bundles found. Run 2.8 first.\")\n",
        "\n",
        "latest = _pick_latest_window(WINDEX_PATH)\n",
        "bundle_path = latest[\"bundle_path\"]\n",
        "meta_path   = latest[\"meta_path\"]\n",
        "\n",
        "assert os.path.exists(bundle_path), f\"Missing bundle: {bundle_path}\"\n",
        "assert os.path.exists(meta_path),   f\"Missing meta:   {meta_path}\"\n",
        "\n",
        "bundle = joblib.load(bundle_path)\n",
        "with open(meta_path, \"r\") as f:\n",
        "    meta = json.load(f)\n",
        "\n",
        "features = bundle[\"features\"]\n",
        "scaler   = bundle.get(\"scaler\", None)\n",
        "if scaler is None and \"scaler_path\" in bundle:\n",
        "    scaler = joblib.load(bundle[\"scaler_path\"])\n",
        "if scaler is None:\n",
        "    raise RuntimeError(\"Bundle does not contain a scaler; ensure 2.8 saved the scaler in-bundle.\")\n",
        "\n",
        "state_label_map = {int(k): v for k, v in meta.get(\"state_label_map\", {}).items()}\n",
        "\n",
        "# ---- Build the smoke base (truncate last N days of labels) ---\n",
        "lab_prod_sorted = lab_prod.sort_values(\"date\")\n",
        "if len(lab_prod_sorted) <= SMOKE_N_DAYS:\n",
        "    raise RuntimeError(\"Not enough labeled rows to build a smoke base; reduce SMOKE_N_DAYS.\")\n",
        "\n",
        "smoke_cutoff = lab_prod_sorted[\"date\"].iloc[-SMOKE_N_DAYS-1]\n",
        "lab_smoke_base = lab_prod_sorted.loc[lab_prod_sorted[\"date\"] <= smoke_cutoff].copy()\n",
        "base_last_date = lab_smoke_base[\"date\"].max()\n",
        "\n",
        "base_path_parq = os.path.join(SMOKE_DIR, \"regime_labels_smoke_base.parquet\")\n",
        "lab_smoke_base.to_parquet(base_path_parq, index=False)\n",
        "try:\n",
        "    lab_smoke_base.to_csv(base_path_parq.replace(\".parquet\", \".csv\"), index=False)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# ---- Determine the forward tail to score ---------------------\n",
        "panel_tail = panel.loc[panel[\"date\"] > base_last_date].copy()\n",
        "if panel_tail.empty:\n",
        "    raise RuntimeError(\"Panel has no dates after the smoke base cutoff; cannot simulate forward.\")\n",
        "\n",
        "# Use only the first SMOKE_N_DAYS after base_last_date\n",
        "fwd_dates = sorted(panel_tail[\"date\"].unique())[:SMOKE_N_DAYS]\n",
        "panel_fwd = panel.loc[panel[\"date\"].isin(fwd_dates)].copy()\n",
        "panel_fwd = panel_fwd[[\"date\"] + features].dropna().sort_values(\"date\").reset_index(drop=True)\n",
        "\n",
        "if panel_fwd.empty or len(panel_fwd) < 1:\n",
        "    raise RuntimeError(\"No complete feature rows for smoke forward dates; check features coverage.\")\n",
        "\n",
        "# ---- Transform with saved scaler & predict posteriors --------\n",
        "X_fwd = scaler.transform(panel_fwd[features].to_numpy(dtype=float))\n",
        "post  = bundle[\"model\"].predict_proba(X_fwd)\n",
        "hard  = post.argmax(axis=1)\n",
        "\n",
        "# ---- Map to labels (no re-profiling; use saved mapping) ------\n",
        "if state_label_map:\n",
        "    reg_lbl = [state_label_map.get(int(s), f\"State{int(s)}\") for s in hard]\n",
        "else:\n",
        "    reg_lbl = [f\"State{int(s)}\" for s in hard]  # fallback\n",
        "\n",
        "# ---- (Optional) minimal debounce just within the smoke block -\n",
        "def _debounce_series(state_ids: np.ndarray, min_dwell_days: int=CFG.min_dwell_days):\n",
        "    out = state_ids.copy()\n",
        "    i = 1\n",
        "    while i < len(out)-1:\n",
        "        if out[i] != out[i-1] and out[i] != out[i+1]:\n",
        "            out[i] = out[i-1]\n",
        "            i += 1\n",
        "        i += 1\n",
        "    return out\n",
        "\n",
        "hard_db = _debounce_series(hard)\n",
        "\n",
        "# ---- Build the rows to append --------------------------------\n",
        "rows = {\n",
        "    \"date\": panel_fwd[\"date\"].to_numpy(),\n",
        "    \"state_id\": hard,\n",
        "    \"state_id_smoothed\": hard_db,\n",
        "    \"regime_label\": np.array(reg_lbl, dtype=object),\n",
        "    \"regime_label_smoothed\": np.array([reg_lbl[i] if hard_db[i]==hard[i] else state_label_map.get(int(hard_db[i]), f\"State{int(hard_db[i])}\") for i in range(len(hard))], dtype=object),\n",
        "}\n",
        "# Add posteriors p0..pK-1\n",
        "K = post.shape[1]\n",
        "for s in range(K):\n",
        "    rows[f\"p{s}\"] = post[:, s]\n",
        "\n",
        "lab_smoke_new = pd.DataFrame(rows).sort_values(\"date\").reset_index(drop=True)\n",
        "\n",
        "# ---- Append to smoke base & save ------------------------------\n",
        "lab_smoke_all = pd.concat([lab_smoke_base, lab_smoke_new], axis=0).sort_values(\"date\").reset_index(drop=True)\n",
        "\n",
        "out_parq = os.path.join(SMOKE_DIR, \"regime_labels_smoke.parquet\")\n",
        "lab_smoke_all.to_parquet(out_parq, index=False)\n",
        "try:\n",
        "    lab_smoke_all.to_csv(out_parq.replace(\".parquet\", \".csv\"), index=False)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# ---- Also emit a smoke policy map (same spirit as 2.7) -------\n",
        "def _entropy(p: np.ndarray) -> float:\n",
        "    p = np.clip(p, 1e-12, 1.0)\n",
        "    return float(-(p*np.log(p)).sum() / np.log(len(p)))\n",
        "\n",
        "# #TOCHANGE: smooth window for confidence in prod 5â€“10\n",
        "N_SMOOTH = 3\n",
        "tail = lab_smoke_all.tail(N_SMOOTH)\n",
        "p_cols = [c for c in lab_smoke_all.columns if c.startswith(\"p\")]\n",
        "p_tail = tail[p_cols].to_numpy(dtype=float)\n",
        "p_mean = p_tail.mean(axis=0) if p_tail.size else np.ones(K)/K\n",
        "c_max = float(p_mean.max())\n",
        "c_ent = 1.0 - _entropy(p_mean)\n",
        "c_comb = 0.5*(c_max + c_ent)\n",
        "g_min, g_max = 0.35, 1.00   # #TOCHANGE deeper throttling for prod\n",
        "g = g_min + (g_max-g_min)*c_comb\n",
        "\n",
        "latest_row = lab_smoke_all.iloc[-1]\n",
        "latest_label = str(latest_row[\"regime_label_smoothed\"] if \"regime_label_smoothed\" in lab_smoke_all.columns else latest_row[\"regime_label\"])\n",
        "\n",
        "policy_smoke = {\n",
        "    \"created_at\": pd.Timestamp.utcnow().isoformat()+\"Z\",\n",
        "    \"mode\": \"smoke\",\n",
        "    \"latest_date\": str(pd.to_datetime(latest_row[\"date\"]).date()),\n",
        "    \"k\": int(K),\n",
        "    \"latest_regime_label\": latest_label,\n",
        "    \"latest_state_id\": int(latest_row[\"state_id_smoothed\"] if \"state_id_smoothed\" in lab_smoke_all.columns else latest_row[\"state_id\"]),\n",
        "    \"latest_posteriors\": {f\"p{s}\": float(latest_row.get(f\"p{s}\", np.nan)) for s in range(K)},\n",
        "    \"confidence\": {\n",
        "        \"aggressiveness_scalar_g\": g,\n",
        "        \"confidence_components\": {\"c_max\": c_max, \"c_entropy\": c_ent, \"combined\": c_comb},\n",
        "        \"recommendations\": {\n",
        "            \"scale_position_sizes_by_g\": True,\n",
        "            \"scale_turnover_cap_by_g\": True,\n",
        "            \"scale_hedge_intensity_by_(1-g)\": True\n",
        "        }\n",
        "    },\n",
        "    \"inputs\": {\n",
        "        \"bundle_path\": bundle_path,\n",
        "        \"meta_path\": meta_path,\n",
        "        \"features\": features,\n",
        "        \"smoke_base_labels\": base_path_parq,\n",
        "        \"smoke_labels_out\": out_parq,\n",
        "        \"n_days_scored\": int(len(lab_smoke_new)),\n",
        "        \"window\": latest.get(\"window\", {\n",
        "            \"train_start\": latest.get(\"train_start\"),\n",
        "            \"train_end\":   latest.get(\"train_end\"),\n",
        "            \"test_start\":  latest.get(\"test_start\"),\n",
        "            \"test_end\":    latest.get(\"test_end\"),\n",
        "        })\n",
        "    }\n",
        "}\n",
        "with open(os.path.join(SMOKE_DIR, \"regime_policy_map_smoke.json\"), \"w\") as f:\n",
        "    json.dump(policy_smoke, f, indent=2)\n",
        "\n",
        "print(json.dumps({\n",
        "    \"status\": \"Forward smoke COMPLETE\",\n",
        "    \"base_last_date\": str(pd.to_datetime(base_last_date).date()),\n",
        "    \"scored_dates\": [str(pd.to_datetime(d).date()) for d in lab_smoke_new[\"date\"]],\n",
        "    \"out_labels_parquet\": out_parq,\n",
        "    \"out_policy_smoke\": os.path.join(SMOKE_DIR, \"regime_policy_map_smoke.json\"),\n",
        "    \"notes\": [\n",
        "        f\"Truncated prod labels by last {SMOKE_N_DAYS} day(s) to create smoke base.\",\n",
        "        \"Used latest windowed HMM bundle + scaler; no re-profiling (state map from meta).\",\n",
        "        \"For a heavier smoke, increase SMOKE_N_DAYS (#TOCHANGE).\"\n",
        "    ]\n",
        "}, indent=2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGl9UfNW_shE",
        "outputId": "98f23266-7580-496d-bcbf-2c4c933ddcd4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"status\": \"Forward smoke COMPLETE\",\n",
            "  \"base_last_date\": \"2025-08-05\",\n",
            "  \"scored_dates\": [\n",
            "    \"2025-08-06\",\n",
            "    \"2025-08-07\",\n",
            "    \"2025-08-08\"\n",
            "  ],\n",
            "  \"out_labels_parquet\": \"artifacts/regimes/forward_smoketest/regime_labels_smoke.parquet\",\n",
            "  \"out_policy_smoke\": \"artifacts/regimes/forward_smoketest/regime_policy_map_smoke.json\",\n",
            "  \"notes\": [\n",
            "    \"Truncated prod labels by last 3 day(s) to create smoke base.\",\n",
            "    \"Used latest windowed HMM bundle + scaler; no re-profiling (state map from meta).\",\n",
            "    \"For a heavier smoke, increase SMOKE_N_DAYS (#TOCHANGE).\"\n",
            "  ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Section 2.10 â€” Configuration & Reproducibility\n",
        "# Tasks:\n",
        "#   â€¢ Snapshot effective config + key artifacts (hashes, sizes, dates)\n",
        "#   â€¢ Basic determinism check (same bundle, same scores)\n",
        "#   â€¢ Validations: posterior sums, date order, gaps, label semantics sanity\n",
        "#   â€¢ Emit run manifest + validation report for auditability\n",
        "#\n",
        "# Reuses:\n",
        "#   - CFG (from 2.0)\n",
        "#   - artifacts/regimes/market_panel.parquet (2.0)\n",
        "#   - artifacts/regimes/regime_labels.parquet (2.8)\n",
        "#   - artifacts/regimes/windowed/windows_index.json (2.8 QoL)\n",
        "#   - artifacts/regimes/windowed/regime_hmm_<win>.pkl + meta (2.8)\n",
        "#   - artifacts/regimes/diagnostics/state_profiles_table.csv (2.6; optional)\n",
        "#\n",
        "# Outputs:\n",
        "#   - artifacts/regimes/run_manifest.json\n",
        "#   - artifacts/regimes/validation_report.json\n",
        "#   - artifacts/regimes/run_fingerprint.txt  (short human-readable summary)\n",
        "#\n",
        "# Notes:\n",
        "#   - #TOCHANGE marks heavier/production settings.\n",
        "# ============================================================\n",
        "\n",
        "from __future__ import annotations\n",
        "import os, json, hashlib, sys\n",
        "from typing import Dict, Any\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "REGIME_DIR   = CFG.regime_dir\n",
        "PANEL_PATH   = os.path.join(REGIME_DIR, \"market_panel.parquet\")\n",
        "LABELS_PATH  = os.path.join(REGIME_DIR, \"regime_labels.parquet\")\n",
        "CONFIG_EFF   = os.path.join(REGIME_DIR, \"regime_config_effective.json\")\n",
        "WINDEX_PATH  = os.path.join(REGIME_DIR, \"windowed\", \"windows_index.json\")\n",
        "WIN_DIR      = os.path.join(REGIME_DIR, \"windowed\")\n",
        "DIAG_DIR     = os.path.join(REGIME_DIR, \"diagnostics\")\n",
        "PROF_TABLE   = os.path.join(DIAG_DIR, \"state_profiles_table.csv\")\n",
        "\n",
        "def _sha256_file(path: str) -> str:\n",
        "    h = hashlib.sha256()\n",
        "    with open(path, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(65536), b\"\"):\n",
        "            h.update(chunk)\n",
        "    return h.hexdigest()\n",
        "\n",
        "def _pick_latest_window(windex: str) -> Dict[str, Any]:\n",
        "    if os.path.exists(windex):\n",
        "        with open(windex, \"r\") as f:\n",
        "            idx = json.load(f)\n",
        "        if isinstance(idx, list) and len(idx) > 0:\n",
        "            idx_sorted = sorted(idx, key=lambda d: pd.to_datetime(d[\"test_end\"]))\n",
        "            return idx_sorted[-1]\n",
        "    # fallback to W0\n",
        "    meta_fallback = os.path.join(WIN_DIR, \"regime_meta_W0.json\")\n",
        "    if os.path.exists(meta_fallback):\n",
        "        with open(meta_fallback, \"r\") as f:\n",
        "            m = json.load(f)\n",
        "        return {\n",
        "            \"win_id\": \"W0\",\n",
        "            \"train_start\": m[\"window\"][\"train_start\"],\n",
        "            \"train_end\":   m[\"window\"][\"train_end\"],\n",
        "            \"test_start\":  m[\"window\"][\"test_start\"],\n",
        "            \"test_end\":    m[\"window\"][\"test_end\"],\n",
        "            \"bundle_path\": m[\"bundle_path\"],\n",
        "            \"meta_path\":   meta_fallback,\n",
        "            \"labels_path\": os.path.join(WIN_DIR, \"regime_labels_W0.parquet\"),\n",
        "        }\n",
        "    raise FileNotFoundError(\"No windowed artifacts found; run 2.8 first.\")\n",
        "\n",
        "# â”€â”€ 1) Load essentials\n",
        "assert os.path.exists(PANEL_PATH), f\"Missing panel: {PANEL_PATH}\"\n",
        "assert os.path.exists(LABELS_PATH), f\"Missing stitched labels: {LABELS_PATH}\"\n",
        "panel  = pd.read_parquet(PANEL_PATH).sort_values(\"date\").reset_index(drop=True)\n",
        "labels = pd.read_parquet(LABELS_PATH).sort_values(\"date\").reset_index(drop=True)\n",
        "panel[\"date\"]  = pd.to_datetime(panel[\"date\"])\n",
        "labels[\"date\"] = pd.to_datetime(labels[\"date\"])\n",
        "\n",
        "latest = _pick_latest_window(WINDEX_PATH)\n",
        "bundle_path = latest[\"bundle_path\"]; meta_path = latest[\"meta_path\"]\n",
        "assert os.path.exists(bundle_path), f\"Missing bundle: {bundle_path}\"\n",
        "assert os.path.exists(meta_path),   f\"Missing meta:   {meta_path}\"\n",
        "bundle = joblib.load(bundle_path)\n",
        "\n",
        "\n",
        "# â”€â”€ 2) Build run manifest (config + artifacts snapshot)\n",
        "manifest: Dict[str, Any] = {\n",
        "    \"created_at\": pd.Timestamp.utcnow().isoformat()+\"Z\",\n",
        "    \"config_effective_path\": CONFIG_EFF if os.path.exists(CONFIG_EFF) else None,\n",
        "    \"artifacts\": {\n",
        "        \"panel\": {\"path\": PANEL_PATH, \"rows\": int(len(panel)), \"sha256\": _sha256_file(PANEL_PATH)},\n",
        "        \"labels_stitched\": {\"path\": LABELS_PATH, \"rows\": int(len(labels)), \"sha256\": _sha256_file(LABELS_PATH)},\n",
        "        \"bundle_latest\": {\"path\": bundle_path, \"sha256\": _sha256_file(bundle_path)},\n",
        "        \"meta_latest\":   {\"path\": meta_path,   \"sha256\": _sha256_file(meta_path)},\n",
        "    },\n",
        "    \"latest_window\": {\n",
        "        \"win_id\": latest.get(\"win_id\"),\n",
        "        \"train_start\": latest.get(\"train_start\"), \"train_end\": latest.get(\"train_end\"),\n",
        "        \"test_start\":  latest.get(\"test_start\"),  \"test_end\":  latest.get(\"test_end\"),\n",
        "        \"labels_path\": latest.get(\"labels_path\"),\n",
        "    },\n",
        "    \"features\": bundle.get(\"features\", []),\n",
        "    \"hmm\": {\n",
        "        \"k\": int(bundle.get(\"k\", -1)),\n",
        "        \"covariance_type\": bundle.get(\"covariance_type\", \"full\"),\n",
        "        \"n_iter\": int(bundle.get(\"n_iter\", -1)),\n",
        "        \"n_init\": int(bundle.get(\"n_init\", -1)),\n",
        "        \"tol\": float(bundle.get(\"tol\", -1)),\n",
        "        \"random_state\": int(bundle.get(\"random_state\", -1)),\n",
        "        \"sticky_lambda\": float(bundle.get(\"sticky_lambda\", np.nan)),\n",
        "        \"recency\": {\n",
        "            \"enabled\": bool(bundle.get(\"recency_weighting\", False)),\n",
        "            \"half_life_days\": int(bundle.get(\"recency_half_life_days\", 0)),\n",
        "            \"seg_len\": int(bundle.get(\"recency_seg_len\", 0)),\n",
        "            \"n_segments\": int(bundle.get(\"recency_n_segments\", 0)),\n",
        "            \"epsilon_floor\": float(bundle.get(\"recency_epsilon_floor\", 0.0)),\n",
        "        },\n",
        "    }\n",
        "}\n",
        "if manifest[\"config_effective_path\"]:\n",
        "    manifest[\"config_effective_sha256\"] = _sha256_file(manifest[\"config_effective_path\"])\n",
        "\n",
        "with open(os.path.join(REGIME_DIR, \"run_manifest.json\"), \"w\") as f:\n",
        "    json.dump(manifest, f, indent=2)\n",
        "\n",
        "# --- Checks container + helper (must be defined before any _add_check calls)\n",
        "report = {\"checks\": [], \"status\": \"ok\"}\n",
        "\n",
        "def _add_check(name: str, ok: bool, details: Dict[str, Any] | None=None):\n",
        "    report[\"checks\"].append({\"name\": name, \"pass\": bool(ok), \"details\": details or {}})\n",
        "    if not ok:\n",
        "        report[\"status\"] = \"fail\"\n",
        "\n",
        "# Config keys presence (from CFG)\n",
        "try:\n",
        "    cfg_checks = {\n",
        "        \"hmm_features_present\": bool(getattr(CFG, \"hmm_features\", None)),\n",
        "        \"min_dwell_days_present\": hasattr(CFG, \"min_dwell_days\"),\n",
        "        \"posterior_thresh_present\": hasattr(CFG, \"posterior_thresh\"),\n",
        "        \"recency_weighting_flag_present\": hasattr(CFG, \"recency_weighting\"),\n",
        "    }\n",
        "    _add_check(\"config_keys_present\", all(cfg_checks.values()), cfg_checks)\n",
        "except Exception as e:\n",
        "    _add_check(\"config_keys_present_error\", False, {\"error\": repr(e)})\n",
        "\n",
        "# Optional: environment snapshot (helps reproducibility)  # TOCHANGE: expand in prod\n",
        "try:\n",
        "    import platform, sys\n",
        "    env = {\n",
        "        \"python\": sys.version.split()[0],\n",
        "        \"platform\": platform.platform(),\n",
        "        \"numpy\": np.__version__,\n",
        "        \"pandas\": pd.__version__,\n",
        "        \"hmmlearn\": __import__(\"hmmlearn\").__version__,\n",
        "        \"scikit_learn\": __import__(\"sklearn\").__version__,\n",
        "        # \"git_commit\": <inject if available>  # TOCHANGE\n",
        "    }\n",
        "    with open(os.path.join(REGIME_DIR, \"run_env.json\"), \"w\") as f:\n",
        "        json.dump(env, f, indent=2)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "\n",
        "# â”€â”€ 3) Validations\n",
        "# deleted: report = {\"checks\": [], \"status\": \"ok\"}\n",
        "\n",
        "\n",
        "\n",
        "# 3.a Dates monotonic / duplicates\n",
        "is_sorted = labels[\"date\"].is_monotonic_increasing\n",
        "has_dupes = labels[\"date\"].duplicated().any()\n",
        "_add_check(\"dates_sorted\", is_sorted, {\"monotonic_increasing\": bool(is_sorted)})\n",
        "_add_check(\"dates_no_duplicates\", not has_dupes, {\"duplicates\": int(labels[\"date\"].duplicated().sum())})\n",
        "\n",
        "# 3.b Posterior rows sum â‰ˆ 1 (if present)\n",
        "p_cols = [c for c in labels.columns if c.startswith(\"p\")]\n",
        "if p_cols:\n",
        "    row_sums = labels[p_cols].sum(axis=1).to_numpy(dtype=float)\n",
        "    max_dev = float(np.max(np.abs(row_sums - 1.0)))\n",
        "    _add_check(\"posteriors_sum_to_one\", bool(max_dev < 1e-6), {\"max_abs_deviation\": max_dev})\n",
        "else:\n",
        "    _add_check(\"posteriors_present\", False, {\"note\": \"No p* columns found.\"})\n",
        "\n",
        "# 3.c Gap check vs panel dates (labels âŠ† panel)\n",
        "panel_dates  = set(pd.to_datetime(panel[\"date\"]).dt.date)\n",
        "labels_dates = set(pd.to_datetime(labels[\"date\"]).dt.date)\n",
        "missing_in_panel = sorted([str(d) for d in (labels_dates - panel_dates)])\n",
        "_add_check(\"labels_dates_subset_of_panel\", len(missing_in_panel)==0, {\"labels_not_in_panel\": missing_in_panel[:10]})\n",
        "\n",
        "\n",
        "# Insert inside the validations block, before determinism check\n",
        "\n",
        "# 3.bis No gaps after stitching (business days)\n",
        "try:\n",
        "    bd = pd.bdate_range(start=labels[\"date\"].min(), end=labels[\"date\"].max(), freq=\"C\")  # business days\n",
        "    labd = pd.to_datetime(labels[\"date\"]).dt.normalize().unique()\n",
        "    labd = pd.DatetimeIndex(labd)\n",
        "    missing = bd.difference(labd)\n",
        "    # Allow known market holidays (we canâ€™t list them here), so only flag if large consecutive gaps\n",
        "    # TOCHANGE: tighten policy (e.g., require an exchange calendar)\n",
        "    ok_nogaps = len(missing) == 0\n",
        "    report_missing = [str(d.date()) for d in missing[:10]]\n",
        "    _add_check(\"no_business_day_gaps_in_labels\", ok_nogaps, {\"missing_first_10\": report_missing, \"missing_count\": int(len(missing))})\n",
        "except Exception as e:\n",
        "    _add_check(\"no_business_day_gaps_in_labels_error\", False, {\"error\": repr(e)})\n",
        "\n",
        "# 3.d Label semantics sanity (Risk-On higher mean than Risk-Off; vol ordering)\n",
        "# Prefer 2.6 table if available; else quick compute on full labeled set (heuristic).\n",
        "try:\n",
        "    if os.path.exists(PROF_TABLE):\n",
        "        prof = pd.read_csv(PROF_TABLE)\n",
        "        # columns expected: state_id, ret_mean, rv20_mean, ...\n",
        "        ron_mean = float(prof[\"ret_mean\"].max())\n",
        "        roff_vol = float(prof[\"rv20_mean\"].max())\n",
        "        ok = np.isfinite(ron_mean) and np.isfinite(roff_vol)\n",
        "        _add_check(\"semantics_profiles_present\", ok, {})\n",
        "    else:\n",
        "        # crude fallback by state_id on labeled rows\n",
        "        tmp = labels.merge(panel[[\"date\",\"spy_ret\",\"spy_rv_20\"]], on=\"date\", how=\"left\")\n",
        "        grp = tmp.groupby(\"state_id\", dropna=True).agg(ret_mean=(\"spy_ret\",\"mean\"), rv20_mean=(\"spy_rv_20\",\"mean\"))\n",
        "        cond = (grp[\"ret_mean\"].max() == grp[\"ret_mean\"].max()) and (grp[\"rv20_mean\"].max() == grp[\"rv20_mean\"].max())\n",
        "        _add_check(\"semantics_profiles_fallback\", bool(cond), {\"n_states\": int(grp.shape[0])})\n",
        "except Exception as e:\n",
        "    _add_check(\"semantics_profiles_error\", False, {\"error\": repr(e)})\n",
        "\n",
        "# 3.e Determinism smoke: rescore a tiny slice with same bundle\n",
        "# #TOCHANGE: widen slice or repeat N times in prod\n",
        "try:\n",
        "    # take last ~200 rows available in panelâˆ©labels\n",
        "    common = labels.merge(panel[[\"date\"]], on=\"date\", how=\"inner\").tail(200)\n",
        "    if not common.empty and \"scaler\" in bundle:\n",
        "        feats = bundle[\"features\"]\n",
        "        scaler = bundle[\"scaler\"]\n",
        "        # join features\n",
        "        X = panel.merge(common[[\"date\"]], on=\"date\", how=\"inner\").sort_values(\"date\")\n",
        "        X = X[feats].dropna().to_numpy(dtype=float)\n",
        "        post1 = bundle[\"model\"].predict_proba(scaler.transform(X))\n",
        "        post2 = bundle[\"model\"].predict_proba(scaler.transform(X))\n",
        "        max_diff = float(np.max(np.abs(post1 - post2)))\n",
        "        _add_check(\"determinism_same_inputs_same_scores\", bool(max_diff < 1e-10), {\"max_abs_diff\": max_diff})\n",
        "    else:\n",
        "        _add_check(\"determinism_skipped\", True, {\"reason\": \"No overlap or scaler missing in bundle.\"})\n",
        "except Exception as e:\n",
        "    _add_check(\"determinism_error\", False, {\"error\": repr(e)})\n",
        "\n",
        "# 3.f Leakage guard (structural): ensure features are present at t\n",
        "# (We canâ€™t fully prove non-leakage without lineage; this is a structural check.)\n",
        "feats = bundle.get(\"features\", [])\n",
        "missing_feats = [f for f in feats if f not in panel.columns]\n",
        "_add_check(\"features_present_in_panel\", len(missing_feats)==0, {\"missing\": missing_feats})\n",
        "\n",
        "# â”€â”€ 4) Save report + short fingerprint\n",
        "rep_path = os.path.join(REGIME_DIR, \"validation_report.json\")\n",
        "with open(rep_path, \"w\") as f:\n",
        "    json.dump(report, f, indent=2)\n",
        "\n",
        "# short human-readable fingerprint\n",
        "fp_lines = [\n",
        "    f\"Run: {manifest['created_at']}\",\n",
        "    f\"K={manifest['hmm']['k']}, cov={manifest['hmm']['covariance_type']}, sticky_lambda={manifest['hmm']['sticky_lambda']}\",\n",
        "    f\"Panel rows: {manifest['artifacts']['panel']['rows']}, Labels rows: {manifest['artifacts']['labels_stitched']['rows']}\",\n",
        "    f\"Latest window: {manifest['latest_window']['win_id']}  \"\n",
        "    f\"({manifest['latest_window']['train_start']}â†’{manifest['latest_window']['test_end']})\",\n",
        "    f\"Validation status: {report['status']}  (checks: {len(report['checks'])})\"\n",
        "]\n",
        "with open(os.path.join(REGIME_DIR, \"run_fingerprint.txt\"), \"w\") as f:\n",
        "    f.write(\"\\n\".join(fp_lines) + \"\\n\")\n",
        "\n",
        "print(json.dumps({\n",
        "    \"status\": \"2.10 reproducibility snapshot complete\",\n",
        "    \"manifest\": os.path.join(REGIME_DIR, \"run_manifest.json\"),\n",
        "    \"validation_report\": rep_path,\n",
        "    \"fingerprint\": os.path.join(REGIME_DIR, \"run_fingerprint.txt\"),\n",
        "    \"notes\": [\n",
        "        \"For production, expand determinism checks (multiple random restarts held fixed).\",  # TOCHANGE\n",
        "        \"Consider capturing git commit hash and Python/package versions for full lineage.\",  # TOCHANGE\n",
        "        \"Posterior-sum, date order, and feature presence checks included.\"\n",
        "    ]\n",
        "}, indent=2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7lWAdL5Migj",
        "outputId": "eddc4e03-03ae-47b0-de9f-ec2960f37842"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"status\": \"2.10 reproducibility snapshot complete\",\n",
            "  \"manifest\": \"artifacts/regimes/run_manifest.json\",\n",
            "  \"validation_report\": \"artifacts/regimes/validation_report.json\",\n",
            "  \"fingerprint\": \"artifacts/regimes/run_fingerprint.txt\",\n",
            "  \"notes\": [\n",
            "    \"For production, expand determinism checks (multiple random restarts held fixed).\",\n",
            "    \"Consider capturing git commit hash and Python/package versions for full lineage.\",\n",
            "    \"Posterior-sum, date order, and feature presence checks included.\"\n",
            "  ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>Summary of section 2</summary>\n",
        "\n",
        "# Section 2.0â€“2.1\n",
        "\n",
        "### **What Weâ€™ve Done**\n",
        "- **2.0**:  \n",
        "  - Defined configuration (`RegimeConfig`) for regime modeling â€” feature set, HMM params, recency weighting, I/O paths.\n",
        "  - Loaded `features_filtered.parquet` from Section 1 and **assembled a clean, date-aligned market panel** containing:\n",
        "    - SPY daily log returns (`spy_ret`)\n",
        "    - SPY realized volatility (20-day) (`spy_rv_20`)\n",
        "    - VIX level (`vix_close`) and change (`dvix`, optional)\n",
        "    - Market breadth (`breadth`)\n",
        "  - Saved this **market-level panel** to:\n",
        "    - `artifacts/regimes/market_panel.parquet` (+ CSV if enabled) â€” *core input for all subsequent regime modeling steps*.\n",
        "  - Wrote `regime_config_effective.json` â€” the **final config** used for reproducibility.\n",
        "\n",
        "- **2.1**:  \n",
        "  - Loaded the above market panel and **prepared train/test matrices for HMM** on a first walk-forward window:\n",
        "    - Features: `spy_rv_20`, `vix_close`, `breadth`, `dvix`\n",
        "    - Train period: `2007-02-06` â†’ `2016-12-30`\n",
        "    - Test period: `2017-01-03` â†’ latest date (`2025-08-08`)\n",
        "  - Standardized features **per train window** with `StandardScaler`; applied same transform to test.\n",
        "  - Saved:\n",
        "    - Per-window **scaler**: `scaler_<dates>.joblib`\n",
        "    - **QC JSON** with mean/std per feature in train vs test.\n",
        "    - **Window manifest** (`window_manifest.json`) describing date ranges, features, scaler path, and sample counts.\n",
        "\n",
        "---\n",
        "\n",
        "### **Artifacts for Reuse**\n",
        "| File | Contents | Purpose |\n",
        "|------|----------|---------|\n",
        "| `artifacts/regimes/market_panel.parquet` | Date-level DataFrame: `date`, `spy_ret`, `spy_rv_20`, `vix_close`, `breadth`, `dvix` (if enabled) | Core market context for HMM; already cleaned, aligned, NaN-free. |\n",
        "| `artifacts/regimes/regime_config_effective.json` | JSON dump of final `RegimeConfig` | Ensures downstream sections use same config; includes feature list, HMM params, I/O paths. |\n",
        "| `artifacts/regimes/scaler_<train>__<test>.joblib` | Fitted `StandardScaler` for the given walk-forward window | Apply same scaling to new data in this window. |\n",
        "| `artifacts/regimes/scaler_<train>__<test>_qc.json` | QC metrics: train/test row counts, means, stds per feature | For diagnostics and reproducibility. |\n",
        "| `artifacts/regimes/window_manifest.json` | Dict with train/test date ranges, feature list, scaler path, sample counts | Downstream code can iterate over these windows without recalculating splits. |\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Variables**\n",
        "| Variable | Value Example | Description |\n",
        "|----------|---------------|-------------|\n",
        "| `CFG` | `RegimeConfig(...)` | Active config object after merging defaults and `config.yaml`. |\n",
        "| `mkt` | Pandas DataFrame, ~4657 rows Ã— 6 cols | Market panel from 2.0; date-indexed features for HMM. |\n",
        "| `hmm_feat_cols` | `[\"spy_rv_20\", \"vix_close\", \"breadth\", \"dvix\"]` | Feature list for HMM modeling. |\n",
        "| `window` | Dict with keys: `X_train`, `X_test`, `dates_train`, `dates_test`, `scaler_path`, `qc` | All matrices and metadata for one walk-forward window. |\n",
        "| `manifest` | Dict with `window`, `features`, `scaler_path`, `n_train`, `n_test` | Summary of the first walk-forward split, persisted for reuse. |\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# 2.2 â€” Model Choice & Configuration (Gaussian HMM)\n",
        "\n",
        "## What we did\n",
        "- Trained a **GaussianHMM (covariance_type=\"full\")** on the standardized train window from 2.1.\n",
        "- Used a **time-decayed â€œrecencyâ€ sampler** to bias training toward recent data (segment length 60, 80 segments, half-life 756 days).\n",
        "- Enforced **regime persistence** with a sticky diagonal blend on the transition matrix (Î»=0.15).\n",
        "- Tried a small **K grid** (test run K=[2]) and **multiple inits** (2 restarts); **picked the best** by train log-likelihood on the *full* train sequence.\n",
        "- Persisted a self-contained bundle for reuse.\n",
        "\n",
        "## Key hyperparams (test run; marked **#TOCHANGE** in code)\n",
        "- `K grid`: `[2]` (real run: `[2, 3]`)\n",
        "- `n_iter`: `200` (real run: `1000`)\n",
        "- `n_init`: `2` (real run: `10`)\n",
        "- `tol`: `1e-3` (real run: `1e-4`)\n",
        "- `sticky_lambda`: `0.15` (real run: `0.30â€“0.50`)\n",
        "- Recency: `half_life_days=756`, `seg_len=60`, `n_segments=80`, `epsilon_floor=0.10`\n",
        "\n",
        "## Files produced (and whatâ€™s inside)\n",
        "- **`artifacts/regimes/regime_hmm.pkl`** *(joblib bundle)*  \n",
        "  - `model` â€” fitted `GaussianHMM`  \n",
        "  - `k`, `random_state`, `covariance_type`, `n_iter`, `n_init`, `tol`  \n",
        "  - `features` â€” the exact feature list used  \n",
        "  - **Scaler info:** `scaler_path` (points to the 2.1 scaler)  \n",
        "  - **Windows:** `train_dates`, `test_dates`  \n",
        "  - **Stickiness:** `sticky_lambda`  \n",
        "  - **Recency config** and `fit_mode`  \n",
        "  - `created_at`\n",
        "- **`artifacts/regimes/hmm_kgrid.json`** â€” scores for all `(k, seed)` runs and the chosen combo.\n",
        "\n",
        "## Reusable variables/outputs for Section 3\n",
        "- **Bundle** (`regime_hmm.pkl`) â€” trained HMM, feature list, scaler path, recency/stickiness configs.\n",
        "- **Feature list** â€” `bundle[\"features\"]`\n",
        "- **Hyperparams** â€” recency and persistence knobs, if needed for downstream logic.\n",
        "\n",
        "---\n",
        "\n",
        "# 2.3 â€” State Labeling & Semantics\n",
        "\n",
        "## What we did\n",
        "- Loaded `regime_hmm.pkl` + 2.1 scaler, scored posteriors for **all dates** in `market_panel.parquet`.\n",
        "- Built **state profiles** on TRAIN only:\n",
        "  - `spy_ret` (mean & std), `spy_rv_20` (mean), `vix_close` (mean), `breadth` (mean), `dvix` (if present), `ret_q05` (5% tail).\n",
        "- **Assigned semantic labels**:\n",
        "  - **Risk-On**: highest mean return (tie-breakers: breadthâ†‘, VIXâ†“, better tails)\n",
        "  - **Risk-Off**: highest vol & lowest return (tie-breakers: volâ†‘, Î”VIXâ†‘, breadthâ†“)\n",
        "  - **Transition**: remaining state\n",
        "- Persisted labels + posteriors and saved the label map to ensure stability.\n",
        "\n",
        "## Files produced\n",
        "- **`artifacts/regimes/regime_labels.parquet`** (+ CSV):\n",
        "  - `date`\n",
        "  - `state_id` (hard assignment)\n",
        "  - `p0..pK-1` (posteriors)\n",
        "  - `regime_label`\n",
        "- **`artifacts/regimes/state_profiles.csv`**:\n",
        "  - Per-state TRAIN stats (returns, vol, breadth, tails).\n",
        "- **`artifacts/regimes/regime_meta.json`**:\n",
        "  - `state_label_map`\n",
        "  - `diagnostics.state_profiles_train`\n",
        "  - `features_used`\n",
        "  - Notes on labeling policy.\n",
        "\n",
        "## Reusable variables/outputs for Section 3\n",
        "- **`regime_labels.parquet`** â€” main regime feed for Section 3:\n",
        "  - `regime_label` or max-posterior state.\n",
        "  - `p*` columns for confidence metrics.\n",
        "- **`regime_meta.json â†’ state_label_map`** â€” ensures consistent label meanings across windows.\n",
        "- **`state_profiles.csv`** â€” sanity check and seed values for regime-aware policies.\n",
        "\n",
        "---\n",
        "\n",
        "## Quick outputs recap from run\n",
        "- 2.2: `{\"chosen_k\": 2, \"fit_mode\": \"recency\", \"train_score\": -8178.567..., \"n_iter\": 200, \"n_init\": 2, \"sticky_lambda\": 0.15, \"half_life_days\": 756, \"seg_len\": 60, \"n_segments\": 80}`\n",
        "- 2.3: `{\"k\": 2, \"label_map\": {\"0\": \"Risk-On\", \"1\": \"Risk-Off\"}, \"profiles_path\": \".../state_profiles.csv\", \"labels_path\": \".../regime_labels.parquet\"}`\n",
        "\n",
        "# 2.4 â€” Smoothing, Persistence & Debounce\n",
        "\n",
        "## What we did\n",
        "- **Base path selection** (controlled by config):\n",
        "  - `SMOOTH_MTH` in `CFG` â†’ `\"viterbi\"` uses `model.predict(...)`; `\"posterior\"` uses `post.argmax(...)`.\n",
        "- **Posterior threshold gating** (debounce step 1):\n",
        "  - If a dayâ€™s new state differs from the prior day but the **max posterior** that day `< P_THRESH`, keep the **previous** state.\n",
        "  - Config knobs:\n",
        "    - `P_THRESH = CFG.posterior_thresh` (default **0.55**)\n",
        "    - `MIN_DWELL = CFG.min_dwell_days` (default **3**)\n",
        "    - `SMOOTH_MTH = CFG.smoothing_method` (default **\"posterior\"**)\n",
        "- **Minimum dwell enforcement** (debounce step 2):\n",
        "  - Collapse any **short runs** (`run_len < MIN_DWELL`) to the better neighbor using average posterior over the short segment.\n",
        "- **Label mapping**:\n",
        "  - Map smoothed state IDs to labels via `state_label_map` from `regime_meta.json` (set in 2.3).\n",
        "- **Gap handling**:\n",
        "  - Dates are already business days from the panel; no forward-looking fills are introduced.\n",
        "\n",
        "## Inputs reused\n",
        "- `artifacts/regimes/market_panel.parquet` (from 2.0)  \n",
        "- `artifacts/regimes/window_manifest.json` (from 2.1)  \n",
        "- `artifacts/regimes/regime_hmm.pkl` (from 2.2)  \n",
        "- `artifacts/regimes/regime_labels.parquet` (from 2.3)  \n",
        "- `artifacts/regimes/regime_meta.json` (from 2.3)\n",
        "\n",
        "## Outputs\n",
        "- **`artifacts/regimes/regime_labels.parquet`** *(updated in-place)*  \n",
        "  - `date`\n",
        "  - `p0..pK-1` (posteriors)\n",
        "  - `state_id` (original argmax)\n",
        "  - `state_id_smoothed` (after threshold + dwell collapse)  \n",
        "  - `regime_label_smoothed` (`state_id_smoothed` â†’ `state_label_map`)\n",
        "- **`artifacts/regimes/regime_labels.csv`**\n",
        "- **`artifacts/regimes/regime_meta.json`** *(updated)*  \n",
        "  - Adds `diagnostics.smoothing`:\n",
        "    - `method` (posterior|viterbi)\n",
        "    - `posterior_thresh`\n",
        "    - `min_dwell_days`\n",
        "    - `dwell_stats`\n",
        "\n",
        "## Reuse in Section 3+\n",
        "- Use **`regime_label_smoothed`** (or `state_id_smoothed`) as the regime signal.\n",
        "- Use **`p0..pK-1`** for confidence logic.\n",
        "- Read **`diagnostics.smoothing.dwell_stats`** for dwell/chattering monitoring.\n",
        "\n",
        "---\n",
        "\n",
        "# 2.5 â€” Robustness & Sensitivity\n",
        "\n",
        "## What we did\n",
        "- **Baseline context** from 2.2 and 2.1:\n",
        "  - `features_base = bundle[\"features\"]`\n",
        "  - `k_base = bundle[\"k\"]`\n",
        "  - Recency/sticky params from bundle.\n",
        "- **K sensitivity** (`K_GRID = [2, 3]`):\n",
        "  - Refit with recency-weighted subsequences.\n",
        "  - Score on train sequence; record best per K.\n",
        "  - Compute **agreement vs baseline**.\n",
        "- **Feature sensitivity**:\n",
        "  - Variants: `baseline`, `no_vix`, `no_breadth`, `no_dvix`, `core_rv_vix`.\n",
        "  - Refit at `k_base`, record label map, **agreement vs baseline**.\n",
        "- **Era stability**:\n",
        "  - Refit on `pre_2015`, `post_2015`, `crisis_2020`.\n",
        "  - Record profiles, label map, transition matrix.\n",
        "- **Bootstrap (block)**:\n",
        "  - `BLOCK_DAYS = 20`, `BOOT_REPS = 5` (light test).\n",
        "  - Refit and compute **agreement vs baseline**; report mean/std.\n",
        "\n",
        "> All refits fit a local `StandardScaler` on the relevant subset.\n",
        "\n",
        "## Inputs reused\n",
        "- `artifacts/regimes/market_panel.parquet` (2.0)  \n",
        "- `artifacts/regimes/window_manifest.json` (2.1)  \n",
        "- `artifacts/regimes/regime_hmm.pkl` (2.2)  \n",
        "- `artifacts/regimes/regime_labels.parquet` (2.3/2.4)\n",
        "\n",
        "## Outputs\n",
        "- **`artifacts/regimes/regime_sensitivity.json`**  \n",
        "  - `created_at`\n",
        "  - `inputs` (features_base, k_base, recency, sticky, etc.)\n",
        "  - `results`:\n",
        "    - **`k_sensitivity`**: best per K, agreement vs baseline, profiles, label map, transmat.\n",
        "    - **`feature_sensitivity`**: best per feature set, agreement, profiles, label map, transmat.\n",
        "    - **`era_stability`**: per era, profiles, label map, transmat.\n",
        "    - **`bootstrap`**: agreement list, mean, std.\n",
        "\n",
        "## Reuse in Section 3+\n",
        "- Pick **K** balancing separation and stability.\n",
        "- Decide on features based on agreement.\n",
        "- Adapt hedging/caps for era drift.\n",
        "- Gate production with bootstrap agreement thresholds.\n",
        "\n",
        "---\n",
        "\n",
        "## File Index for 2.4 & 2.5\n",
        "\n",
        "| File | Produced/Updated in | Purpose |\n",
        "|------|---------------------|---------|\n",
        "| `artifacts/regimes/regime_labels.parquet` (+ `.csv`) | 2.4 | Adds smoothed IDs/labels; ensures posteriors. |\n",
        "| `artifacts/regimes/regime_meta.json` | 2.4 | Smoothing diagnostics + state map. |\n",
        "| `artifacts/regimes/regime_sensitivity.json` | 2.5 | K/feature/era/bootstrap results with stability metrics. |\n",
        "\n",
        "# 2.6 & 2.7\n",
        "\n",
        "# 2.6 â€” Diagnostics & QA\n",
        "\n",
        "**What we did**\n",
        "- Computed core diagnostics from existing labels:\n",
        "  - Transition matrix, steadyâ€state distribution, dwell/run statistics, switch/chattering metrics.\n",
        "- Generated plots:\n",
        "  - `regime_timeline.png` (SPY price w/ regime shading), `timeline_drawdown.png`\n",
        "  - `regime_posteriors.png` (stacked pâ€™s)\n",
        "  - Per-state return histograms `state_<s>_ret_hist.png` and QQ plots `state_<s>_qq.png`\n",
        "  - `transition_matrix_heatmap.png`, `dwell_time_distribution.png`\n",
        "- Emitted tables and alerts for QA (semantics, dwell < 3d, chattering, mapping flips).\n",
        "\n",
        "**Reused inputs (exact paths/objects)**\n",
        "- `artifacts/regimes/market_panel.parquet` â†’ market series (`date, spy_ret, spy_rv_20, vix_close, breadth, dvix`)\n",
        "- `artifacts/regimes/window_manifest.json` â†’ window bounds (train/test)\n",
        "- `artifacts/regimes/regime_hmm.pkl` â†’ bundle with `features`, `k` (for K fallback)\n",
        "- `artifacts/regimes/regime_labels.parquet` â†’ label timeline\n",
        "  - Columns used: `date`, `p0..pK-1` (if present), `state_id` (or `state_id_smoothed`), `regime_label` (or `regime_label_smoothed`)\n",
        "- `artifacts/regimes/state_profiles.csv` â†’ state profile stats from 2.3 (fallback recompute if missing)\n",
        "- `artifacts/regimes/regime_meta.json` â†’ optional label map/notes\n",
        "\n",
        "**Key variables (in-code names)**\n",
        "- `DIAG_DIR = artifacts/regimes/diagnostics`\n",
        "- `state_col` = `\"state_id_smoothed\"` if present else `\"state_id\"`\n",
        "- `label_col` = `\"regime_label_smoothed\"` if present else `\"regime_label\"`\n",
        "- `p_cols` = all columns starting with `\"p\"`; `K = len(p_cols)` (else fallback to bundle `k`)\n",
        "- Diagnostics computed:\n",
        "  - `Tmat` (KÃ—K transition matrix), `ss_emp` (steady state)\n",
        "  - `dwell` (run lengths by state), `switches`, `switch_rate`\n",
        "  - `one_day_runs` (share of 1-day runs), `lt3_runs` (share runs <3d)\n",
        "  - `alerts` list (semantics, dwell, chattering, flips)\n",
        "\n",
        "**Outputs (exact filenames & contents)**\n",
        "- PNGs in `artifacts/regimes/diagnostics/`:\n",
        "  - `regime_timeline.png`, `timeline_drawdown.png`, `regime_posteriors.png`,\n",
        "    `state_<s>_ret_hist.png`, `state_<s>_qq.png`,\n",
        "    `transition_matrix_heatmap.png`, `dwell_time_distribution.png`\n",
        "- CSVs in `artifacts/regimes/diagnostics/`:\n",
        "  - `state_profiles_table.csv` (state_id, ret_mean, ret_std, rv20_mean, vix_mean, dvix_mean, breadth_mean, ret_q05)\n",
        "  - `transition_matrix.csv` (row=from_i, cols=to_j), `steady_state.csv` (state_id, steady_state_prob)\n",
        "  - `switches_by_year.csv` (year, n_switches), `summary_metrics.csv` (K, switches, switch_rate, one_day_runs_frac, lt3_runs_frac)\n",
        "- JSON:\n",
        "  - `alerts.json` (list of QA alerts)\n",
        "\n",
        "---\n",
        "\n",
        "# 2.7 â€” Regime-Aware Policy Hooks (Interfaces to Sections 3â€“5)\n",
        "\n",
        "**What we did**\n",
        "- Built a single hand-off file for downstream portfolio logic:\n",
        "  - Latest regime, smoothed confidence, and per-regime policy defaults (weights multipliers, turnover caps, risk targets, hedge intensity).\n",
        "- Confidence proxy combines **max posterior** and **(1 âˆ’ normalized entropy)**, then maps to an **aggressiveness scalar `g` âˆˆ [0.35, 1.00]**.\n",
        "- If `p*` columns are missing, we rescore posteriors from the HMM bundle.\n",
        "\n",
        "**Reused inputs (exact paths/objects)**\n",
        "- `artifacts/regimes/regime_labels.parquet` â†’ posteriors & (smoothed) states\n",
        "  - Uses `p_cols` when present; else rescored via bundle\n",
        "  - Picks `state_col` / `label_col` as in 2.6\n",
        "- `artifacts/regimes/regime_meta.json` â†’ `state_label_map` (state â†’ semantic label)\n",
        "- `artifacts/regimes/regime_hmm.pkl` â†’ `model`, `features`, `k`, `scaler_path` (for fallback scoring)\n",
        "- `artifacts/regimes/window_manifest.json` â†’ `scaler_path`, `window`\n",
        "- `artifacts/regimes/market_panel.parquet` â†’ fallback features matrix for rescoring\n",
        "\n",
        "**Key variables (in-code names)**\n",
        "- `OUT_PATH = artifacts/regimes/regime_policy_map.json`\n",
        "- `N_SMOOTH = 3` (days) â†’ average recent posteriors for confidence\n",
        "- `p_cols` (derived or rescored), `K` (len(p_cols) or bundle `k`)\n",
        "- `state_col`, `label_col` (same logic as 2.6)\n",
        "- Confidence helpers: `entropy(p)`, `aggressiveness_from_confidence(p)` â†’ returns `{c_max, c_entropy, c, g}`\n",
        "\n",
        "**Output (exact file & schema)**\n",
        "- `artifacts/regimes/regime_policy_map.json`\n",
        "  - Top-level:\n",
        "    - `created_at`, `latest_date`, `k`, `latest_regime_label`, `latest_state_id`\n",
        "    - `latest_posteriors`: `{ \"p0\": float, ..., \"p{K-1}\": float }`\n",
        "    - `confidence`:\n",
        "      - `aggressiveness_scalar_g` (float in [0.35, 1.00])\n",
        "      - `confidence`: `{ \"c_max\", \"c_entropy\", \"c\", \"g\" }`\n",
        "      - `recommendations`: `{ \"scale_position_sizes_by_g\", \"scale_turnover_cap_by_g\", \"scale_hedge_intensity_by_(1-g)\" }`\n",
        "    - `policy_by_regime`:\n",
        "      - Keys: semantic labels present in data (e.g., `\"Risk-On\"`, `\"Transition\"`, `\"Risk-Off\"`) or synthesized `State<i>` when no mapping.\n",
        "      - Values per regime:\n",
        "        - `weights_multipliers`: `{ \"momentum\", \"quality\", \"value\", \"low_vol\" }`\n",
        "        - `turnover_cap` (float)\n",
        "        - `risk_target_vol_annual` (float, e.g., 0.10/0.08/0.06)\n",
        "        - `hedge_intensity` (float)\n",
        "    - `inputs`:\n",
        "      - `labels_path`, `meta_path`, `bundle_path`, `scaler_path`\n",
        "      - `features` (list), `window` (train/test bounds), `smoothing_window_days`\n",
        "      - If present: `sensitivity_path = artifacts/regimes/regime_sensitivity.json`\n",
        "      - If present: `diagnostics_dir = artifacts/regimes/diagnostics`\n",
        "    - `signature` (sha256 over features/window/k)\n",
        "\n",
        "**How Section 3â€“5 should reuse**\n",
        "- Read **one file**: `artifacts/regimes/regime_policy_map.json`.\n",
        "  - Use `latest_regime_label` to branch logic.\n",
        "  - Scale exposures and caps by `confidence.aggressiveness_scalar_g`.\n",
        "  - Pull regime-specific knobs from `policy_by_regime[<label>]`.\n",
        "  - Optionally reference `inputs.sensitivity_path` and `inputs.diagnostics_dir` for auditability.\n",
        "\n",
        "\n",
        "## **Section 2.8 â€” Walk-Forward Integration**\n",
        "\n",
        "This section implements **rolling or expanding window walk-forward evaluation** for regime detection, matching the methodology in Section 6 (when available).  \n",
        "It ensures **out-of-sample (OOS) scoring** and prevents **regime meaning drift** by preserving the `state â†’ label` mapping per window.\n",
        "\n",
        "---\n",
        "\n",
        "### **Core Logic**\n",
        "1. **Window Handling**\n",
        "   - **Preferred:** Load `windows_manifest.json` (multi-window plan).\n",
        "   - **Fallback:** Wrap `window_manifest.json` (single-window).\n",
        "   - **Smoke Test Autogen:** Generate a small rolling test plan for quick runs.\n",
        "\n",
        "2. **Per-Window Workflow**\n",
        "   - **Train Phase:**\n",
        "     - Fit `StandardScaler` and `GaussianHMM` **only on training subset**.\n",
        "     - Apply **recency-weighted sampling** if enabled (`APPLY_RECENCY`).\n",
        "   - **Test Phase:**\n",
        "     - Transform features using the **train-fitted scaler**.\n",
        "     - Predict posteriors (`p0...pK-1`) and hard regime states.\n",
        "     - Map numeric states to semantic labels (`Risk-On`, `Risk-Off`, `Transition`) using training-set profiling.\n",
        "     - Apply light debouncing (`min_dwell_days`) to remove 1-day flips.\n",
        "   - **Save Artifacts:**\n",
        "     - Model bundle (`regime_hmm_<winid>.pkl`) with scaler + params.\n",
        "     - Labels (`regime_labels_<winid>.parquet` + `.csv`) with smoothed & raw states + posteriors.\n",
        "     - Metadata (`regime_meta_<winid>.json`) with window dates, features, mappings, and file paths.\n",
        "\n",
        "3. **Output Stitching**\n",
        "   - Concatenate all test chunks into a **continuous timeline** (`regime_labels.parquet` + `.csv`) for backtests.\n",
        "   - Save a **window index** (`windows_index.json` + `.csv`) with summary info.\n",
        "\n",
        "---\n",
        "\n",
        "### **Output Example**\n",
        "```json\n",
        "{\n",
        "  \"status\": \"2.8 walk-forward complete\",\n",
        "  \"n_windows\": 1,\n",
        "  \"windows\": [\n",
        "    {\n",
        "      \"win_id\": \"W0\",\n",
        "      \"train_start\": \"2007-02-06\",\n",
        "      \"train_end\": \"2016-12-30\",\n",
        "      \"test_start\": \"2017-01-03\",\n",
        "      \"test_end\": \"2025-08-08\",\n",
        "      \"n_train\": 2495,\n",
        "      \"n_test\": 2162,\n",
        "      \"bundle_path\": \"artifacts/regimes/windowed/regime_hmm_W0.pkl\",\n",
        "      \"labels_path\": \"artifacts/regimes/windowed/regime_labels_W0.parquet\",\n",
        "      \"meta_path\": \"artifacts/regimes/windowed/regime_meta_W0.json\"\n",
        "    }\n",
        "  ],\n",
        "  \"stitched_out\": {\n",
        "    \"parquet\": \"artifacts/regimes/regime_labels.parquet\",\n",
        "    \"csv\": \"artifacts/regimes/regime_labels.csv\"\n",
        "  },\n",
        "  \"windows_index\": {\n",
        "    \"json\": \"artifacts/regimes/windowed/windows_index.json\",\n",
        "    \"csv\": \"artifacts/regimes/windowed/windows_index.csv\"\n",
        "  },\n",
        "  \"notes\": [\n",
        "    \"Per-window scaler fitted on TRAIN only; TEST scored out-of-sample.\",\n",
        "    \"Stateâ†’label semantics are saved per window and applied to the test chunk.\",\n",
        "    \"For real run: increase N_ITER/N_INIT and recency sampler size; align windows with Section 6.\"\n",
        "  ]\n",
        "}\n",
        "\n",```
        "# other details about 2.8:\n",
        "Files Reused\n",
        "market_panel.parquet (from 2.0):\n",
        "Main panel of market features (date, spy_ret, and CFG.hmm_features), sorted by date.\n",
        "\n",
        "window_manifest.json / windows_manifest.json (from 2.1):\n",
        "Defines rolling/expanding window splits with train/test boundaries.\n",
        "\n",
        "regime_sensitivity.json (from 2.5, optional):\n",
        "Stores results of regime sensitivity tests (e.g., best K values).\n",
        "\n",
        "Diagnostics directory (from 2.6, optional):\n",
        "Extra per-state statistics or plots for debugging.\n",
        "\n",
        "Variables Reused\n",
        "CFG.hmm_features â€” Feature names for the HMM (from config).\n",
        "\n",
        "CFG.min_dwell_days â€” Minimum days before switching regimes.\n",
        "\n",
        "APPLY_RECENCY / HALF_LIFE_DAYS / SEG_LEN / N_SEGMENTS â€” Recency sampling parameters (from 2.2).\n",
        "\n",
        "K â€” Number of HMM states (can come from sensitivity analysis in 2.5).\n",
        "\n",
        "LAMBDA_STICK â€” Sticky transition smoothing factor.\n",
        "\n",
        "Artifacts Produced\n",
        "Per-Window:\n",
        "\n",
        "Model + scaler bundle â†’ regime_hmm_<winid>.pkl\n",
        "\n",
        "Regime labels â†’ regime_labels_<winid>.parquet (+ .csv)\n",
        "\n",
        "Metadata â†’ regime_meta_<winid>.json\n",
        "\n",
        "Global:\n",
        "\n",
        "Continuous labels â†’ regime_labels.parquet (+ .csv)\n",
        "\n",
        "Windows index â†’ windows_index.json (+ .csv)\n",
        "\n",
        "## 2.9 â€” Forward (Shadow) Mode\n",
        "\n",
        "### What we implemented\n",
        "- **Daily append loop**:\n",
        "  1) Load latest window metadata via `windows_index.json` (fallback: scan `regime_meta_*.json`), then load the **bundle** (`regime_hmm_<winid>.pkl`) â†’ `MODEL`, `SCALER`, `FEATS`, `K`.\n",
        "  2) Read newest feature rows from `market_panel.parquet` **after** the last date in `regime_labels.parquet`.\n",
        "  3) `SCALER.transform` â†’ `MODEL.predict_proba` â†’ `argmax` to get `state_id`.\n",
        "  4) Map `state_id` â†’ `regime_label` using metaâ€™s `state_label_map` (no re-profiling).\n",
        "  5) Append rows (with `p0..p{K-1}`) to `regime_labels.parquet` (+ CSV), **no backfill**.\n",
        "  6) **Log** each new row to JSONL with a **model signature hash**.\n",
        "  7) **Alerts**: rolling last `ROLL_WINDOW_D` days; flag if switches â‰¥ `ROLL_MAX_SWITCH`.\n",
        "  8) **Optional** policy refresh (2.7-lite): compute confidence on last `N_CONF_TAIL` posteriors and write `regime_policy_map.json`.\n",
        "\n",
        "- **Retrain cadence**: not in the daily path â€” set to weekly/bi-weekly (**#TOCHANGE**).\n",
        "\n",
        "---\n",
        "\n",
        "### Reusable globals / config knobs\n",
        "- Paths:\n",
        "  - `REGIME_DIR`, `PANEL_PATH`, `LAB_PATH_PQ`, `LAB_PATH_CSV`, `WIN_DIR`, `WIN_INDEX`\n",
        "- Optional:\n",
        "  - `START_DATE` (taken from `globals()` if present)\n",
        "- Policy refresh:\n",
        "  - `UPDATE_POLICY_MAP` (bool), `POLICY_OUT`\n",
        "- Logging & alerts:\n",
        "  - `FWD_LOG`, `ALERTS_FP`\n",
        "- Guardrails / smoothing:\n",
        "  - `FWD_DEBOUNCE` (bool), `ROLL_WINDOW_D` (int), `ROLL_MAX_SWITCH` (int)\n",
        "- Confidence window:\n",
        "  - `N_CONF_TAIL` (int)\n",
        "\n",
        "**Bundle/meta fields (loaded, reused):**\n",
        "- From `regime_hmm_<winid>.pkl` â†’ `BUNDLE`: `model`, `scaler`, `features`, `k`\n",
        "- From `regime_meta_<winid>.json` â†’ `META`: `\"state_label_map\"`, `\"window\"` (for signature)\n",
        "\n",
        "---\n",
        "\n",
        "### Reusable helper functions\n",
        "- `_load_latest_window_meta()`: choose latest window from `windows_index.json` (fallback scan of `regime_meta_*.json`); returns keys like `win_id`, `bundle_path`, `meta_path`, `test_end`.\n",
        "- `_model_signature(features, k, window)`: SHA256 hash for audit/lineage.\n",
        "- `_entropy(p)`: normalized entropy of a posterior vector.\n",
        "- `_aggressiveness_from_posterior(p_mean)`: returns `{c_max, c_entropy, c, g}` for policy scaling.\n",
        "- `_append_jsonl(path, rec)`: append a record to JSONL log.\n",
        "\n",
        "*(Smoke-test only, but reusable if desired)*  \n",
        "- `_pick_latest_window(win_index_json)`: pick latest window from index (used in forward smoke test).\n",
        "- `_debounce_series(state_ids, min_dwell_days=CFG.min_dwell_days)`: minimal 1-day blip squash (used in smoke test).\n",
        "\n",
        "---\n",
        "\n",
        "### Files / artifacts (with exact paths) and what they contain\n",
        "- **`artifacts/regimes/market_panel.parquet`**: full feature panel (`date` + `FEATS`) used to find new rows to score.\n",
        "- **`artifacts/regimes/windowed/windows_index.json`**: list of window records; includes `win_id`, `train_*`, `test_*`, `bundle_path`, `meta_path`, `labels_path`.\n",
        "- **`artifacts/regimes/windowed/regime_meta_<winid>.json`**: per-window meta including `\"state_label_map\"`, `\"window\"`, `\"features\"`, `\"k\"`, recency/sticky knobs, `\"bundle_path\"`.\n",
        "- **`artifacts/regimes/windowed/regime_hmm_<winid>.pkl`**: joblib bundle with `model` (GaussianHMM), `scaler` (StandardScaler), `features` (list), `k` (int).\n",
        "- **`artifacts/regimes/regime_labels.parquet`** (+ `regime_labels.csv`): master labels time series (stitched history + new forward rows). Columns:  \n",
        "  `date`, `state_id`, `regime_label`, `p0..p{K-1}`, and if present, `state_id_smoothed`, `regime_label_smoothed`.\n",
        "- **`artifacts/regimes/regime_forward_log.jsonl`**: one JSON record per appended row:  \n",
        "  `{ \"ts\", \"date\", \"model_sig\", \"state_id\", \"regime_label\", \"posteriors\": { \"p0\":..., ... } }`.\n",
        "- **`artifacts/regimes/forward_alerts.json`**: alerts like `\"High switch count in last {ROLL_WINDOW_D}d: {sw} (>= {ROLL_MAX_SWITCH})\"`.\n",
        "- **`artifacts/regimes/regime_policy_map.json`** (optional refresh): latest label, posteriors, confidence `{g, c_max, c_entropy, c}`, and `\"inputs\"` with paths + `\"signature\"`.\n",
        "\n",
        "*(Smoke-test outputs â€” safe sandbox)*  \n",
        "- **`artifacts/regimes/forward_smoketest/regime_labels_smoke_base.parquet`**: truncated base.\n",
        "- **`artifacts/regimes/forward_smoketest/regime_labels_smoke.parquet`** (+ CSV): base + newly scored tail.\n",
        "- **`artifacts/regimes/forward_smoketest/regime_policy_map_smoke.json`**: policy map built from smoke labels tail.\n",
        "\n",
        "---\n",
        "\n",
        "### Key columns appended each day\n",
        "- `date`, `state_id`, `regime_label`, `p0..p{K-1}`  \n",
        "- (If the historical file already had them) `state_id_smoothed`, `regime_label_smoothed` are mirrored from raw.\n",
        "\n",
        "## 2.10 â€” Configuration & Reproducibility\n",
        "\n",
        "### What we implemented\n",
        "- **Snapshot effective config & artifacts**:\n",
        "  - Captures `CFG` keys (HMM features, min dwell, posterior threshold, recency flags).\n",
        "  - Records artifact paths, row counts, SHA256 hashes.\n",
        "  - Stores latest window metadata (`win_id`, train/test dates, bundle/meta paths).\n",
        "  - Saves model parameters (`k`, covariance, n_iter, sticky_lambda, recency params).\n",
        "  - Optional: environment snapshot (Python, platform, package versions).\n",
        "\n",
        "- **Validations**:\n",
        "  - Config key presence.\n",
        "  - Dates sorted, no duplicates.\n",
        "  - Posterior columns present and sum to 1.\n",
        "  - Labelsâ€™ dates âŠ† panel dates, no unexpected business day gaps.\n",
        "  - Label semantics sanity check (risk-on/off profiles).\n",
        "  - Determinism check (same inputs â†’ identical posteriors).\n",
        "  - Features in bundle all present in panel (basic leakage guard).\n",
        "\n",
        "- **Auditability outputs**:\n",
        "  - `run_manifest.json` â†’ full config & artifact snapshot.\n",
        "  - `validation_report.json` â†’ pass/fail status of each check.\n",
        "  - `run_fingerprint.txt` â†’ short human-readable summary.\n",
        "\n",
        "---\n",
        "\n",
        "### Reusable globals / config knobs\n",
        "- **Paths**:\n",
        "  - `REGIME_DIR`, `PANEL_PATH`, `LABELS_PATH`, `CONFIG_EFF`,  \n",
        "    `WINDEX_PATH`, `WIN_DIR`, `DIAG_DIR`, `PROF_TABLE`\n",
        "- **From CFG**:\n",
        "  - `hmm_features`, `min_dwell_days`, `posterior_thresh`, `recency_weighting`\n",
        "\n",
        "---\n",
        "\n",
        "### Reusable helper functions\n",
        "- `_sha256_file(path)`: file hash for artifact integrity.\n",
        "- `_pick_latest_window(windex)`: load latest window metadata (fallback to W0).\n",
        "- `_add_check(name, ok, details)`: append validation result to report.\n",
        "\n",
        "---\n",
        "\n",
        "### Files / artifacts produced\n",
        "- **`artifacts/regimes/run_manifest.json`** â€” config + artifact snapshot with hashes, sizes, dates.\n",
        "- **`artifacts/regimes/validation_report.json`** â€” structured validation results (`status`, per-check pass/fail).\n",
        "- **`artifacts/regimes/run_fingerprint.txt`** â€” concise run summary (K, sticky_lambda, row counts, latest window).\n",
        "- **`artifacts/regimes/run_env.json`** *(optional)* â€” Python & package versions, platform info.\n",
        "\n",
        "---\n",
        "\n",
        "### Reused artifacts from earlier sections\n",
        "- `artifacts/regimes/market_panel.parquet` (2.0)\n",
        "- `artifacts/regimes/regime_labels.parquet` (2.8)\n",
        "- `artifacts/regimes/windowed/windows_index.json` (2.8 QoL)\n",
        "- Latest `regime_hmm_<win>.pkl` + `regime_meta_<win>.json` (2.8)\n",
        "- `artifacts/regimes/diagnostics/state_profiles_table.csv` (2.6; optional)\n",
        "\n",
        "# Quick summary of everything\n",
        "\n",
        "# ðŸ“¦ Section 2 â€” Regime Modeling (HMM â†’ Regime Labels & Probabilities)\n",
        "\n",
        "**Goal:** Detect daily market regimes (**Risk-On**, **Risk-Off**, **Transition**) with posterior probabilities, for use in Sections 3â€“5 (alpha models, sizing, risk caps).\n",
        "\n",
        "---\n",
        "\n",
        "## 1ï¸âƒ£ What We Built\n",
        "\n",
        "### 2.0â€“2.1 â€” Market Panel & Train/Test Prep\n",
        "- Created **clean, date-aligned market panel** from Section 1 features:\n",
        "  - `spy_ret` (SPY log returns)\n",
        "  - `spy_rv_20` (20d realized vol)\n",
        "  - `vix_close` (+ optional `dvix` daily change)\n",
        "  - `breadth` (% advancers in S&P)\n",
        "- Saved to: `artifacts/regimes/market_panel.parquet`\n",
        "- Generated **train/test matrices** for walk-forward window:\n",
        "  - Standardized **per-train-window** using `StandardScaler`\n",
        "  - QC checks: row counts, mean/std drift, NaNs\n",
        "  - Saved `scaler_<train>__<test>.joblib` + QC JSON + `window_manifest.json`\n",
        "\n",
        "### 2.2 â€” HMM Model Training\n",
        "- Trained **GaussianHMM** (`covariance_type=\"full\"`) with:\n",
        "  - Optional **recency-weighted sampling**\n",
        "  - Sticky transitions for persistence\n",
        "- Searched `K` in {2, 3}, picked best by log-likelihood\n",
        "- Saved self-contained bundle: model, scaler path, config, training dates\n",
        "\n",
        "### 2.3 â€” State Labeling\n",
        "- Profiled states on train set â†’ assigned semantic labels:\n",
        "  - Risk-On: highest mean return, lowest vol\n",
        "  - Risk-Off: highest vol, lowest return\n",
        "  - Transition: remainder\n",
        "- Persisted mapping in `regime_meta.json`\n",
        "- Created regime timeline with posteriors\n",
        "\n",
        "### 2.4 â€” Smoothing & Debounce\n",
        "- Removed short noisy flips using:\n",
        "  - Posterior threshold (`posterior_thresh`)\n",
        "  - Min dwell days (`min_dwell_days`)\n",
        "- Updated regime labels with smoothed states\n",
        "\n",
        "### 2.5 â€” Robustness Tests\n",
        "- Sensitivity to:\n",
        "  - `K` choice\n",
        "  - Feature removal\n",
        "  - Era splits (pre/post-2015, 2020 crisis)\n",
        "- Block bootstrap stability check\n",
        "- Saved results for audit\n",
        "\n",
        "### 2.6 â€” Diagnostics & QA\n",
        "- Computed:\n",
        "  - Transition matrix, dwell-time stats, chattering metrics\n",
        "  - State return distributions & QQ plots\n",
        "- Generated plots + summary CSVs + QA alerts\n",
        "\n",
        "### 2.7 â€” Regime Policy Map\n",
        "- Created single JSON for downstream use:\n",
        "  - Latest regime + confidence score\n",
        "  - Per-regime weights, turnover caps, risk targets, hedge intensity\n",
        "  - Confidence scalar `g` âˆˆ [0.35, 1.00]\n",
        "\n",
        "### 2.8 â€” Walk-Forward Integration\n",
        "- Automated multi-window HMM training & stitching of test outputs\n",
        "- Ensured **stateâ†’label** stability across windows\n",
        "- Produced continuous regime timeline for backtests\n",
        "\n",
        "### 2.9 â€” Forward (Shadow) Mode\n",
        "- Daily append loop:\n",
        "  - Score new rows from `market_panel.parquet`\n",
        "  - Append regime + posteriors to `regime_labels.parquet`\n",
        "  - Optional policy map refresh\n",
        "  - Alerts if excessive regime switches\n",
        "\n",
        "### 2.10 â€” Config & Reproducibility\n",
        "- Snapshotted:\n",
        "  - Effective config\n",
        "  - Artifact paths & hashes\n",
        "  - Validation checks\n",
        "- Produced concise run fingerprint\n",
        "\n",
        "---\n",
        "\n",
        "## 2ï¸âƒ£ Key Global Variables & Functions (Reusable)\n",
        "\n",
        "| Name | Type | Description |\n",
        "|------|------|-------------|\n",
        "| `CFG` | `RegimeConfig` | Loaded from `config.yaml` + defaults; holds all regime model params & paths |\n",
        "| `mkt` | `pd.DataFrame` | Clean market panel (`market_panel.parquet`) |\n",
        "| `hmm_feat_cols` | `list[str]` | Features used for HMM (e.g. `[\"spy_rv_20\",\"vix_close\",\"breadth\",\"dvix\"]`) |\n",
        "| `window` | `dict` | Train/test matrices & metadata for one walk-forward window |\n",
        "| `manifest` | `dict` | Summary of window bounds, features, scaler path, sample counts |\n",
        "| `_entropy(p)` | `func` | Normalized entropy from posterior vector |\n",
        "| `_aggressiveness_from_posterior(p)` | `func` | Returns `{c_max, c_entropy, c, g}` for sizing/risk scaling |\n",
        "| `_debounce_series(states, min_dwell)` | `func` | Remove short state flips |\n",
        "| `_model_signature(features,k,window)` | `func` | SHA256 signature for auditability |\n",
        "| `_load_latest_window_meta()` | `func` | Loads latest walk-forward model/meta paths |\n",
        "\n",
        "---\n",
        "\n",
        "## 3ï¸âƒ£ Artifacts & Their Contents\n",
        "\n",
        "| File | Purpose | Key Fields |\n",
        "|------|---------|------------|\n",
        "| `market_panel.parquet` | Core HMM input features | `date, spy_ret, spy_rv_20, vix_close, breadth, dvix` |\n",
        "| `regime_hmm.pkl` | HMM model bundle | model, features, scaler_path, training dates, config |\n",
        "| `regime_labels.parquet` (+ CSV) | Regime timeline | `date, state_id, p0..pK-1, regime_label` (+ smoothed) |\n",
        "| `regime_meta.json` | Stateâ†’label mapping & diagnostics | mapping, profiles, config |\n",
        "| `state_profiles.csv` | Per-state stats | mean/std returns, vol, VIX, breadth, tails |\n",
        "| `regime_sensitivity.json` | Robustness test results | k/feature/era/bootstrap outcomes |\n",
        "| `diagnostics/*.png` | Plots | timeline, posteriors, histograms, QQ, transmat, dwell dist |\n",
        "| `diagnostics/*.csv` | Metrics | state_profiles_table, transition_matrix, steady_state, run stats |\n",
        "| `regime_policy_map.json` | Per-regime strategy knobs | latest regime, g-scalar, per-regime caps & weights |\n",
        "| `windows_index.json` | Walk-forward plan | window IDs, dates, artifact paths |\n",
        "| `regime_forward_log.jsonl` | Forward mode log | date, state, posteriors, model signature |\n",
        "| `forward_alerts.json` | Alerts | excessive switching, anomalies |\n",
        "| `run_manifest.json` | Full config + artifact snapshot | cfg keys, paths, hashes |\n",
        "| `validation_report.json` | Pass/fail checks | leakage, dates, semantics |\n",
        "| `run_fingerprint.txt` | Short summary | key params & latest status |\n",
        "\n",
        "---\n",
        "\n",
        "## 4ï¸âƒ£ How to Reuse in Later Sections\n",
        "\n",
        "- **For alpha models (Section 3)**  \n",
        "  - Read `regime_labels.parquet` (use smoothed label columns)  \n",
        "  - Use `p*` columns for regime-confidence scaling  \n",
        "  - Read `regime_policy_map.json` to set factor weights, turnover caps, hedge targets  \n",
        "\n",
        "- **For walk-forward runs**  \n",
        "  - Use `windows_index.json` to iterate windows  \n",
        "  - Load matching `regime_hmm_<win>.pkl` and `regime_meta_<win>.json`  \n",
        "\n",
        "- **For forward mode**  \n",
        "  - Extend `regime_labels.parquet` daily using `_load_latest_window_meta()` and scoring pipeline  \n",
        "  - Refresh `regime_policy_map.json` as needed  \n",
        "\n",
        "- **For diagnostics or tuning**  \n",
        "  - Use `regime_sensitivity.json` to choose stable `K` and feature set  \n",
        "  - Use `diagnostics/` CSVs for deeper QA or visual overlays  \n",
        "\n",
        "---\n",
        "\n",
        "## 5ï¸âƒ£ Deliverables Checklist âœ…\n",
        "\n",
        "- [x] `regime_labels.parquet` (+ CSV)  \n",
        "- [x] `regime_hmm.pkl`  \n",
        "- [x] `regime_meta.json`  \n",
        "- [x] `regime_timeline.png`, `regime_posteriors.png`  \n",
        "- [x] `state_profiles.csv`  \n",
        "- [x] `transition_matrix.csv`  \n",
        "- [x] `regime_sensitivity.json`  \n",
        "- [x] `regime_policy_map.json`  \n",
        "\n",
        "---\n",
        "\n",
        "**Next Dev Tip:**  \n",
        "All core regime logic is already modularized. Before coding, scan `regime_policy_map.json` and `regime_labels.parquet` â€” they cover 90% of what youâ€™ll need without touching model code.\n",
        "\n",
        "\n",
        "\n",
        "</details>\n"
      ],
      "metadata": {
        "id": "i11zULMBWjdn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.11"
      ],
      "metadata": {
        "id": "QR0dVyT6SqgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Alpha Layer (Signals)"
      ],
      "metadata": {
        "id": "j3s49iJvw3VU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ErnR5GetzRjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Portfolio Construction & Risk"
      ],
      "metadata": {
        "id": "GB56azskw8x_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XIo2hIhCzR14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. RL Sizing Policy (PPO)"
      ],
      "metadata": {
        "id": "OVTV8iNaxcly"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nR9-doWYzSSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Backtesting (Backward Testing) â€” Rigor"
      ],
      "metadata": {
        "id": "MxL1szv9x19g"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_8u_WWZvzStF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Forward Testing (No Orders; Shadow Runs)"
      ],
      "metadata": {
        "id": "24zaJ3GOx5RJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "45bA-KpizTFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Cost Model & Execution Assumptions"
      ],
      "metadata": {
        "id": "o_yX0Y24x8qb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H-8_FoZ5zTl-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Reproducibility & Testability"
      ],
      "metadata": {
        "id": "ic1uXdOax_ak"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jTVyA9gyzT-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. Visualization & Reporting"
      ],
      "metadata": {
        "id": "NJVGj0mKyBZX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VYhvh69GzUaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11. Automation Options (Optional, no trading)"
      ],
      "metadata": {
        "id": "cL8XA8jqyDfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t1tL4b7EzU3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12. Optional Alpaca Integration (disabled by default)"
      ],
      "metadata": {
        "id": "0Ywf60UCyFbB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ab8-P8WEzWq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 13. File/Module Structure (Colab-friendly)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "/project\n",
        "  config.yaml\n",
        "  data/\n",
        "    universe.csv\n",
        "    features.parquet\n",
        "    regime_labels.parquet\n",
        "  models/\n",
        "    lstm_*.pt / .h5\n",
        "    gbm_*.txt\n",
        "    stacker_*.pkl\n",
        "    rl_policy_*.pkl\n",
        "  runs/YYYY-MM-DD/\n",
        "    signals.parquet\n",
        "    weights.parquet\n",
        "    hedges.parquet\n",
        "    daily_pnl.csv\n",
        "    risk.json\n",
        "  reports/\n",
        "    backtest_tearsheet.html\n",
        "    forward_tearsheet_YYYY-MM.html\n",
        "  src/\n",
        "    data_loader.py\n",
        "    feature_engineering.py\n",
        "    regime.py\n",
        "    models_lstm.py\n",
        "    models_tabular.py\n",
        "    stacking.py\n",
        "    uncertainty.py\n",
        "    portfolio_bl_rp.py\n",
        "    hedging.py\n",
        "    rl_policy.py\n",
        "    backtest.py\n",
        "    forward_shadow.py\n",
        "    risk_metrics.py\n",
        "    stats_tests.py  # DM, SPA/White RC, Sharpe inference\n",
        "    monte_carlo.py  # block bootstrap\n",
        "    reporting.py    # plots & HTML/PDF\n",
        "  main.py          # CLI: daily-shadow / weekly-train / monthly-report\n",
        "  notebook.ipynb   # Colab master: end-to-end run with toggles\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "Jb1M33e5yJKy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 14. More info\n",
        "\n",
        "- Suggested stack: pandas, numpy, scikit-learn, lightgbm, xgboost, tensorflow/PyTorch (choose one for LSTM), hmmlearn, stable-baselines3, cvxpy (for BL/optimization), arch (optional), statsmodels, scipy, matplotlib/plotly.\n",
        "\n",
        "Compute plan (fits $50â€“$100):\n",
        "\n",
        "- S&P 100, 5â€“8 walk-forward windows.\n",
        "\n",
        "- LSTM 1â€“2 layers (64â€“128 units), MC-dropout 20 samples.\n",
        "\n",
        "- PPO with modest timesteps per window.\n",
        "\n",
        "- 200â€“400 Monte Carlo bootstrap paths.\n",
        "\n",
        "- 1â€“3 GPU hours on Colab Pro/Pro+; RAM < 24GB."
      ],
      "metadata": {
        "id": "LYsWzmxjyXy8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 15. Build Order (fastest to value)\n",
        "\n",
        "1. Data + Features + Regimes â†’ validate leakage & plots.\n",
        "\n",
        "2. Multifactor composite â†’ baseline cross-sec L/S backtest.\n",
        "\n",
        "3. GBM/MLP + LSTM â†’ stacking + uncertainty; re-run backtest.\n",
        "\n",
        "4. BL + RP + Dynamic hedge â†’ re-run backtest & stress.\n",
        "\n",
        "5. RL sizing â†’ ablation vs no-RL; finalize backtest.\n",
        "\n",
        "6. Forward shadow loop (daily), weekly retrain, monthly reports.\n",
        "\n",
        "7. Automation (Actions/cron), optional Alpaca paper stub (off)."
      ],
      "metadata": {
        "id": "KZCqpKiHynK6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 16. What you'll see in the first results\n",
        "- Backtest tear sheet with OO-S equity curve, MC bands, by-regime tables, SPA/DM outcomes, VaR/CVaR & stress.\n",
        "\n",
        "- Ablation:\n",
        "\n",
        "  - Multifactor only â†’ +ML â†’ +ML+RL;\n",
        "\n",
        "  - Market-neutral vs long-only w/ hedging;\n",
        "\n",
        "  - Cost sensitivity 5â€“20 bps.\n",
        "\n",
        "- A live forward dashboard (from Day 1) accumulating daily PnL + monthly report.\n",
        "\n"
      ],
      "metadata": {
        "id": "GR2YWGzNy6Ip"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 17. Forward-Testing Duration Recommendation\n",
        "\n",
        "- Run at least 4 weeks forward shadow to confirm plumbing & stability.\n",
        "\n",
        "- Prefer 8â€“12 weeks to evaluate regime adaptation, RL sizing behavior under drawdowns, and cost realism.\n",
        "\n",
        "- Only after the forward period matches backtest risk/return within expected error bands should you consider paper-trading execution.\n",
        "\n"
      ],
      "metadata": {
        "id": "2Y4YzjnzzF_r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2rj3UeBraWs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary><strong>Outline Details</strong></summary>\n",
        "\n",
        "# Project Outline â€” Regime-Aware Multifactor + LSTM/Ensembles + RL (with rigorous back & forward testing)\n",
        "\n",
        "## 0) Objectives & Success Criteria\n",
        "**Primary objective:** Generate statistically significant pure alpha (market-neutral) with controlled drawdowns after transaction costs.  \n",
        "\n",
        "**Secondary objective:** Build a repeatable process capable of ongoing, unattended forward testing that outputs monthly tear sheets.  \n",
        "\n",
        "**Pass/Fail gates (OO-S):**  \n",
        "- Annualized Sharpe â‰¥ 1.0 (cost-adjusted) across walk-forward windows.  \n",
        "- SPA/White Reality Check non-rejection vs family of alternatives at 5â€“10% level.  \n",
        "- Max DD â‰¤ 15â€“20% (tunable) in backtests.  \n",
        "- Forward test (4â€“8+ weeks): positive return, rolling Sharpe > 0.8, tail losses consistent with backtest VaR/CVaR.  \n",
        "\n",
        "---\n",
        "\n",
        "## 1) Data & Universe\n",
        "\n",
        "### 1.1 Universe\n",
        "- S&P 100 equities (liquid, keeps compute sane).  \n",
        "- Hedging instruments: SPY + sector ETFs (XLY, XLF, XLV, XLK, XLI, XLE, XLP, XLB, XLU, XLRE).  \n",
        "- Source: Yahoo Finance (daily bars).  \n",
        "- Lookback: 10â€“15 years if available (train 2012â†’, test recent).  \n",
        "\n",
        "### 1.2 Features\n",
        "- **Returns/vol:** log returns (1â€“60d lags), realized vol, ATR.  \n",
        "- **Momentum:** 12â€“1, 6â€“1, 20d, trend filters (e.g., SMA cross, slope).  \n",
        "- **Value:** B/P, E/P, CF/P, shareholder yield (latest available; forward-fill monthly/quarterly).  \n",
        "- **Quality:** gross profitability, ROE, accruals, leverage, F-Score-like composite.  \n",
        "- **Market context:** VIX, SPY vol, market breadth (% advancers, optional).  \n",
        "- Leakage controls: strictly lag all features, align to t-1; winsorize & z-score cross-sectionally.  \n",
        "\n",
        "### 1.3 Data Hygiene\n",
        "- Survivorship-bias approach: use current S&P 100 for practicality; (optional) point-in-time later.  \n",
        "- Corporate actions: use adjusted prices.  \n",
        "- Missing fundamentals: impute conservatively or drop; record masks for model.  \n",
        "- **Deliverables:** `features.parquet`, `universe.csv`, `meta.yaml`.  \n",
        "\n",
        "---\n",
        "\n",
        "## 2) Regime Modeling\n",
        "\n",
        "### 2.1 HMM (2â€“3 states)\n",
        "- Inputs: SPY daily returns/vol, VIX level/change, market breadth.  \n",
        "- States: Risk-On, Risk-Off, Transition (labeled by average return/vol).  \n",
        "- **Output:** daily regime label + posterior probabilities.  \n",
        "\n",
        "### 2.2 Usage\n",
        "- Regime-specific ensemble weights, turnover caps, and risk targets.  \n",
        "- Momentum throttled in Risk-Off; quality emphasized.  \n",
        "- **Deliverables:** `regime_labels.parquet`, regime plot.  \n",
        "\n",
        "---\n",
        "\n",
        "## 3) Alpha Layer (Signals)\n",
        "\n",
        "### 3.1 Multifactor Composite\n",
        "- Value/Momentum/Quality composites (winsorized, z-scored).  \n",
        "- Per-regime blend fit with ridge.  \n",
        "- **Output:** factor alpha score per asset/day.  \n",
        "\n",
        "### 3.2 ML Overlays\n",
        "- **LSTM:** 60-day sequences â†’ t+5/t+10 returns; MC-dropout for uncertainty.  \n",
        "- **Tabular ensembles:** LightGBM (primary), XGBoost, small MLP; also quantile versions.  \n",
        "- **Stacking meta-learner:** ridge/LightGBM; OOF training within walk-forward train window.  \n",
        "- **Output:** final forecast (mean) + uncertainty proxy.  \n",
        "\n",
        "### 3.3 Uncertainty â†’ Confidence\n",
        "- Expected Sharpe proxy = mean / std_hat.  \n",
        "- Bucket confidence for analytics.  \n",
        "- **Deliverables:** `alpha_raw.parquet`, `alpha_ensemble.parquet`, feature importance charts.  \n",
        "\n",
        "---\n",
        "\n",
        "## 4) Portfolio Construction & Risk\n",
        "\n",
        "### 4.1 Baseline Weights\n",
        "- Cross-sectional L/S: long top decile, short bottom decile by forecasted Sharpe.  \n",
        "- Beta-neutral, per-name and sector caps.  \n",
        "\n",
        "### 4.2 Blackâ€“Litterman (BL)\n",
        "- Prior: market-cap weights â†’ implied Î¼.  \n",
        "- Views: ensemble alphas scaled by uncertainty.  \n",
        "- Posterior Î¼Ì‚ â†’ mean-variance with L2 & turnover penalty.  \n",
        "\n",
        "### 4.3 Risk Parity & Vol Target\n",
        "- Equalize risk across sector/factor clusters.  \n",
        "- Target portfolio vol (8â€“12% ann.).  \n",
        "\n",
        "### 4.4 Dynamic Hedging\n",
        "- Daily orthogonalization vs SPY + sectors; hedge ratios adjustable by RL.  \n",
        "- **Deliverables:** weights, exposures, hedge plots.  \n",
        "\n",
        "---\n",
        "\n",
        "## 5) RL Sizing Policy (PPO)\n",
        "\n",
        "### 5.1 Role\n",
        "- Scales risk target and tunes hedges.  \n",
        "\n",
        "### 5.2 State\n",
        "- Regime, vol, drawdown, alpha strength, uncertainty, turnover, betas, cost model.  \n",
        "\n",
        "### 5.3 Reward\n",
        "- PnL â€“ costs â€“ Î»Â·CVaR_tail â€“ ÎºÂ·Î”drawdown â€“ penalties.  \n",
        "\n",
        "### 5.4 Training\n",
        "- Train within walk-forward segments; fixed seeds.  \n",
        "- **Deliverables:** `rl_policy.pkl`, diagnostics.  \n",
        "\n",
        "---\n",
        "\n",
        "## 6) Backtesting (Backward Testing) â€” Rigor\n",
        "\n",
        "### 6.1 Walk-Forward Engine\n",
        "- Rolling/expanding windows; purged & embargoed CV.  \n",
        "- Refit all models per window; test daily with costs.  \n",
        "\n",
        "### 6.2 Significance & Reality Checks\n",
        "- DM test, SPA/White RC, Sharpe inference.  \n",
        "\n",
        "### 6.3 Tail Risk & Stress\n",
        "- VaR/CVaR; stress tests (2008/2020, vol shocks, liquidity cuts).  \n",
        "\n",
        "### 6.4 Monte Carlo Robustness\n",
        "- Block bootstrap; output PnL envelopes.  \n",
        "- **Deliverables:** equity curves, DD charts, ablations.  \n",
        "\n",
        "---\n",
        "\n",
        "## 7) Forward Testing (Shadow Mode)\n",
        "\n",
        "### 7.1 Daily Shadow Run\n",
        "- No backfill; use latest models; log all artifacts.  \n",
        "\n",
        "### 7.2 Retraining Cadence\n",
        "- Weekly or bi-weekly; strict forward-only.  \n",
        "\n",
        "### 7.3 Monthly Auto-Report\n",
        "- Tear sheets with returns, Sharpe, DD, risk, regime PnL, VaR/CVaR.  \n",
        "\n",
        "### 7.4 Duration\n",
        "- Min: 4 weeks; Pref: 8â€“12 weeks.  \n",
        "- **Deliverables:** daily run files, monthly reports.  \n",
        "\n",
        "---\n",
        "\n",
        "## 8) Cost Model & Execution Assumptions\n",
        "- Costs: 10 bps round-trip (sweep 5â€“20).  \n",
        "- Slippage: 1â€“2 bps; higher in Risk-Off.  \n",
        "- Short borrow: 10â€“50 bps ann.  \n",
        "- Liquidity caps: â‰¤5â€“10% ADV.  \n",
        "\n",
        "---\n",
        "\n",
        "## 9) Reproducibility & Testability\n",
        "- Config-driven (`config.yaml`); fixed seeds.  \n",
        "- Unit/integration tests for leakage, CV folds, NaNs, RL bounds.  \n",
        "- Experiment tracking with CSV/JSON + git hash.  \n",
        "\n",
        "---\n",
        "\n",
        "## 10) Visualization & Reporting\n",
        "- Equity curves with regime shading, rolling metrics, exposures, attribution, bucket PnL, by-regime performance, risk dashboards.  \n",
        "\n",
        "---\n",
        "\n",
        "## 11) Automation Options\n",
        "- **Colab:** manual or scheduled;  \n",
        "- **GitHub Actions:** nightly, weekly, monthly;  \n",
        "- **VM + cron:** low-budget option.  \n",
        "\n",
        "---\n",
        "\n",
        "## 12) Optional Alpaca Integration\n",
        "- Disabled by default; forward test never sends orders; later optional paper fills.  \n",
        "\n",
        "</details>\n"
      ],
      "metadata": {
        "id": "Wh8ahohLNbIu"
      }
    }
  ]
}
