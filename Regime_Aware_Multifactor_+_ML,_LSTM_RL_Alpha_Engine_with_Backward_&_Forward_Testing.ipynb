{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Regime-Aware Multifactor + ML/RL Alpha Engine with Backward & Forward Testing\n",
        "\n",
        "## Project Description\n",
        "\n",
        "This project is a **modular trading research system** designed to generate **pure alpha** (market-neutral returns independent of market beta) by combining **proven multifactor investing principles** with **modern machine learning and reinforcement learning techniques**, and testing them with **rigorous statistical validation**.\n",
        "\n",
        "The strategy’s profit engine comes from exploiting cross-sectional mispricings in a broad large-cap U.S. universe (**S&P 500 training set with dynamic top-N selection by confidence**) by identifying which stocks are likely to outperform or underperform others over the next 5–10 days. This is achieved through:\n",
        "\n",
        "- **Multifactor Alpha Layer:**  \n",
        "  - **Value** (cheap stocks with potential to mean-revert up)  \n",
        "  - **Momentum** (stocks in persistent trends)  \n",
        "  - **Quality** (financially strong, operationally robust companies)  \n",
        "  - Per-regime factor blending with shrinkage to avoid overfitting.\n",
        "\n",
        "- **Machine Learning Overlays:**  \n",
        "  - **LSTM** (sequence model) to capture time-series patterns in returns, volatility, and technicals.  \n",
        "  - **LightGBM/XGBoost/MLP** (tabular models) to detect nonlinear interactions in cross-sectional features.  \n",
        "  - **Stacking meta-learner** to optimally blend factor and ML outputs.  \n",
        "  - **Uncertainty quantification** via MC-dropout and quantile models to control position sizing.\n",
        "\n",
        "- **Regime Detection:**  \n",
        "  - Hidden Markov Model (HMM) to classify markets as **Risk-On**, **Risk-Off**, or **Transition**, adjusting model weights and risk accordingly.\n",
        "\n",
        "- **Portfolio Construction & Risk Management:**  \n",
        "  - **Black–Litterman optimization** to integrate model views with market-implied returns.  \n",
        "  - **Risk parity** to balance sector/factor exposures.  \n",
        "  - **Dynamic hedging** against SPY/sector ETFs to maintain market neutrality.\n",
        "\n",
        "- **Reinforcement Learning (PPO):**  \n",
        "  - Learns a sizing and hedging policy that adapts risk-taking to forecast strength, uncertainty, and current market regime, maximizing return per unit of tail risk (CVaR-aware reward).\n",
        "\n",
        "## Testing & Validation\n",
        "\n",
        "The project integrates **both backward and forward testing** to ensure robustness:\n",
        "\n",
        "- **Backward Testing (Historical):**  \n",
        "  - Walk-forward analysis with purged cross-validation to avoid look-ahead bias.  \n",
        "  - Statistical significance tests (Diebold–Mariano, SPA/White Reality Check) to confirm non-randomness.  \n",
        "  - Monte Carlo block bootstrap to estimate confidence intervals and failure probabilities.  \n",
        "  - VaR/CVaR analysis and stress testing against historical crisis scenarios.\n",
        "\n",
        "- **Forward Testing (Shadow, No Trades):**  \n",
        "  - Daily simulation using only forward data, logging PnL and risk metrics without sending orders.  \n",
        "  - Weekly retraining and monthly auto-generated tear sheets to track live performance against backtest expectations.  \n",
        "  - Recommended forward-testing period: 4–12 weeks before considering paper/live execution.\n",
        "\n",
        "## Goal\n",
        "\n",
        "The system’s goal is to produce **consistent, statistically validated alpha** with low correlation to the market and controlled drawdowns, using a combination of **factor investing, machine learning, and reinforcement learning**. This approach maximizes the probability of sustainable profitability before any real capital is risked.\n",
        "\n"
      ],
      "metadata": {
        "id": "Lh0jFF7ysB3v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Objectives & Success Criteria\n",
        "- Primary objective: Generate statistically significant pure alpha (market-neutral) with controlled drawdowns after transaction costs.\n",
        "\n",
        "- Secondary objective: Build a repeatable process capable of ongoing, unattended forward testing that outputs monthly tear sheets.\n",
        "\n",
        "- Pass/Fail gates (OO-S):\n",
        "  - Annualized Sharpe ≥ 1.0 (cost-adjusted) across walk-forward windows.\n",
        "  - SPA/White Reality Check non-rejection vs family of alternatives at 5–10% level.\n",
        "  - Max DD ≤ 15–20% (tunable) in backtests.\n",
        "  - Forward test (4–8+ weeks): positive return, rolling Sharpe > 0.8, tail losses consistent with backtest VaR/CVaR.\n",
        "\n"
      ],
      "metadata": {
        "id": "33IwP6ukt9c3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Data & Universe"
      ],
      "metadata": {
        "id": "dc5X57hiwx36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "if device == \"cuda\":\n",
        "    print(torch.cuda.get_device_name(0))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mwrsmOX39bI",
        "outputId": "8c2c9ce5-1124-4e7d-bfa5-8a8df0b8d5c8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip -q install yfinance pandas numpy PyYAML pyarrow statsmodels tenacity"
      ],
      "metadata": {
        "id": "8t8ZL73mzgiv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 1.1 UNIVERSE (UPDATED)\n",
        "# S&P 500 training set with dynamic top-N selection by confidence (later in pipeline).\n",
        "# Hedging instruments: SPY + sector ETFs.\n",
        "# Source: Yahoo Finance (daily bars). Lookback from 2006-01-01 to today.\n",
        "# Saves: universe.csv and raw_prices.parquet (OHLCV + Adj Close for all tickers incl. hedges + ^VIX)\n",
        "# ============================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yfinance as yf\n",
        "from datetime import datetime\n",
        "\n",
        "START_DATE = \"2006-01-01\"\n",
        "END_DATE = datetime.today().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "def to_fmp_symbol(sym: str) -> str:\n",
        "    # map Yahoo/WSJ style class tickers to FMP\n",
        "    return sym.replace(\"-\", \".\") if \"-\" in sym else sym\n",
        "\n",
        "def is_index_like(sym: str) -> bool:\n",
        "    # skip ^VIX and other index-style series for FMP backfill\n",
        "    return sym.startswith(\"^\")\n",
        "\n",
        "# --- Get S&P 500 constituents from Wikipedia (survivorship bias acknowledged) ---\n",
        "sp500_url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
        "tables = pd.read_html(sp500_url)\n",
        "sp500 = tables[0]  # first table\n",
        "tickers_raw = sp500[\"Symbol\"].tolist()\n",
        "# Some tickers on Wikipedia have periods; yfinance uses dashes for certain cases\n",
        "tickers = [t.replace(\".\", \"-\") for t in tickers_raw]\n",
        "\n",
        "# --- Hedging instruments (market & sector ETFs) ---\n",
        "hedges = [\"SPY\", \"XLY\", \"XLF\", \"XLV\", \"XLK\", \"XLI\", \"XLE\", \"XLP\", \"XLB\", \"XLU\", \"XLRE\"]\n",
        "context_symbols = [\"^VIX\"]  # market context series\n",
        "\n",
        "universe = sorted(set(tickers))\n",
        "universe_all = sorted(set(universe + hedges + context_symbols))\n",
        "\n",
        "# --- Save universe to CSV ---\n",
        "pd.DataFrame({\"ticker\": universe}).to_csv(f\"universe_{END_DATE}.csv\", index=False)\n",
        "pd.DataFrame({\"ticker\": universe}).to_csv(\"universe.csv\", index=False)  # pointer\n",
        "\n",
        "\n",
        "# --- Download daily OHLCV for all symbols ---\n",
        "# yfinance handles adjusted prices; we’ll keep both Close & Adj Close.\n",
        "data = yf.download(\n",
        "    universe_all,\n",
        "    start=START_DATE,\n",
        "    end=END_DATE,\n",
        "    auto_adjust=False,\n",
        "    group_by=\"ticker\",\n",
        "    progress=False,\n",
        "    threads=True,\n",
        ")\n",
        "\n",
        "if data is None or getattr(data, \"empty\", False):\n",
        "    raise RuntimeError(\"yfinance returned no data — try rerunning or chunking the request.\")\n",
        "\n",
        "def top_level_symbols(df):\n",
        "    # Handles both MultiIndex (normal multi-ticker) and flat columns (edge cases)\n",
        "    if isinstance(df.columns, pd.MultiIndex):\n",
        "        return set(df.columns.get_level_values(0))\n",
        "    # flat columns -> we can only have one symbol; yfinance puts OHLCV names as columns\n",
        "    return set()  # treat as empty to trigger backfill logic safely\n",
        "\n",
        "# added: tells us if yfinance skipped any tickers\n",
        "available = top_level_symbols(data)\n",
        "missing = [sym for sym in universe_all if sym not in available]\n",
        "if missing:\n",
        "    pd.Series(missing, name=\"missing_symbols\").to_csv(\"missing_symbols.csv\", index=False)\n",
        "    print(f\"WARNING: {len(missing)} symbols missing from download. Saved to missing_symbols.csv\")\n",
        "\n",
        "# Normalize to tidy format: MultiIndex -> long DataFrame\n",
        "frames = []\n",
        "if isinstance(data.columns, pd.MultiIndex):\n",
        "    for sym in universe_all:\n",
        "        if sym not in available:\n",
        "            continue\n",
        "        df = data[sym].copy()\n",
        "        df.columns = [c.lower().replace(\" \", \"_\") for c in df.columns]\n",
        "        df[\"ticker\"] = sym\n",
        "        frames.append(df.reset_index().rename(columns={\"Date\": \"date\"}))\n",
        "else:\n",
        "    # Edge: flat columns — shouldn't happen with many symbols, but keep it safe\n",
        "    df = data.copy()\n",
        "    df.columns = [c.lower().replace(\" \", \"_\") for c in df.columns]\n",
        "    df[\"ticker\"] = universe_all[0]\n",
        "    frames.append(df.reset_index().rename(columns={\"Date\": \"date\"}))\n",
        "\n",
        "prices = pd.concat(frames, ignore_index=True).sort_values([\"ticker\", \"date\"])\n",
        "prices[\"date\"] = pd.to_datetime(prices[\"date\"])\n",
        "\n",
        "# Basic sanity: drop rows with all NaNs for OHLCV\n",
        "keep_cols = [\"open\", \"high\", \"low\", \"close\", \"adj_close\", \"volume\"]\n",
        "prices = prices.dropna(subset=keep_cols, how=\"all\")\n",
        "\n",
        "# Save raw prices\n",
        "prices.to_parquet(\"raw_prices.parquet\", index=False)\n",
        "\n",
        "print(f\"Universe size (S&P 500): {len(universe)} tickers\")\n",
        "print(f\"Total symbols incl. hedges/context: {len(universe_all)}\")\n",
        "print(\"Saved: universe.csv, raw_prices.parquet\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7kItZpBzQyM",
        "outputId": "0b75d5f7-b5bc-4273-95ff-f502781765e7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Universe size (S&P 500): 502 tickers\n",
            "Total symbols incl. hedges/context: 514\n",
            "Saved: universe.csv, raw_prices.parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- Optional: Backfill any missing tickers with FMP (skip ^VIX etc.) ----\n",
        "import os, requests, time\n",
        "from getpass import getpass\n",
        "\n",
        "if os.path.exists(\"missing_symbols.csv\"):\n",
        "    missing = pd.read_csv(\"missing_symbols.csv\")[\"missing_symbols\"].tolist()\n",
        "else:\n",
        "    uni = pd.read_csv(\"universe.csv\")[\"ticker\"].tolist()\n",
        "    hedges = [\"SPY\",\"XLY\",\"XLF\",\"XLV\",\"XLK\",\"XLI\",\"XLE\",\"XLP\",\"XLB\",\"XLU\",\"XLRE\"]\n",
        "    context = [\"^VIX\"]\n",
        "    universe_all = sorted(set(uni + hedges + context))\n",
        "    base_prices = pd.read_parquet(\"raw_prices.parquet\")\n",
        "    present = set(base_prices[\"ticker\"].unique())\n",
        "    missing = [s for s in universe_all if s not in present]\n",
        "\n",
        "missing = [s for s in missing if not is_index_like(s)]\n",
        "if not missing:\n",
        "    print(\"No missing symbols to backfill.\")\n",
        "else:\n",
        "    print(f\"Backfilling {len(missing)} symbols from FMP (skipping indexes):\", missing[:8], \"...\")\n",
        "    FMP_API_KEY = os.environ.get(\"FMP_API_KEY\", \"\").strip() or getpass(\"Enter FMP API key for price backfill: \").strip()\n",
        "    if not FMP_API_KEY:\n",
        "        raise RuntimeError(\"FMP_API_KEY required for backfill.\")\n",
        "\n",
        "    base_url = \"https://financialmodelingprep.com/api/v3/historical-price-full\"\n",
        "    def fetch_fmp_prices(sym):\n",
        "        fmp_sym = to_fmp_symbol(sym)\n",
        "        url = f\"{base_url}/{fmp_sym}?from={START_DATE}&to={END_DATE}&serietype=line&apikey={FMP_API_KEY}\"\n",
        "        r = requests.get(url, timeout=30)\n",
        "        r.raise_for_status()\n",
        "        js = r.json()\n",
        "        hist = js.get(\"historical\", [])\n",
        "        if not hist:\n",
        "            return None\n",
        "        df = pd.DataFrame(hist)\n",
        "        df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
        "        # Map columns; fall back if adjClose missing\n",
        "        df = df.rename(columns={\"adjClose\":\"adj_close\"})\n",
        "        if \"adj_close\" not in df.columns:\n",
        "            df[\"adj_close\"] = df[\"close\"]\n",
        "        cols = [\"date\",\"open\",\"high\",\"low\",\"close\",\"adj_close\",\"volume\"]\n",
        "        for c in cols:\n",
        "            if c not in df.columns: df[c] = np.nan\n",
        "        df = df[cols]\n",
        "        df[\"ticker\"] = sym\n",
        "        return df.sort_values(\"date\")\n",
        "\n",
        "    filled = []\n",
        "    for i, sym in enumerate(missing, 1):\n",
        "        try:\n",
        "            df = fetch_fmp_prices(sym)\n",
        "            if df is not None and len(df):\n",
        "                filled.append(df)\n",
        "        except Exception:\n",
        "            pass\n",
        "        if i % 10 == 0:\n",
        "            time.sleep(0.5)  # be polite\n",
        "\n",
        "    if filled:\n",
        "        add = pd.concat(filled, ignore_index=True)\n",
        "        base_prices = pd.read_parquet(\"raw_prices.parquet\")\n",
        "        prices_fixed = pd.concat([base_prices, add], ignore_index=True).sort_values([\"ticker\",\"date\"])\n",
        "        prices_fixed.to_parquet(\"raw_prices.parquet\", index=False)\n",
        "        print(f\"Backfilled {add['ticker'].nunique()} symbols and re-saved raw_prices.parquet\")\n",
        "    else:\n",
        "        print(\"FMP backfill returned no data; proceeding without these tickers.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcFnrcEUU52H",
        "outputId": "d1162e85-6bc6-4b3c-9a7c-49098922e53e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No missing symbols to backfill.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 1.2 FEATURES (FMP Premium, no hard-coded key)\n",
        "# ------------------------------------------------------------\n",
        "# Builds:\n",
        "#   • Price/technical features (returns/vol/ATR/momentum/trend)\n",
        "#   • Market context (SPY vol, ^VIX, breadth)\n",
        "#   • Fundamentals via FMP (quarterly BS/IS/CF), cached per ticker,\n",
        "#     forward-filled to daily, and ratio metrics (Value + Quality)\n",
        "# Post-merge:\n",
        "#   • Leakage control (shift all predictive features by 1 day)\n",
        "#   • Winsorize & cross-sectional z-score (by date)\n",
        "#   • Fundamentals imputation + missing masks\n",
        "# Saves:\n",
        "#   • features.parquet\n",
        "#   • funda_quarterly.parquet, funda_daily.parquet\n",
        "#   • cache/funda_q_<TICKER>.parquet (per-ticker cache)\n",
        "# Notes:\n",
        "#   - API key is taken from env var FMP_API_KEY or prompted securely.\n",
        "#   - GPU not used here (CPU/I/O heavy); that’s normal.\n",
        "# ============================================================\n",
        "\n",
        "# %pip -q install yfinance pyarrow tenacity\n",
        "\n",
        "import os, time, random, gc\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yfinance as yf\n",
        "from getpass import getpass\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n",
        "\n",
        "# ---------- Config / Toggles ----------\n",
        "COMPUTE_SLOPE = True      # slope_20 via vectorized method\n",
        "SLOPE_WINDOW = 20\n",
        "RV_WIN = 20\n",
        "ATR_WIN = 14\n",
        "\n",
        "# Fundamentals provider config (FMP Premium)\n",
        "PROVIDER = \"fmp\"          # fixed to FMP for reliability\n",
        "FMP_API_KEY = os.environ.get(\"FMP_API_KEY\", \"\").strip()\n",
        "if not FMP_API_KEY:\n",
        "    # Prompt securely; not echoed, not written to disk\n",
        "    FMP_API_KEY = getpass(\"Enter your FMP API key (kept in-memory for this session): \").strip()\n",
        "if not FMP_API_KEY:\n",
        "    raise RuntimeError(\"FMP_API_KEY is required. Set env var FMP_API_KEY or enter it when prompted.\")\n",
        "\n",
        "def to_fmp_symbol(sym: str) -> str:\n",
        "    \"\"\"\n",
        "    Convert Yahoo-style tickers to FMP-style.\n",
        "    Yahoo uses '-' for class/shared tickers (e.g., BRK-B),\n",
        "    while FMP uses '.' (e.g., BRK.B). Everything else stays the same.\n",
        "    \"\"\"\n",
        "    # common class/delimiter cases\n",
        "    # e.g., BRK-B, BF-B, FOXA (no change), META (no change)\n",
        "    if \"-\" in sym:\n",
        "        return sym.replace(\"-\", \".\")\n",
        "    return sym\n",
        "\n",
        "# Chunking: Premium can fetch all at once. If you ever need throttling, set CHUNK_TICKERS to an int.\n",
        "CHUNK_TICKERS = 20        # TOCHANGE: None = process entire universe in one go > change this later to None to check all stocks instead of just 100\n",
        "START_AT = 0              # offset if chunking\n",
        "SKIP_IF_CACHED = True     # skip ticker if cache exists\n",
        "\n",
        "MAX_WORKERS = 4           # Premium can handle more concurrency; tune 4–12 as you like\n",
        "RETRY_ATTEMPTS = 5\n",
        "BATCH_SLEEP = (0.2, 0.6)  # polite jitter between HTTP calls\n",
        "CACHE_DIR = \"cache\"\n",
        "os.makedirs(CACHE_DIR, exist_ok=True)\n",
        "\n",
        "# ---------- Load raw prices & universe (from 1.1) ----------\n",
        "prices = pd.read_parquet(\"raw_prices.parquet\")\n",
        "universe_full = list(pd.read_csv(\"universe.csv\")[\"ticker\"])\n",
        "hedges = {\"SPY\", \"XLY\", \"XLF\", \"XLV\", \"XLK\", \"XLI\", \"XLE\", \"XLP\", \"XLB\", \"XLU\", \"XLRE\"}\n",
        "context_symbols = {\"^VIX\"}\n",
        "\n",
        "# ============================================================\n",
        "# A) PRICE / TECHNICAL FEATURES\n",
        "# ============================================================\n",
        "\n",
        "def compute_atr(df, window=ATR_WIN):\n",
        "    high, low, close = df[\"high\"], df[\"low\"], df[\"close\"]\n",
        "    prev_close = close.shift(1)\n",
        "    tr = pd.concat([(high - low),\n",
        "                    (high - prev_close).abs(),\n",
        "                    (low - prev_close).abs()], axis=1).max(axis=1)\n",
        "    return tr.rolling(window).mean()\n",
        "\n",
        "def vectorized_rolling_slope(y: pd.Series, window=SLOPE_WINDOW) -> pd.Series:\n",
        "    N = window\n",
        "    if N <= 1:\n",
        "        return pd.Series(np.nan, index=y.index, dtype=float)\n",
        "    x = np.arange(N, dtype=float)\n",
        "    Sx = x.sum()\n",
        "    Sxx = (x**2).sum()\n",
        "    yv = y.to_numpy(dtype=float)\n",
        "    yv = np.where(np.isfinite(yv), yv, 0.0)\n",
        "    k = np.ones(N, dtype=float)\n",
        "    Sy  = np.convolve(yv, k[::-1], mode=\"full\")[N-1:len(yv)+N-1]\n",
        "    Sxy = np.convolve(yv, x[::-1], mode=\"full\")[N-1:len(yv)+N-1]\n",
        "    denom = N * Sxx - Sx * Sx + 1e-12\n",
        "    slope = (N * Sxy - Sx * Sy) / denom\n",
        "    out = pd.Series(np.nan, index=y.index, dtype=float)\n",
        "    out.iloc[N-1:] = slope[N-1:]\n",
        "    return out\n",
        "\n",
        "def mom_over_n(adj_close, n):\n",
        "    return np.log(adj_close / adj_close.shift(n))\n",
        "\n",
        "feat_frames = []\n",
        "tickers = sorted(prices[\"ticker\"].unique())\n",
        "total = len(tickers)\n",
        "\n",
        "for i, (sym, df_sym) in enumerate(prices.groupby(\"ticker\"), start=1):\n",
        "    if sym in context_symbols:\n",
        "        continue\n",
        "    if i % 25 == 0:\n",
        "        print(f\"[Features] {i}/{total} processed… ({sym})\")\n",
        "\n",
        "    df = df_sym.sort_values(\"date\").copy()\n",
        "    df[\"ret_1d\"] = np.log(df[\"adj_close\"] / df[\"adj_close\"].shift(1))\n",
        "    for l in range(1, 61):\n",
        "        df[f\"ret_lag_{l}\"] = df[\"ret_1d\"].shift(l)\n",
        "\n",
        "    df[\"rv_20\"] = df[\"ret_1d\"].rolling(RV_WIN).std() * np.sqrt(252)\n",
        "    df[\"atr_14\"] = compute_atr(df, ATR_WIN)\n",
        "\n",
        "    df[\"mom_20\"]  = mom_over_n(df[\"adj_close\"], 20)\n",
        "    df[\"mom_6m\"]  = mom_over_n(df[\"adj_close\"], 126)\n",
        "    df[\"mom_12m\"] = mom_over_n(df[\"adj_close\"], 252)\n",
        "    df[\"mom_12_1\"] = np.log(df[\"adj_close\"].shift(21) / df[\"adj_close\"].shift(252))\n",
        "    df[\"mom_6_1\"]  = np.log(df[\"adj_close\"].shift(21) / df[\"adj_close\"].shift(126))\n",
        "\n",
        "    df[\"sma_20\"] = df[\"adj_close\"].rolling(20).mean()\n",
        "    df[\"sma_50\"] = df[\"adj_close\"].rolling(50).mean()\n",
        "    df[\"sma_20_gt_50\"] = (df[\"sma_20\"] > df[\"sma_50\"]).astype(\"float32\")\n",
        "    df[\"slope_20\"] = vectorized_rolling_slope(df[\"adj_close\"], window=SLOPE_WINDOW) if COMPUTE_SLOPE else np.nan\n",
        "\n",
        "    df[\"mom_20_vs_vol\"] = df[\"mom_20\"] / (df[\"ret_1d\"].rolling(20).std() + 1e-8)\n",
        "\n",
        "    feat_frames.append(df)\n",
        "\n",
        "features = pd.concat(feat_frames, ignore_index=True)\n",
        "\n",
        "# ============================================================\n",
        "# B) MARKET CONTEXT (SPY vol, VIX, breadth)\n",
        "# ============================================================\n",
        "\n",
        "vix = prices[prices[\"ticker\"] == \"^VIX\"][[\"date\", \"adj_close\"]].rename(columns={\"adj_close\": \"vix_close\"})\n",
        "spy = prices[prices[\"ticker\"] == \"SPY\"].copy()\n",
        "spy[\"spy_ret\"] = np.log(spy[\"adj_close\"] / spy[\"adj_close\"].shift(1))\n",
        "spy[\"spy_rv_20\"] = spy[\"spy_ret\"].rolling(20).std() * np.sqrt(252)\n",
        "ctx = spy[[\"date\", \"spy_rv_20\"]].merge(vix, on=\"date\", how=\"left\")\n",
        "\n",
        "rets = features.pivot(index=\"date\", columns=\"ticker\", values=\"ret_1d\")\n",
        "advancers = (rets > 0).sum(axis=1)\n",
        "# Fixed denominator for stability = full S&P 500 count from universe.csv\n",
        "breadth = (advancers / len(universe_full)).rename(\"breadth\")\n",
        "ctx = ctx.merge(breadth.reset_index(), on=\"date\", how=\"left\")\n",
        "\n",
        "features = features.merge(ctx, on=\"date\", how=\"left\")\n",
        "\n",
        "# ============================================================\n",
        "# C) FUNDAMENTALS (FMP Premium primary; cached per ticker)\n",
        "# ============================================================\n",
        "\n",
        "px_daily_all = prices[prices[\"ticker\"].isin(universe_full)][[\"date\", \"ticker\", \"adj_close\"]].copy()\n",
        "px_daily_all[\"date\"] = pd.to_datetime(px_daily_all[\"date\"])\n",
        "dates_all = px_daily_all[[\"date\"]].drop_duplicates().sort_values(\"date\")\n",
        "\n",
        "def _tidy_quarterly_df(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    if \"date\" in df.columns:\n",
        "        df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
        "    elif \"fillingDate\" in df.columns:\n",
        "        df[\"date\"] = pd.to_datetime(df[\"fillingDate\"])\n",
        "    return df\n",
        "\n",
        "def _coalesce_cols(df: pd.DataFrame, cols: list[str], default=np.nan) -> pd.Series:\n",
        "    avail = [c for c in cols if c in df.columns]\n",
        "    if not avail:\n",
        "        return pd.Series(default, index=df.index)\n",
        "    tmp = df[avail].apply(pd.to_numeric, errors=\"coerce\")\n",
        "    # first non-null across the candidate columns\n",
        "    s = tmp.bfill(axis=1).iloc[:, 0]\n",
        "    return s\n",
        "\n",
        "def _fetch_quarterly_funda_fmp(ticker: str) -> pd.DataFrame:\n",
        "    import requests\n",
        "    base = \"https://financialmodelingprep.com/api/v3\"\n",
        "    fmp_ticker = to_fmp_symbol(ticker)   # BRK-B -> BRK.B\n",
        "\n",
        "    def jget(url):\n",
        "        r = requests.get(url, timeout=30)\n",
        "        r.raise_for_status()\n",
        "        return r.json()\n",
        "\n",
        "    # pull a long history (Premium supports it)\n",
        "    bs = _tidy_quarterly_df(pd.DataFrame(jget(f\"{base}/balance-sheet-statement/{fmp_ticker}?period=quarter&limit=120&apikey={FMP_API_KEY}\")))\n",
        "    is_ = _tidy_quarterly_df(pd.DataFrame(jget(f\"{base}/income-statement/{fmp_ticker}?period=quarter&limit=120&apikey={FMP_API_KEY}\")))\n",
        "    cf = _tidy_quarterly_df(pd.DataFrame(jget(f\"{base}/cash-flow-statement/{fmp_ticker}?period=quarter&limit=120&apikey={FMP_API_KEY}\")))\n",
        "    if bs.empty and is_.empty and cf.empty:\n",
        "        raise RuntimeError(f\"FMP fundamentals empty for {ticker} (queried as {fmp_ticker})\")\n",
        "\n",
        "    out = bs.merge(is_, on=\"date\", how=\"outer\").merge(cf, on=\"date\", how=\"outer\")\n",
        "\n",
        "    # Coalesce across schema variants\n",
        "    out[\"book_equity\"]  = _coalesce_cols(out, [\"totalStockholdersEquity\",\"totalShareholderEquity\",\"totalEquity\"]).astype(float)\n",
        "    out[\"net_income\"]   = _coalesce_cols(out, [\"netIncome\",\"netIncomeApplicableToCommonShares\"]).astype(float)\n",
        "    out[\"ocf\"]          = _coalesce_cols(out, [\n",
        "        \"netCashProvidedByOperatingActivities\",\n",
        "        \"netCashProvidedByUsedInOperatingActivities\",\n",
        "        \"netCashProvidedByUsedInOperatingActivitiesContinuingOperations\"\n",
        "    ]).astype(float)\n",
        "    out[\"gross_profit\"] = _coalesce_cols(out, [\"grossProfit\"]).astype(float)\n",
        "    out[\"total_assets\"] = _coalesce_cols(out, [\"totalAssets\"]).astype(float)\n",
        "\n",
        "    # total_debt: prefer totalDebt; else short + long\n",
        "    td = _coalesce_cols(out, [\"totalDebt\"])\n",
        "    if td.isna().all():\n",
        "        short = _coalesce_cols(out, [\"shortTermDebt\",\"shortLongTermDebtTotal\"])\n",
        "        long  = _coalesce_cols(out, [\"longTermDebt\"])\n",
        "        td = (short.fillna(0) + long.fillna(0)).replace({0: np.nan})\n",
        "    out[\"total_debt\"] = td.astype(float)\n",
        "\n",
        "    # dividends / buybacks (raw signs as provided by FMP)\n",
        "    out[\"dividends\"] = _coalesce_cols(out, [\"dividendsPaid\",\"dividendsPaidCashFlow\"]).astype(float)\n",
        "    out[\"buybacks\"]  = _coalesce_cols(out, [\"commonStockRepurchased\",\"purchaseOfCommonStock\"]).astype(float)\n",
        "\n",
        "    out[\"ticker\"] = ticker  # keep Yahoo-style symbol for our dataset\n",
        "\n",
        "    cols = [\"date\",\"ticker\",\"book_equity\",\"net_income\",\"ocf\",\"gross_profit\",\n",
        "            \"total_assets\",\"total_debt\",\"dividends\",\"buybacks\"]\n",
        "    return out[cols].dropna(subset=[\"date\"])\n",
        "\n",
        "\n",
        "def fetch_or_load_cached_quarterly(ticker: str) -> pd.DataFrame | None:\n",
        "    path = os.path.join(CACHE_DIR, f\"funda_q_{ticker}.parquet\")\n",
        "    if SKIP_IF_CACHED and os.path.exists(path):\n",
        "        try:\n",
        "            return pd.read_parquet(path)\n",
        "        except Exception:\n",
        "            pass\n",
        "    try:\n",
        "        df = _fetch_quarterly_funda_fmp(ticker)\n",
        "        if df is None or df.empty:\n",
        "            return None\n",
        "        df.to_parquet(path, index=False)\n",
        "        time.sleep(random.uniform(*BATCH_SLEEP))  # polite pause\n",
        "        return df\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "# ---- SMOKE TEST (run once, then you can comment it out) ----\n",
        "try:\n",
        "    from IPython.display import display\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "test_syms = [\"AAPL\", \"MSFT\", \"BRK-B\", \"BF-B\"]\n",
        "for s in test_syms:\n",
        "    try:\n",
        "        df = _fetch_quarterly_funda_fmp(s)\n",
        "        # 👇 trim preview to match your price history\n",
        "        df = df[df[\"date\"] >= pd.to_datetime(START_DATE)]\n",
        "        print(s, \"→\", to_fmp_symbol(s), \"rows:\", len(df))\n",
        "        try:\n",
        "            display(df.head(2))\n",
        "        except Exception:\n",
        "            print(df.head(2))\n",
        "    except Exception as e:\n",
        "        print(\"ERR\", s, e)\n",
        "\n",
        "# Determine chunk (or all)\n",
        "if CHUNK_TICKERS:\n",
        "    end_at = min(len(universe_full), START_AT + CHUNK_TICKERS)\n",
        "    tickers_chunk = sorted(universe_full[START_AT:end_at])\n",
        "    print(f\"[FMP] Processing chunk {START_AT}:{end_at} (size={len(tickers_chunk)})\")\n",
        "else:\n",
        "    tickers_chunk = sorted(universe_full)\n",
        "    print(f\"[FMP] Processing entire universe (size={len(tickers_chunk)})\")\n",
        "\n",
        "# Parallel fetch with caching\n",
        "funda_parts, successes = [], 0\n",
        "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
        "    futs = {ex.submit(fetch_or_load_cached_quarterly, t): t for t in tickers_chunk}\n",
        "    for j, fut in enumerate(as_completed(futs), start=1):\n",
        "        df = fut.result()\n",
        "        if df is not None and len(df):\n",
        "            funda_parts.append(df)\n",
        "            successes += 1\n",
        "        if j % 25 == 0:\n",
        "            print(f\"[Fundamentals/FMP] {j}/{len(tickers_chunk)} processed… (successes: {successes})\")\n",
        "\n",
        "if successes == 0:\n",
        "    raise RuntimeError(\"No fundamentals fetched. Check your FMP key or try a smaller CHUNK_TICKERS with fewer workers.\")\n",
        "\n",
        "# Merge chunk with existing quarterly file (so multiple runs accumulate)\n",
        "q_path = \"funda_quarterly.parquet\"\n",
        "funda_q_chunk = pd.concat(funda_parts, ignore_index=True).sort_values([\"ticker\",\"date\"])\n",
        "\n",
        "# after you build funda_q_chunk\n",
        "cutoff = pd.to_datetime(px_daily_all[\"date\"].min())\n",
        "funda_q_chunk = funda_q_chunk[funda_q_chunk[\"date\"] >= cutoff]\n",
        "\n",
        "if os.path.exists(q_path):\n",
        "    old = pd.read_parquet(q_path)\n",
        "    old[\"date\"] = pd.to_datetime(old[\"date\"])\n",
        "    old = old[old[\"date\"] >= cutoff]  # <- trim the old file too\n",
        "    funda_q = (\n",
        "        pd.concat([old, funda_q_chunk], ignore_index=True)\n",
        "          .drop_duplicates([\"ticker\",\"date\"], keep=\"last\")\n",
        "          .sort_values([\"ticker\",\"date\"])\n",
        "    )\n",
        "else:\n",
        "    funda_q = funda_q_chunk\n",
        "\n",
        "funda_q = funda_q.sort_values([\"ticker\",\"date\"])\n",
        "funda_q.to_parquet(q_path, index=False)\n",
        "print(\n",
        "    f\"Saved: {q_path}  \"\n",
        "    f\"(tickers with funda total: {funda_q['ticker'].nunique()}, \"\n",
        "    f\"rows: {len(funda_q)})\"\n",
        ")\n",
        "\n",
        "# Quarterly → daily (forward-fill per ticker) across ALL tickers collected so far\n",
        "ff = []\n",
        "for sym, grp in funda_q.groupby(\"ticker\"):\n",
        "    g = dates_all.merge(grp, on=\"date\", how=\"left\")\n",
        "    g[\"ticker\"] = sym\n",
        "    g = g.sort_values(\"date\").ffill()\n",
        "    ff.append(g)\n",
        "funda_daily = pd.concat(ff, ignore_index=True)\n",
        "\n",
        "# Ratios (Value & Quality)\n",
        "fd = funda_daily.merge(px_daily_all, on=[\"date\",\"ticker\"], how=\"left\")\n",
        "price = fd[\"adj_close\"].replace(0, np.nan)\n",
        "\n",
        "fd[\"book_to_price\"]     = fd[\"book_equity\"] / price\n",
        "fd[\"earnings_yield\"]    = fd[\"net_income\"]  / price\n",
        "fd[\"cf_yield\"]          = fd[\"ocf\"]         / price\n",
        "fd[\"shareholder_yield\"] = (fd[\"dividends\"].fillna(0) * -1 + fd[\"buybacks\"].fillna(0)) / price\n",
        "\n",
        "fd[\"gross_profitability\"] = fd[\"gross_profit\"] / fd[\"total_assets\"].replace(0, np.nan)\n",
        "fd[\"roe\"]                 = fd[\"net_income\"] / fd[\"book_equity\"].replace(0, np.nan)\n",
        "fd[\"accruals\"]            = (fd[\"net_income\"] - fd[\"ocf\"]) / fd[\"total_assets\"].replace(0, np.nan)\n",
        "fd[\"leverage\"]            = fd[\"total_debt\"] / fd[\"total_assets\"].replace(0, np.nan)\n",
        "\n",
        "funda_daily = fd[[\n",
        "    \"date\",\"ticker\",\"book_to_price\",\"earnings_yield\",\"cf_yield\",\"shareholder_yield\",\n",
        "    \"gross_profitability\",\"roe\",\"accruals\",\"leverage\"\n",
        "]]\n",
        "funda_daily.to_parquet(\"funda_daily.parquet\", index=False)\n",
        "print(\"Saved: funda_daily.parquet\")\n",
        "\n",
        "# Merge fundamentals into features\n",
        "features = features.merge(funda_daily, on=[\"date\",\"ticker\"], how=\"left\")\n",
        "\n",
        "# ============================================================\n",
        "# D) POST-MERGE HYGIENE\n",
        "# ============================================================\n",
        "\n",
        "# 1) Leakage control\n",
        "non_feature_cols = {\"date\",\"ticker\",\"open\",\"high\",\"low\",\"close\",\"adj_close\",\"volume\"}\n",
        "cols_to_shift = [c for c in features.columns if c not in non_feature_cols]\n",
        "features[cols_to_shift] = features.groupby(\"ticker\")[cols_to_shift].shift(1)\n",
        "\n",
        "# 2) Winsorize & cross-sectional z-score\n",
        "def winsorize_cs(s, lo=0.01, hi=0.99):\n",
        "    ql, qh = s.quantile(lo), s.quantile(hi)\n",
        "    return s.clip(ql, qh)\n",
        "\n",
        "# --- choose features for cross-sectional standardization (exclude context & raw SMAs) ---\n",
        "cs_cols = [\n",
        "    \"rv_20\",\"atr_14\",\"mom_20\",\"mom_6m\",\"mom_12m\",\"mom_12_1\",\"mom_6_1\",\n",
        "    \"sma_20_gt_50\",\"slope_20\",\"mom_20_vs_vol\",\n",
        "    # fundamentals\n",
        "    \"book_to_price\",\"earnings_yield\",\"cf_yield\",\"shareholder_yield\",\n",
        "    \"gross_profitability\",\"roe\",\"accruals\",\"leverage\"\n",
        "] + [f\"ret_lag_{l}\" for l in range(1,61)]\n",
        "\n",
        "# keep context raw (no CS z-score)\n",
        "context_keep_raw = [\"spy_rv_20\",\"vix_close\",\"breadth\"]\n",
        "\n",
        "present = [c for c in cs_cols if c in features.columns]\n",
        "\n",
        "print(f\"[Standardize] Cross-sectional z-score on {len(present)} features\")\n",
        "\n",
        "def cs_standardize_fast(df, cols, lo=0.01, hi=0.99):\n",
        "    out = df.copy()\n",
        "    out[cols] = out[cols].astype(\"float32\")\n",
        "\n",
        "    d = out[\"date\"]\n",
        "    for c in cols:\n",
        "        s = out[c]\n",
        "\n",
        "        ql = s.groupby(d).transform(lambda x: x.quantile(lo))\n",
        "        qh = s.groupby(d).transform(lambda x: x.quantile(hi))\n",
        "        s_clip = s.clip(ql, qh)\n",
        "\n",
        "        mu = s_clip.groupby(d).transform(\"mean\")\n",
        "        sd = s_clip.groupby(d).transform(\"std\")\n",
        "\n",
        "        # if std is 0 or NaN (date-constant or all-NaN), set denom=1 to avoid blowing up / NaNs\n",
        "        denom = sd.fillna(0.0).replace(0.0, 1.0)\n",
        "\n",
        "        out[c] = ((s_clip - mu) / (denom + 1e-9)).astype(\"float32\")\n",
        "\n",
        "    return out\n",
        "\n",
        "features = cs_standardize_fast(features, present)\n",
        "\n",
        "# 3) Fundamentals imputation + masks\n",
        "funda_cols = [\"book_to_price\",\"earnings_yield\",\"cf_yield\",\"shareholder_yield\",\n",
        "              \"gross_profitability\",\"roe\",\"accruals\",\"leverage\"]\n",
        "for c in funda_cols:\n",
        "    if c in features.columns:\n",
        "        features[f\"{c}_is_missing\"] = features[c].isna().astype(int)\n",
        "        features[c] = features.groupby(\"date\")[c].transform(lambda s: s.fillna(s.median()))\n",
        "\n",
        "# Save final\n",
        "features.to_parquet(\"features.parquet\", index=False)\n",
        "print(\"Saved: features.parquet (lagged, winsorized, cross-sectional z-scored)\")\n",
        "print(\"Artifacts: funda_quarterly.parquet, funda_daily.parquet, cache/funda_q_*.parquet\")\n",
        "\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 998
        },
        "id": "5qa3M9pAzvtv",
        "outputId": "a1957f4f-21c1-4919-ea47-1b9bf5516a76"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your FMP API key (kept in-memory for this session): ··········\n",
            "[Features] 25/514 processed… (AMAT)\n",
            "[Features] 50/514 processed… (BA)\n",
            "[Features] 75/514 processed… (CARR)\n",
            "[Features] 100/514 processed… (CNP)\n",
            "[Features] 125/514 processed… (DAL)\n",
            "[Features] 150/514 processed… (EA)\n",
            "[Features] 175/514 processed… (EXE)\n",
            "[Features] 200/514 processed… (GEHC)\n",
            "[Features] 225/514 processed… (HON)\n",
            "[Features] 250/514 processed… (IT)\n",
            "[Features] 275/514 processed… (LDOS)\n",
            "[Features] 300/514 processed… (MCO)\n",
            "[Features] 325/514 processed… (MTCH)\n",
            "[Features] 350/514 processed… (OKE)\n",
            "[Features] 375/514 processed… (PNR)\n",
            "[Features] 400/514 processed… (RTX)\n",
            "[Features] 425/514 processed… (SYF)\n",
            "[Features] 450/514 processed… (TT)\n",
            "[Features] 475/514 processed… (VTR)\n",
            "[Features] 500/514 processed… (XLI)\n",
            "AAPL → AAPL rows: 78\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "         date ticker   book_equity  net_income           ocf  gross_profit  \\\n",
              "42 2006-04-01   AAPL  8.682000e+09         NaN -1.250000e+08  1.297000e+09   \n",
              "43 2006-07-01   AAPL  9.330000e+09         NaN  1.007000e+09  1.325000e+09   \n",
              "\n",
              "    total_assets  total_debt  dividends   buybacks  \n",
              "42  1.391100e+10         0.0        0.0        0.0  \n",
              "43  1.511400e+10         0.0        0.0 -1000000.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-126ddfb9-e89f-41af-9965-0c15a3d0587f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>ticker</th>\n",
              "      <th>book_equity</th>\n",
              "      <th>net_income</th>\n",
              "      <th>ocf</th>\n",
              "      <th>gross_profit</th>\n",
              "      <th>total_assets</th>\n",
              "      <th>total_debt</th>\n",
              "      <th>dividends</th>\n",
              "      <th>buybacks</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>2006-04-01</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>8.682000e+09</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-1.250000e+08</td>\n",
              "      <td>1.297000e+09</td>\n",
              "      <td>1.391100e+10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>2006-07-01</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>9.330000e+09</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.007000e+09</td>\n",
              "      <td>1.325000e+09</td>\n",
              "      <td>1.511400e+10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1000000.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-126ddfb9-e89f-41af-9965-0c15a3d0587f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-126ddfb9-e89f-41af-9965-0c15a3d0587f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-126ddfb9-e89f-41af-9965-0c15a3d0587f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-f3cad253-15aa-4a08-900a-148334d5c7eb\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f3cad253-15aa-4a08-900a-148334d5c7eb')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-f3cad253-15aa-4a08-900a-148334d5c7eb button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"gc\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"date\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2006-04-01 00:00:00\",\n        \"max\": \"2006-07-01 00:00:00\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"2006-07-01 00:00:00\",\n          \"2006-04-01 00:00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ticker\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"AAPL\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"book_equity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 458205194.2088828,\n        \"min\": 8682000000.0,\n        \"max\": 9330000000.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          9330000000.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"net_income\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ocf\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 800444876.3031718,\n        \"min\": -125000000.0,\n        \"max\": 1007000000.0,\n        \"num_unique_values\": 2,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"gross_profit\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 19798989.87322333,\n        \"min\": 1297000000.0,\n        \"max\": 1325000000.0,\n        \"num_unique_values\": 2,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"total_assets\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 850649457.7674167,\n        \"min\": 13911000000.0,\n        \"max\": 15114000000.0,\n        \"num_unique_values\": 2,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"total_debt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dividends\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"buybacks\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 707106.7811865475,\n        \"min\": -1000000.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 2,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSFT → MSFT rows: 78\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "         date ticker   book_equity  net_income           ocf  gross_profit  \\\n",
              "42 2006-03-31   MSFT  4.203800e+10         NaN  4.563000e+09  8.872000e+09   \n",
              "43 2006-06-30   MSFT  4.010400e+10         NaN  3.281000e+09  9.674000e+09   \n",
              "\n",
              "    total_assets  total_debt    dividends      buybacks  \n",
              "42  6.685400e+10         0.0 -925000000.0 -4.675000e+09  \n",
              "43  6.959700e+10         0.0 -917000000.0 -3.981000e+09  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3d9d8cd6-fe05-4c9b-b807-004303fca5b6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>ticker</th>\n",
              "      <th>book_equity</th>\n",
              "      <th>net_income</th>\n",
              "      <th>ocf</th>\n",
              "      <th>gross_profit</th>\n",
              "      <th>total_assets</th>\n",
              "      <th>total_debt</th>\n",
              "      <th>dividends</th>\n",
              "      <th>buybacks</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>2006-03-31</td>\n",
              "      <td>MSFT</td>\n",
              "      <td>4.203800e+10</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.563000e+09</td>\n",
              "      <td>8.872000e+09</td>\n",
              "      <td>6.685400e+10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-925000000.0</td>\n",
              "      <td>-4.675000e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>2006-06-30</td>\n",
              "      <td>MSFT</td>\n",
              "      <td>4.010400e+10</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.281000e+09</td>\n",
              "      <td>9.674000e+09</td>\n",
              "      <td>6.959700e+10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-917000000.0</td>\n",
              "      <td>-3.981000e+09</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3d9d8cd6-fe05-4c9b-b807-004303fca5b6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3d9d8cd6-fe05-4c9b-b807-004303fca5b6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3d9d8cd6-fe05-4c9b-b807-004303fca5b6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-c2ddf562-59a6-4fdd-862a-bd06f6cfae2e\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c2ddf562-59a6-4fdd-862a-bd06f6cfae2e')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-c2ddf562-59a6-4fdd-862a-bd06f6cfae2e button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"gc\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"date\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2006-03-31 00:00:00\",\n        \"max\": \"2006-06-30 00:00:00\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"2006-06-30 00:00:00\",\n          \"2006-03-31 00:00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ticker\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"MSFT\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"book_equity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1367544514.8147829,\n        \"min\": 40104000000.0,\n        \"max\": 42038000000.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          40104000000.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"net_income\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ocf\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 906510893.481154,\n        \"min\": 3281000000.0,\n        \"max\": 4563000000.0,\n        \"num_unique_values\": 2,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"gross_profit\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 567099638.5116111,\n        \"min\": 8872000000.0,\n        \"max\": 9674000000.0,\n        \"num_unique_values\": 2,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"total_assets\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1939593900.7947,\n        \"min\": 66854000000.0,\n        \"max\": 69597000000.0,\n        \"num_unique_values\": 2,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"total_debt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dividends\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5656854.24949238,\n        \"min\": -925000000.0,\n        \"max\": -917000000.0,\n        \"num_unique_values\": 2,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"buybacks\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 490732106.14346397,\n        \"min\": -4675000000.0,\n        \"max\": -3981000000.0,\n        \"num_unique_values\": 2,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BRK-B → BRK.B rows: 78\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "         date ticker   book_equity  net_income           ocf  gross_profit  \\\n",
              "42 2006-03-31  BRK-B  9.534900e+10         NaN  2.359000e+09  6.162000e+09   \n",
              "43 2006-06-30  BRK-B  9.761300e+10         NaN  1.092000e+09  8.197000e+09   \n",
              "\n",
              "    total_assets    total_debt  dividends  buybacks  \n",
              "42  2.302060e+11  3.047900e+10        0.0       0.0  \n",
              "43  2.323310e+11  3.055700e+10        0.0       0.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c27cc58c-d83c-4558-971d-23f83eb84b6b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>ticker</th>\n",
              "      <th>book_equity</th>\n",
              "      <th>net_income</th>\n",
              "      <th>ocf</th>\n",
              "      <th>gross_profit</th>\n",
              "      <th>total_assets</th>\n",
              "      <th>total_debt</th>\n",
              "      <th>dividends</th>\n",
              "      <th>buybacks</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>2006-03-31</td>\n",
              "      <td>BRK-B</td>\n",
              "      <td>9.534900e+10</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.359000e+09</td>\n",
              "      <td>6.162000e+09</td>\n",
              "      <td>2.302060e+11</td>\n",
              "      <td>3.047900e+10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>2006-06-30</td>\n",
              "      <td>BRK-B</td>\n",
              "      <td>9.761300e+10</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.092000e+09</td>\n",
              "      <td>8.197000e+09</td>\n",
              "      <td>2.323310e+11</td>\n",
              "      <td>3.055700e+10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c27cc58c-d83c-4558-971d-23f83eb84b6b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c27cc58c-d83c-4558-971d-23f83eb84b6b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c27cc58c-d83c-4558-971d-23f83eb84b6b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-bb5741bf-38ae-4f70-90ca-caf157ad3c99\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-bb5741bf-38ae-4f70-90ca-caf157ad3c99')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-bb5741bf-38ae-4f70-90ca-caf157ad3c99 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"gc\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"date\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2006-03-31 00:00:00\",\n        \"max\": \"2006-06-30 00:00:00\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"2006-06-30 00:00:00\",\n          \"2006-03-31 00:00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ticker\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"BRK-B\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"book_equity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1600889752.6063435,\n        \"min\": 95349000000.0,\n        \"max\": 97613000000.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          97613000000.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"net_income\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ocf\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 895904291.7633557,\n        \"min\": 1092000000.0,\n        \"max\": 2359000000.0,\n        \"num_unique_values\": 2,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"gross_profit\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1438962299.7146242,\n        \"min\": 6162000000.0,\n        \"max\": 8197000000.0,\n        \"num_unique_values\": 2,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"total_assets\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1502601910.0214136,\n        \"min\": 230206000000.0,\n        \"max\": 232331000000.0,\n        \"num_unique_values\": 2,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"total_debt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 55154328.932550706,\n        \"min\": 30479000000.0,\n        \"max\": 30557000000.0,\n        \"num_unique_values\": 2,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dividends\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"buybacks\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERR BF-B FMP fundamentals empty for BF-B (queried as BF.B)\n",
            "[FMP] Processing chunk 0:20 (size=20)\n",
            "Saved: funda_quarterly.parquet  (tickers with funda total: 99, rows: 7373)\n",
            "Saved: funda_daily.parquet\n",
            "[Standardize] Cross-sectional z-score on 78 features\n",
            "Saved: features.parquet (lagged, winsorized, cross-sectional z-scored)\n",
            "Artifacts: funda_quarterly.parquet, funda_daily.parquet, cache/funda_q_*.parquet\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 1.3 DATA HYGIENE / QC (non-destructive)\n",
        "# ------------------------------------------------------------\n",
        "# - Summarize coverage & missingness (post 1.2)\n",
        "# - Optional pruning: drop early warmup dates & low-coverage dates\n",
        "# - Write meta.yaml and QC CSVs\n",
        "# ============================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yaml\n",
        "\n",
        "FEATURES_PATH = \"features.parquet\"\n",
        "UNIVERSE_PATH = \"universe.csv\"\n",
        "\n",
        "features = pd.read_parquet(FEATURES_PATH)\n",
        "universe_df = pd.read_csv(UNIVERSE_PATH)\n",
        "\n",
        "# ---------- QC: basics ----------\n",
        "min_date = pd.to_datetime(features[\"date\"]).min()\n",
        "max_date = pd.to_datetime(features[\"date\"]).max()\n",
        "n_rows = len(features)\n",
        "n_tickers = features[\"ticker\"].nunique()\n",
        "\n",
        "# Columns we standardized in 1.2 (will exist if 1.2 ran)\n",
        "feature_cols = [c for c in features.columns\n",
        "                if c not in {\"date\",\"ticker\",\"open\",\"high\",\"low\",\"close\",\"adj_close\",\"volume\"}]\n",
        "\n",
        "# Per-column missingness (after 1.2; should be low except earliest windows)\n",
        "missing_pct = (1.0 - features[feature_cols].notna().mean()).sort_values(ascending=False)\n",
        "missing_pct.to_csv(\"qc_missing_by_feature.csv\", header=[\"missing_pct\"])\n",
        "\n",
        "# Coverage by date (# of tickers with at least 1 valid feature on that date)\n",
        "valid_any = features[feature_cols].notna().sum(axis=1) > 0\n",
        "coverage_by_date = (features.assign(valid_any=valid_any)\n",
        "                             .groupby(\"date\")[\"ticker\"]\n",
        "                             .nunique()\n",
        "                             .rename(\"n_tickers\"))\n",
        "coverage_by_date.to_csv(\"qc_coverage_by_date.csv\")\n",
        "\n",
        "# ---------- Optional: pruning rules (non-destructive by default) ----------\n",
        "# 1) Warmup: many features need long windows (max ≈ 252 + 21). Keep dates after first 273 trading days.\n",
        "#    We'll infer a warmup cutoff from SPY availability to be robust.\n",
        "spy_dates = features.loc[features[\"ticker\"]==\"SPY\", \"date\"].sort_values().unique()\n",
        "if len(spy_dates) > 300:\n",
        "    warmup_cutoff = pd.to_datetime(spy_dates[min(273, len(spy_dates)-1)])\n",
        "else:\n",
        "    warmup_cutoff = min_date  # fallback\n",
        "\n",
        "# 2) Low coverage: drop dates with very few names (e.g., <300) — tweak if you want.\n",
        "COVERAGE_MIN = 300\n",
        "low_cov_dates = coverage_by_date[coverage_by_date < COVERAGE_MIN].index\n",
        "\n",
        "# We don’t mutate features here; write a recommended mask so training can filter.\n",
        "date_mask_keep = (~pd.Series(features[\"date\"]).isin(low_cov_dates)) & (features[\"date\"] >= warmup_cutoff)\n",
        "keep_rate = date_mask_keep.mean()\n",
        "pd.DataFrame({\n",
        "    \"warmup_cutoff\":[warmup_cutoff],\n",
        "    \"coverage_min\":[COVERAGE_MIN],\n",
        "    \"keep_rate\":[float(keep_rate)]\n",
        "}).to_csv(\"qc_recommendations.csv\", index=False)\n",
        "\n",
        "# ---------- Meta ----------\n",
        "meta = {\n",
        "    \"universe\": {\n",
        "        \"description\": \"S&P 500 (current constituents; survivorship bias acknowledged).\",\n",
        "        \"count\": int(len(universe_df)),\n",
        "        \"hedges\": [\"SPY\",\"XLY\",\"XLF\",\"XLV\",\"XLK\",\"XLI\",\"XLE\",\"XLP\",\"XLB\",\"XLU\",\"XLRE\"],\n",
        "        \"context_symbols\": [\"^VIX\"],\n",
        "        \"lookback\": {\"start\": str(min_date.date()), \"end\": str(max_date.date())}\n",
        "    },\n",
        "    \"pricing\": {\n",
        "        \"source\": \"Yahoo Finance via yfinance\",\n",
        "        \"adjusted_prices_used\": True,\n",
        "        \"file\": \"raw_prices.parquet\"\n",
        "    },\n",
        "    \"features\": {\n",
        "        \"file\": \"features.parquet\",\n",
        "        \"rows\": int(n_rows),\n",
        "        \"tickers\": int(n_tickers),\n",
        "        \"leakage_control\": \"All predictive features shifted by 1 day.\",\n",
        "        \"cross_sectional_processing\": \"Winsorized [1%,99%] & z-scored by date (see 1.2).\",\n",
        "        \"imputation\": \"Fundamentals imputed (cross-sectional median) in 1.2; *_is_missing masks present.\"\n",
        "    },\n",
        "    \"qc\": {\n",
        "        \"missing_by_feature_csv\": \"qc_missing_by_feature.csv\",\n",
        "        \"coverage_by_date_csv\": \"qc_coverage_by_date.csv\",\n",
        "        \"recommendations_csv\": \"qc_recommendations.csv\",\n",
        "        \"warmup_cutoff\": str(warmup_cutoff.date()),\n",
        "        \"coverage_min\": COVERAGE_MIN,\n",
        "        \"recommendation\": \"Filter training rows to dates >= warmup_cutoff and dates with coverage >= coverage_min.\"\n",
        "    },\n",
        "    \"deliverables\": [\"universe.csv\", \"raw_prices.parquet\", \"features.parquet\",\n",
        "                     \"funda_quarterly.parquet\", \"funda_daily.parquet\", \"meta.yaml\",\n",
        "                     \"qc_missing_by_feature.csv\", \"qc_coverage_by_date.csv\", \"qc_recommendations.csv\"]\n",
        "}\n",
        "\n",
        "with open(\"meta.yaml\", \"w\") as f:\n",
        "    yaml.safe_dump(meta, f, sort_keys=False)\n",
        "\n",
        "print(\"Saved: meta.yaml + QC CSVs\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXCXZLXEzw6j",
        "outputId": "6d8bb7e1-d2f1-4b6b-d903-57d2b46c8e2e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: meta.yaml + QC CSVs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 1.4 DATA QC & ASSERTIONS (non-destructive; optional filtered view)\n",
        "# Produces: qc_summary.json, qc_constant_cols.csv, qc_missing_by_feature.csv (again),\n",
        "#           qc_skew_kurtosis.csv, qc_outlier_rate.csv, qc_drift.csv,\n",
        "#           features_filtered.parquet (optional, if you turn on APPLY_FILTERS)\n",
        "# ============================================================\n",
        "\n",
        "import json, pandas as pd, numpy as np\n",
        "from scipy.stats import skew, kurtosis\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", message=\"Precision loss occurred in moment calculation\")\n",
        "warnings.filterwarnings(\"ignore\", message=\"Degrees of freedom <= 0 for slice\")\n",
        "\n",
        "FEATURES_PATH = \"features.parquet\"\n",
        "\n",
        "APPLY_FILTERS = True          # set False if you only want reports\n",
        "COVERAGE_MIN = 300            # min tickers per date\n",
        "Z_OUTLIER = 5.0               # |z| threshold post-standardization\n",
        "EARLY_YEARS = 5               # windows for drift check\n",
        "RECENT_YEARS = 5\n",
        "\n",
        "df = pd.read_parquet(FEATURES_PATH)\n",
        "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
        "feature_cols = [c for c in df.columns if c not in {\"date\",\"ticker\",\"open\",\"high\",\"low\",\"close\",\"adj_close\",\"volume\"}]\n",
        "\n",
        "# Basic shape / duplicates\n",
        "dup_count = df.duplicated([\"date\",\"ticker\"]).sum()\n",
        "idx_dupes = int(dup_count)\n",
        "\n",
        "# Per-ticker monotonic date check\n",
        "monotonic_bad = []\n",
        "for t, g in df.groupby(\"ticker\"):\n",
        "    if not g[\"date\"].sort_values().is_monotonic_increasing:\n",
        "        monotonic_bad.append(t)\n",
        "\n",
        "# Constant/empty columns\n",
        "MIN_N = 200  # only compute moments if we’ve got enough points\n",
        "sk_stats = []\n",
        "\n",
        "const_cols, empty_cols = [], []\n",
        "for c in feature_cols:\n",
        "    nn = df[c].notna().sum()\n",
        "    if nn == 0:\n",
        "        empty_cols.append(c)\n",
        "        continue\n",
        "    # treat “constant” as very low variance or single unique value\n",
        "    if df[c].nunique(dropna=True) == 1 or np.nanstd(df[c].to_numpy(dtype=float)) < 1e-12:\n",
        "        const_cols.append(c)\n",
        "\n",
        "pd.Series(const_cols, name=\"constant_cols\").to_csv(\"qc_constant_cols.csv\", index=False)\n",
        "pd.Series(empty_cols,  name=\"empty_cols\").to_csv(\"qc_empty_cols.csv\", index=False)\n",
        "\n",
        "# Missingness\n",
        "missing_pct = (1.0 - df[feature_cols].notna().mean()).sort_values(ascending=False)\n",
        "missing_pct.to_csv(\"qc_missing_by_feature.csv\", header=[\"missing_pct\"])\n",
        "\n",
        "# Coverage by date and warmup/low-coverage mask (reuse warmup logic from 1.3)\n",
        "spy_dates = df.loc[df[\"ticker\"]==\"SPY\", \"date\"].sort_values().unique()\n",
        "warmup_cutoff = pd.to_datetime(spy_dates[min(273, len(spy_dates)-1)]) if len(spy_dates) > 300 else df[\"date\"].min()\n",
        "coverage = df.groupby(\"date\")[\"ticker\"].nunique()\n",
        "low_cov_dates = coverage[coverage < COVERAGE_MIN].index\n",
        "keep_mask = (df[\"date\"] >= warmup_cutoff) & (~df[\"date\"].isin(low_cov_dates))\n",
        "keep_rate = float(keep_mask.mean())\n",
        "\n",
        "# Outlier rate (features are z-scored per date already)\n",
        "outlier_rate = {}\n",
        "for c in feature_cols:\n",
        "    s = df[c]\n",
        "    outlier_rate[c] = float((s.abs() > Z_OUTLIER).mean())\n",
        "pd.Series(outlier_rate, name=\"outlier_rate\").sort_values(ascending=False).to_csv(\"qc_outlier_rate.csv\")\n",
        "\n",
        "# Skew/Kurtosis (global, ignoring NaNs)\n",
        "sk_rows = []\n",
        "for c in feature_cols:\n",
        "    x = df[c].to_numpy(dtype=float)\n",
        "    x = x[np.isfinite(x)]\n",
        "    if len(x) < MIN_N or np.nanstd(x) < 1e-8:\n",
        "        # optional: quantile-based skew as fallback\n",
        "        try:\n",
        "            q1,q2,q3 = np.nanpercentile(x, [25,50,75])\n",
        "            bowley = ((q3 + q1) - 2*q2) / ((q3 - q1) + 1e-9)\n",
        "        except Exception:\n",
        "            bowley = np.nan\n",
        "        sk_rows.append([c, np.nan, np.nan, bowley, np.nan, np.nan])\n",
        "        continue\n",
        "    sk = float(skew(x, bias=False))\n",
        "    ku = float(kurtosis(x, fisher=True, bias=False))\n",
        "    p99 = float(np.nanpercentile(x, 99))\n",
        "    med = float(np.nanmedian(x))\n",
        "    dom = abs(p99) / (abs(med) + 1e-9)\n",
        "    sk_rows.append([c, sk, ku, np.nan, dom, p99])\n",
        "\n",
        "pd.DataFrame(sk_rows, columns=[\"feature\",\"skew\",\"kurtosis_fisher\",\"bowley_skew\",\"p99_to_median_abs\",\"p99\"])\\\n",
        "  .sort_values(\"p99_to_median_abs\", ascending=False)\\\n",
        "  .to_csv(\"qc_skew_kurtosis.csv\", index=False)\n",
        "\n",
        "# Drift: early vs recent windows\n",
        "dstart, dend = df[\"date\"].min(), df[\"date\"].max()\n",
        "span_years = (dend - dstart).days / 365.25\n",
        "if span_years < (EARLY_YEARS + RECENT_YEARS):\n",
        "    # fallback: split the dataset in half\n",
        "    mid = dstart + (dend - dstart) / 2\n",
        "    early = df[(df[\"date\"] >= dstart) & (df[\"date\"] <= mid)]\n",
        "    late  = df[(df[\"date\"] >  mid) & (df[\"date\"] <= dend)]\n",
        "else:\n",
        "    early_end    = pd.Timestamp(dstart) + pd.DateOffset(years=EARLY_YEARS)\n",
        "    recent_start = pd.Timestamp(dend)   - pd.DateOffset(years=RECENT_YEARS)\n",
        "    early = df[(df[\"date\"] >= dstart) & (df[\"date\"] <= early_end)]\n",
        "    late  = df[(df[\"date\"] >= recent_start) & (df[\"date\"] <= dend)]\n",
        "\n",
        "drift_rows = []\n",
        "for c in feature_cols:\n",
        "    e = early[c].astype(\"float64\"); l = late[c].astype(\"float64\")\n",
        "    e = e[np.isfinite(e)]; l = l[np.isfinite(l)]\n",
        "    if len(e) < MIN_N or len(l) < MIN_N:\n",
        "        drift_rows.append([c, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan])\n",
        "        continue\n",
        "    e_mean, e_std = float(np.nanmean(e)), float(np.nanstd(e))\n",
        "    l_mean, l_std = float(np.nanmean(l)), float(np.nanstd(l))\n",
        "    drift_rows.append([c, e_mean, l_mean, l_mean - e_mean, e_std, l_std, (l_std+1e-9)/(e_std+1e-9)])\n",
        "\n",
        "pd.DataFrame(\n",
        "    drift_rows,\n",
        "    columns=[\"feature\",\"early_mean\",\"late_mean\",\"mean_diff\",\"early_std\",\"late_std\",\"std_ratio_late_over_early\"]\n",
        ").to_csv(\"qc_drift.csv\", index=False)\n",
        "\n",
        "def bowley_skew(x):\n",
        "    q1, q2, q3 = np.nanpercentile(x, [25,50,75])\n",
        "    denom = (q3 - q1) + 1e-9\n",
        "    return float(((q3 + q1) - 2*q2) / denom)\n",
        "# you can compute this alongside or instead of moment skew for each feature\n",
        "\n",
        "# Optionally write filtered view for modeling\n",
        "if APPLY_FILTERS:\n",
        "    # Also drop truly empty/constant cols from the filtered file only\n",
        "    drop_cols = list(set(empty_cols) | set(const_cols))\n",
        "    cols_keep = [c for c in df.columns if c not in drop_cols]\n",
        "    df_filt = df.loc[keep_mask, cols_keep].copy()\n",
        "    df_filt.to_parquet(\"features_filtered.parquet\", index=False)\n",
        "\n",
        "# Summary JSON (for quick eyeball)\n",
        "summary = {\n",
        "    \"rows\": int(len(df)),\n",
        "    \"tickers\": int(df[\"ticker\"].nunique()),\n",
        "    \"dates\": int(df[\"date\"].nunique()),\n",
        "    \"date_min\": str(df[\"date\"].min().date()),\n",
        "    \"date_max\": str(df[\"date\"].max().date()),\n",
        "    \"duplicates_idx\": idx_dupes,\n",
        "    \"monotonic_date_issues\": len(monotonic_bad),\n",
        "    \"constant_cols\": len(const_cols),\n",
        "    \"empty_cols\": len(empty_cols),\n",
        "    \"warmup_cutoff\": str(warmup_cutoff.date()),\n",
        "    \"coverage_min\": COVERAGE_MIN,\n",
        "    \"keep_rate_after_filters\": keep_rate,\n",
        "    \"median_missing_pct\": float(missing_pct.median()),\n",
        "    \"max_missing_pct\": float(missing_pct.max()),\n",
        "    \"mean_outlier_rate_|z|>5\": float(pd.Series(outlier_rate).mean()),\n",
        "    \"filtered_file_written\": APPLY_FILTERS\n",
        "}\n",
        "\n",
        "# 🚦 Hard QC checks — stop if these fail\n",
        "assert summary[\"duplicates_idx\"] == 0, \"Duplicate (date,ticker) rows found.\"\n",
        "assert summary[\"keep_rate_after_filters\"] >= 0.85, \"Too many rows dropped by filters.\"\n",
        "assert summary[\"constant_cols\"] <= 10, \"Suspicious number of constant columns.\"\n",
        "\n",
        "with open(\"qc_summary.json\",\"w\") as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "print(\"QC done → qc_summary.json, qc_* CSVs\",\n",
        "      \"and features_filtered.parquet\" if APPLY_FILTERS else \"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8eWz-gvOXhR",
        "outputId": "d5201113-77c4-4b8e-fe41-9dbc7cd7a551"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/numpy/lib/_nanfunctions_impl.py:1633: RuntimeWarning: Mean of empty slice\n",
            "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QC done → qc_summary.json, qc_* CSVs and features_filtered.parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json, pandas as pd\n",
        "\n",
        "with open(\"qc_summary.json\") as f:\n",
        "    s = json.load(f)\n",
        "\n",
        "print(\"=== QC SUMMARY ===\")\n",
        "for k in [\n",
        "    \"rows\",\"tickers\",\"dates\",\"date_min\",\"date_max\",\n",
        "    \"duplicates_idx\",\"monotonic_date_issues\",\n",
        "    \"constant_cols\",\"empty_cols\",\n",
        "    \"warmup_cutoff\",\"coverage_min\",\"keep_rate_after_filters\",\n",
        "    \"median_missing_pct\",\"max_missing_pct\",\"mean_outlier_rate_|z|>5\",\n",
        "    \"filtered_file_written\"\n",
        "]:\n",
        "    print(f\"{k}: {s.get(k)}\")\n",
        "\n",
        "print(\"\\n=== Top 10 most-missing features ===\")\n",
        "print(pd.read_csv(\"qc_missing_by_feature.csv\").head(10))\n",
        "\n",
        "print(\"\\n=== Top 10 highest outlier rates (|z|>5) ===\")\n",
        "print(pd.read_csv(\"qc_outlier_rate.csv\").head(10))\n",
        "\n",
        "print(\"\\n=== Constant / Empty columns ===\")\n",
        "try: print(pd.read_csv(\"qc_constant_cols.csv\").head())\n",
        "except: print(\"none\")\n",
        "try: print(pd.read_csv(\"qc_empty_cols.csv\").head())\n",
        "except: print(\"none\")\n",
        "\n",
        "print(\"\\n=== Drift (largest mean change early→late) ===\")\n",
        "drift = pd.read_csv(\"qc_drift.csv\")\n",
        "drift[\"abs_mean_diff\"] = drift[\"mean_diff\"].abs()\n",
        "print(drift.sort_values(\"abs_mean_diff\", ascending=False).head(10))\n",
        "\n",
        "print(\"\\n=== Filtered file shape ===\")\n",
        "ff = pd.read_parquet(\"features_filtered.parquet\")\n",
        "print(ff.shape, \"rows x cols; dates:\", ff['date'].min(), \"→\", ff['date'].max(), \"; tickers:\", ff['ticker'].nunique())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OApdlkYwWSsC",
        "outputId": "d053f93b-79de-4877-f11c-13a391bc6f63"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== QC SUMMARY ===\n",
            "rows: 2331764\n",
            "tickers: 513\n",
            "dates: 4931\n",
            "date_min: 2006-01-03\n",
            "date_max: 2025-08-08\n",
            "duplicates_idx: 0\n",
            "monotonic_date_issues: 0\n",
            "constant_cols: 3\n",
            "empty_cols: 3\n",
            "warmup_cutoff: 2007-02-05\n",
            "coverage_min: 300\n",
            "keep_rate_after_filters: 0.9515559893711371\n",
            "median_missing_pct: 0.005390125244235655\n",
            "max_missing_pct: 1.0\n",
            "mean_outlier_rate_|z|>5: 0.03214174650234944\n",
            "filtered_file_written: True\n",
            "\n",
            "=== Top 10 most-missing features ===\n",
            "       Unnamed: 0  missing_pct\n",
            "0  earnings_yield     1.000000\n",
            "1             roe     1.000000\n",
            "2        accruals     1.000000\n",
            "3        mom_12_1     0.055661\n",
            "4         mom_12m     0.055661\n",
            "5         mom_6_1     0.027941\n",
            "6          mom_6m     0.027941\n",
            "7      ret_lag_60     0.013640\n",
            "8      ret_lag_59     0.013420\n",
            "9      ret_lag_58     0.013200\n",
            "\n",
            "=== Top 10 highest outlier rates (|z|>5) ===\n",
            "          Unnamed: 0  outlier_rate\n",
            "0          vix_close      0.999780\n",
            "1             sma_20      0.973347\n",
            "2             sma_50      0.967120\n",
            "3             atr_14      0.005802\n",
            "4      book_to_price      0.003091\n",
            "5           slope_20      0.002136\n",
            "6           cf_yield      0.001913\n",
            "7  shareholder_yield      0.001065\n",
            "8       sma_20_gt_50      0.000211\n",
            "9              rv_20      0.000153\n",
            "\n",
            "=== Constant / Empty columns ===\n",
            "               constant_cols\n",
            "0  earnings_yield_is_missing\n",
            "1             roe_is_missing\n",
            "2        accruals_is_missing\n",
            "       empty_cols\n",
            "0  earnings_yield\n",
            "1             roe\n",
            "2        accruals\n",
            "\n",
            "=== Drift (largest mean change early→late) ===\n",
            "                     feature  early_mean   late_mean   mean_diff  early_std  \\\n",
            "68                    sma_20   27.258920  170.296715  143.037795  44.341893   \n",
            "69                    sma_50   27.224041  169.016571  141.792529  44.140373   \n",
            "74                 vix_close   23.547258   20.028145   -3.519113  11.924482   \n",
            "83                  leverage   -0.122982   -0.024472    0.098511   0.420619   \n",
            "75                   breadth    0.428993    0.526163    0.097170   0.224669   \n",
            "78                  cf_yield   -0.236037   -0.156609    0.079428   0.434831   \n",
            "73                 spy_rv_20    0.203216    0.158993   -0.044223   0.150976   \n",
            "76             book_to_price   -0.223970   -0.254669   -0.030699   0.422183   \n",
            "79         shareholder_yield    0.004533   -0.007954   -0.012487   0.458790   \n",
            "84  book_to_price_is_missing    0.833784    0.823038   -0.010746   0.372275   \n",
            "\n",
            "      late_std  std_ratio_late_over_early  abs_mean_diff  \n",
            "68  348.427112                   7.857741     143.037795  \n",
            "69  344.943834                   7.814701     141.792529  \n",
            "74    5.563185                   0.466535       3.519113  \n",
            "83    0.419959                   0.998430       0.098511  \n",
            "75    0.242178                   1.077931       0.097170  \n",
            "78    0.438908                   1.009375       0.079428  \n",
            "73    0.075436                   0.499657       0.044223  \n",
            "76    0.435356                   1.031202       0.030699  \n",
            "79    0.458770                   0.999957       0.012487  \n",
            "84    0.381637                   1.025148       0.010746  \n",
            "\n",
            "=== Filtered file shape ===\n",
            "(2218804, 94) rows x cols; dates: 2007-02-05 00:00:00 → 2025-08-08 00:00:00 ; tickers: 513\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>📦 Summary — Section 1 (Data & Universe)</summary>\n",
        "\n",
        "In this section, we **built the full, modeling-ready dataset** by merging historical prices, technical indicators, market context, and fundamentals into a single leakage-controlled feature matrix.  \n",
        "Key steps included:\n",
        "\n",
        "- **Data acquisition** — pulled long-term daily OHLCV for the equity universe, hedges, and context symbols, plus quarterly fundamentals from FMP.\n",
        "- **Feature engineering** — created lagged returns/volatility, momentum metrics, trend filters, ATR, volatility-adjusted momentum, and value/quality factor composites. Fundamentals were forward-filled to daily frequency.\n",
        "- **Leakage control & scaling** — shifted predictive features by one day, winsorized extreme values, and cross-sectionally z-scored each feature per date.\n",
        "- **Missing data handling** — conservative imputation for fundamentals and binary masks to record missingness.\n",
        "- **Quality control** — removed low-coverage dates, early warmup period, constant/empty columns, and duplicate rows; generated QC reports and metadata.\n",
        "\n",
        "**Outcome:** A clean, consistent, and statistically robust `features_filtered.parquet` file — ready for direct use in **Section 2 (Regime Modeling)** without recomputing or re-fetching any raw data.\n",
        "\n",
        "</details>\n"
      ],
      "metadata": {
        "id": "vOLQATza111V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary> Variables to reuse — Section 1 (Data & Universe) </summary>\n",
        "**Status:** Done. Artifacts are written; QC checks passed; ready to start **Section 2 (Regime Modeling)** using the saved files and globals below.\n",
        "\n",
        "---\n",
        "\n",
        "## Canonical Artifacts (reuse, don’t recompute)\n",
        "- `universe.csv` – S&P 500 tickers (Yahoo-style), excludes hedges/context.\n",
        "- `raw_prices.parquet` – OHLCV + `adj_close` for equities + hedges + `^VIX` (long format).\n",
        "- `features.parquet` – lagged, winsorized, cross-sectionally z-scored features (+ *_is_missing masks).\n",
        "- `features_filtered.parquet` – modeling-ready view (warmup & low-coverage dates removed; empty/constant cols dropped).\n",
        "- `funda_quarterly.parquet`, `funda_daily.parquet` – fundamentals at quarterly/daily granularity.\n",
        "- `meta.yaml` – machine-readable metadata (sources, lookback, QC guidance).\n",
        "- QC reports: `qc_summary.json`, `qc_missing_by_feature.csv`, `qc_coverage_by_date.csv`, `qc_constant_cols.csv`, `qc_empty_cols.csv`, `qc_outlier_rate.csv`, `qc_skew_kurtosis.csv`, `qc_drift.csv`, `qc_recommendations.csv`.\n",
        "\n",
        "---\n",
        "\n",
        "## Reusable Globals (organized)\n",
        "> These exist (or are trivially reloadable) after Section 1. Prefer these over re-deriving.\n",
        "\n",
        "### Dates / Ranges\n",
        "- `START_DATE = \"2006-01-01\"`  \n",
        "- `END_DATE = datetime.today().strftime(\"%Y-%m-%d\")`\n",
        "\n",
        "### Universe & Symbols\n",
        "- `sp500_url` – Wikipedia source for constituents.\n",
        "- `tickers_raw` → raw symbols from Wikipedia.\n",
        "- `tickers` → Yahoo-normalized tickers (periods → dashes).\n",
        "- `hedges` → `[\"SPY\",\"XLY\",\"XLF\",\"XLV\",\"XLK\",\"XLI\",\"XLE\",\"XLP\",\"XLB\",\"XLU\",\"XLRE\"]`\n",
        "- `context_symbols` → `[\"^VIX\"]`  *(later used as `{\"^VIX\"}` set in 1.2)*\n",
        "- `universe` → sorted unique S&P tickers.\n",
        "- `universe_all` → `universe + hedges + context_symbols`\n",
        "- `universe_full` → list from `universe.csv` (canonical equities universe for downstream code).\n",
        "\n",
        "### DataFrames (load-once, reuse)\n",
        "- `prices` → long OHLCV for `universe_all` (saved as `raw_prices.parquet`).\n",
        "- `features` → merged technical + context + fundamentals (post-shift, winsorize, z-score) (saved).\n",
        "- `vix` → `^VIX` close series; `spy` → SPY prices with `spy_ret`, `spy_rv_20`.\n",
        "- `ctx` → market context by date: `[\"spy_rv_20\",\"vix_close\",\"breadth\"]`.\n",
        "- `px_daily_all` → `[\"date\",\"ticker\",\"adj_close\"]` for equities universe.\n",
        "- `dates_all` → unique trading dates.\n",
        "- `funda_q` → quarterly fundamentals by ticker (saved).\n",
        "- `funda_daily` → daily forward-filled fundamentals (saved).\n",
        "\n",
        "### Feature Engineering Toggles / Windows\n",
        "- `COMPUTE_SLOPE = True`\n",
        "- `SLOPE_WINDOW = 20`\n",
        "- `RV_WIN = 20`\n",
        "- `ATR_WIN = 14`\n",
        "\n",
        "### Provider / API / Caching\n",
        "- `PROVIDER = \"fmp\"`\n",
        "- `FMP_API_KEY` – from env or prompt (in-memory only).\n",
        "- `CACHE_DIR = \"cache\"`\n",
        "- `CHUNK_TICKERS = 100`, `START_AT = 0`, `SKIP_IF_CACHED = True`\n",
        "- `MAX_WORKERS = 4`, `RETRY_ATTEMPTS = 5`, `BATCH_SLEEP = (0.2, 0.6)`\n",
        "\n",
        "### Useful Function Handles\n",
        "- `to_fmp_symbol(sym)` – Yahoo “-” ↔ FMP “.” class ticker mapping.\n",
        "- `is_index_like(sym)` – identifies index symbols (e.g., `^VIX`).\n",
        "- `compute_atr(df, window=ATR_WIN)`\n",
        "- `vectorized_rolling_slope(y, window=SLOPE_WINDOW)`\n",
        "- `mom_over_n(adj_close, n)`\n",
        "- `_tidy_quarterly_df(df)`, `_coalesce_cols(df, cols, default)`\n",
        "- `_fetch_quarterly_funda_fmp(ticker)` – pulls BS/IS/CF, coalesces variants.\n",
        "- `fetch_or_load_cached_quarterly(ticker)` – cached loader for fundamentals.\n",
        "- `cs_standardize_fast(df, cols, lo=0.01, hi=0.99)` – per-date winsorize+z-score.\n",
        "\n",
        "### Column Sets / Masks (downstream-friendly)\n",
        "- `non_feature_cols = {\"date\",\"ticker\",\"open\",\"high\",\"low\",\"close\",\"adj_close\",\"volume\"}`\n",
        "- `cols_to_shift` – all predictive feature columns actually shifted by 1 bar.\n",
        "- `cs_cols` – features standardized cross-sectionally (lags, vol, mom, fundamentals, etc.).\n",
        "- `context_keep_raw = [\"spy_rv_20\",\"vix_close\",\"breadth\"]`\n",
        "- *(QC section)*\n",
        "  - `FEATURES_PATH = \"features.parquet\"`, `UNIVERSE_PATH = \"universe.csv\"`\n",
        "  - `COVERAGE_MIN = 300`\n",
        "  - `APPLY_FILTERS = True`\n",
        "  - `Z_OUTLIER = 5.0`, `EARLY_YEARS = 5`, `RECENT_YEARS = 5`\n",
        "  - `warmup_cutoff` – computed from SPY date series (≈273 trading-day warmup).\n",
        "  - `keep_mask` – dates ≥ `warmup_cutoff` and with coverage ≥ `COVERAGE_MIN`.\n",
        "  - *(Note: `features_filtered.parquet` is written using `keep_mask` and pruned columns.)*\n",
        "\n",
        "---\n",
        "\n",
        "## What this means for Section 2 (Regimes)\n",
        "- **Use** `features_filtered.parquet` (or reload `features` and apply `keep_mask`) to build HMM inputs.\n",
        "- Inputs available out of the box: `spy_rv_20`, `vix_close`, `breadth`, and per-asset returns (`ret_1d`), plus everything in `cs_cols`.\n",
        "- **No duplicate `(date, ticker)` rows**, **no monotonic issues**; early sparse periods removed by `warmup_cutoff`/`keep_mask`.\n",
        "\n",
        "---\n",
        "\n",
        "## Sanity Questions (short answers)\n",
        "- **“Are we good to go?”** Yes — Section 1 is complete and validated; proceed to regime modeling.\n",
        "- **“Empty rows?”** Raw OHLCV rows with all NaNs were dropped; the modeling file (`features_filtered.parquet`) is filtered to warmup/coverage and prunes empty/constant columns. Row-level all-NaN feature cases should not remain after these filters.\n",
        "- **“Add the assertions?”** Already present and passing in QC (`qc_summary.json`). No need to add them again unless you change the pipeline.\n",
        "</details>"
      ],
      "metadata": {
        "id": "JGcOoZHEdNC4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Regime Modeling"
      ],
      "metadata": {
        "id": "u5YS_Mluwz_v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details> <summary>\n",
        "Outline (HMM → Regime Labels & Probabilities)</summary>\n",
        "\n",
        "# 2) Regime Modeling — Updated Outline (HMM → Regime Labels & Probabilities)\n",
        "\n",
        "## 2.0 Scope & Interfaces\n",
        "- **Goal:** Assign a daily market regime (Risk-On, Risk-Off, Transition) with posterior probabilities to drive regime-aware weighting, turnover caps, and risk targets in Sections 3–5.\n",
        "- **Inputs (from Section 1):**\n",
        "  - `features_filtered.parquet` with **raw** `spy_rv_20`, `vix_close`, `breadth`, and SPY `adj_close` for return computation.\n",
        "  - Trading calendar (aligned daily business days).\n",
        "- **Outputs (artifacts):**\n",
        "  - `regime_labels.parquet`: `date, state_id, p0..pK, regime_label`\n",
        "  - `regime_labels.csv` (plot-friendly)\n",
        "  - `regime_plot.png` (timeline with shading), `state_profiles.csv` (state stats)\n",
        "  - `regime_hmm.pkl` (bundle: scaler + HMM per walk-forward window)\n",
        "  - `regime_meta.json` (config, state→label map, scaler params, transition matrix, diagnostics)\n",
        "  - `regime_sensitivity.json` (K/feature/era stability tests)\n",
        "- **Pass/Fail gates:**\n",
        "  - Interpretable state profiles (return/vol ordering aligns with labels)\n",
        "  - Reasonable persistence (median run length > 5–10 days; no chattering)\n",
        "  - Stable mapping across walk-forward windows (low semantic flip rate)\n",
        "  - No leakage (all inputs at t known at t)\n",
        "\n",
        "---\n",
        "\n",
        "## 2.1 Data Assembly (Market Panel)\n",
        "- **Series:**\n",
        "  - SPY **log return** at t (computed from `adj_close`, shifted to avoid leakage if needed).\n",
        "  - SPY realized volatility (20-day) — from raw `spy_rv_20`.\n",
        "  - VIX **level** (`vix_close`) and optionally **daily Δ** (t − t-1).\n",
        "  - Market breadth (% advancers in S&P, known at t).\n",
        "- **IMPORTANT:** Use **raw** context series from Section 1 (`spy_rv_20`, `vix_close`, `breadth`), **not** cross-sectional z-scored features.\n",
        "- **Breadth timing:** Confirm that `breadth` reflects t-1 data available at t; if not, shift by 1.\n",
        "- **Alignment:** Daily business days; merge by `date`; forward-fill only for indicators known at t; drop rows with missing core inputs.\n",
        "- **Standardization:** Fit `StandardScaler` **per train window** on the raw context features; persist scaler per window (stored in `regime_hmm.pkl`).\n",
        "- **Sanity checks:**\n",
        "  - Stationarity proxy (mean/var drift over eras).\n",
        "  - Outlier handling: no winsorization needed for HMM since we scale raw series per window.\n",
        "  - Coverage check: ensure no missing dates in test stitching.\n",
        "\n",
        "---\n",
        "\n",
        "## 2.2 Model Choice & Configuration\n",
        "- **Primary:** Gaussian HMM with `covariance_type=\"full\"`; components K ∈ {2,3} (default 3).\n",
        "- **Alternative (optional):** Student-t HMM, GMM-HMM, Markov-Switching VAR, or Bayesian HMM with sticky priors.\n",
        "- **Hyperparameters:**\n",
        "  - `n_components`, `covariance_type`, `n_iter`, `random_state`.\n",
        "  - Dirichlet priors / sticky transitions to enforce regime persistence.\n",
        "- **Training protocol:**\n",
        "  - Train on standardized features in the train window.\n",
        "  - Multiple random restarts; choose model with highest log-likelihood.\n",
        "  - If applying **finance recency weighting rule**: optionally weight log-likelihood so recent data has more influence (can be implemented here if desired).\n",
        "\n",
        "---\n",
        "\n",
        "## 2.3 State Labeling & Semantics\n",
        "- **Profile each state:**\n",
        "  - Mean and vol of SPY returns.\n",
        "  - Mean VIX level, mean ΔVIX.\n",
        "  - Mean breadth, tail metrics (5% quantile returns).\n",
        "- **Label rules:**\n",
        "  - Highest mean return & lowest vol → **Risk-On**\n",
        "  - Highest vol & lowest return → **Risk-Off**\n",
        "  - Remaining state → **Transition**\n",
        "- **Tie-breakers:** breadth, VIX changes, downside tails.\n",
        "- **Persist mapping:** Save `state_id → regime_label` per window in `regime_meta.json` so semantics don’t silently drift across walk-forward windows.\n",
        "\n",
        "---\n",
        "\n",
        "## 2.4 Smoothing, Persistence & Debounce\n",
        "- **Posterior smoothing:** Option to use Viterbi most-likely path vs. raw posterior argmax.\n",
        "- **Debounce parameters:** `MIN_DWELL_DAYS` and `POSTERIOR_THRESH` from `config.yaml`.\n",
        "- **Gap handling:** Holidays/missing days inherit last known regime; no forward-looking fill.\n",
        "\n",
        "---\n",
        "\n",
        "## 2.5 Robustness & Sensitivity\n",
        "- **K sensitivity:** Run K=2 and K=3; prefer K with clearest separation (return/vol) and healthy dwell-time.\n",
        "- **Feature sensitivity:** Drop-one/add-one tests (remove VIX, remove breadth, etc.) to check label stability.\n",
        "- **Era stability:** Compare state profiles and transition matrices pre/post-2015 and during crisis years (e.g., 2020).\n",
        "- **Bootstrap:** Block bootstrap re-fit; produce confusion matrix for label stability across samples.\n",
        "\n",
        "---\n",
        "\n",
        "## 2.6 Diagnostics & QA\n",
        "- **Plots:**\n",
        "  - Timeline with regime shading over SPY price & drawdown.\n",
        "  - Posterior probabilities (stacked area).\n",
        "  - State return histograms, QQ plots.\n",
        "  - Transition matrix heatmap, dwell-time distribution.\n",
        "- **Tables:**\n",
        "  - State profiles (returns, vol, VIX, breadth, tails).\n",
        "  - Transition matrix & steady-state distribution.\n",
        "  - Switch frequency and chattering metrics.\n",
        "- **Alerts:**\n",
        "  - Flag if any state has inconsistent semantics (positive mean but top-2 vol, dwell-time < 3 days, mapping flips).\n",
        "\n",
        "---\n",
        "\n",
        "## 2.7 Regime-Aware Policy Hooks (Interfaces to Sections 3–5)\n",
        "- **Weights & turnover caps:** JSON map per regime (e.g., throttle momentum in Risk-Off, upweight quality).\n",
        "- **Risk targets:** Per-regime vol targets (e.g., 10%/8%/6% for On/Trans/Off).\n",
        "- **Hedge intensity:** Baseline hedge ratios per regime; pass to RL policy as defaults.\n",
        "- **Confidence proxy:** Use max posterior or entropy to scale aggressiveness.\n",
        "\n",
        "---\n",
        "\n",
        "## 2.8 Walk-Forward Integration\n",
        "- **Windows:** Match Section 6 (rolling/expanding).\n",
        "- **Per window:**\n",
        "  - Fit scaler + HMM on train subset.\n",
        "  - Apply to test subset only.\n",
        "  - Save artifacts: `regime_labels_<winid>.parquet`, `regime_hmm.pkl`, `regime_meta.json`.\n",
        "- **Stitching:** Concatenate per-window outputs into one continuous timeline for backtests.\n",
        "- **Label stability:** Use saved state→label mapping to avoid regime meaning drift.\n",
        "\n",
        "---\n",
        "\n",
        "## 2.9 Forward (Shadow) Mode\n",
        "- **Daily update:** Apply persisted scaler + HMM to latest t; append to `regime_labels.parquet`.\n",
        "- **Retrain cadence:** Weekly/bi-weekly.\n",
        "- **Logging:** Save model hash, posterior, chosen label, features vector.\n",
        "- **Alerts:** If mapping flips or dwell-time anomaly detected.\n",
        "\n",
        "---\n",
        "\n",
        "## 2.10 Configuration & Reproducibility\n",
        "- **Config keys (`config.yaml`):**\n",
        "  - Features list for HMM.\n",
        "  - `n_components`, `MIN_DWELL_DAYS`, `POSTERIOR_THRESH`.\n",
        "  - Finance recency weighting toggle & decay parameter (if implemented here).\n",
        "  - Random seed, plot toggles.\n",
        "- **Serialization:**\n",
        "  - joblib for model + scaler.\n",
        "  - JSON for meta (labels, thresholds, diagnostics).\n",
        "- **Tests:**\n",
        "  - Deterministic output with fixed seed.\n",
        "  - No leakage (t-only features).\n",
        "  - Posterior rows sum to 1; dates strictly increasing.\n",
        "  - No gaps after stitching.\n",
        "  - Label semantics test per window.\n",
        "\n",
        "---\n",
        "\n",
        "## 2.11 Deliverables Checklist\n",
        "- `regime_labels.parquet` (+ CSV).\n",
        "- `regime_hmm.pkl` (model + scaler per window).\n",
        "- `regime_meta.json` (state→label, scaler params, diagnostics).\n",
        "- `regime_timeline.png`, `regime_posteriors.png`, `state_profiles.csv`, `transition_matrix.csv`.\n",
        "- `regime_sensitivity.json` (K/feature/era stability).\n",
        "- `regime_policy_map.json` (interfaces to Sections 3–5).\n",
        "\n",
        "\n",
        "---\n",
        "</details>"
      ],
      "metadata": {
        "id": "mo-4MmP4fdjw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Section 2.0 — Scope & Interfaces (Regime Modeling bootstrap)\n",
        "# Builds on Section 1 artifacts; defines config, I/O, sanity checks,\n",
        "# and prepares the market-level panel stub used by 2.1+ (no HMM yet).\n",
        "# ============================================================\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import os\n",
        "import json\n",
        "import yaml\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import Dict, Any, List\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "# 0) Paths & directories (reuse Section 1 outputs)\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "FEATURES_PATH_DEFAULT = (\n",
        "    \"features_filtered.parquet\"\n",
        "    if os.path.exists(\"features_filtered.parquet\")\n",
        "    else \"features.parquet\"\n",
        ")\n",
        "UNIVERSE_PATH = \"universe.csv\"\n",
        "ARTIFACT_DIR = \"artifacts\"\n",
        "REGIME_DIR = os.path.join(ARTIFACT_DIR, \"regimes\")\n",
        "PLOTS_DIR = os.path.join(REGIME_DIR, \"plots\")\n",
        "\n",
        "os.makedirs(REGIME_DIR, exist_ok=True)\n",
        "os.makedirs(PLOTS_DIR, exist_ok=True)\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "# 1) Config — defaults + optional override via config.yaml\n",
        "# Keys are intentionally minimal here; 2.1–2.10 will read them.\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "@dataclass\n",
        "class RegimeConfig:\n",
        "    # Raw context features for HMM (NOT cross-sectional z-scores)\n",
        "    hmm_features: List[str]\n",
        "    include_dvix: bool                 # add ΔVIX feature to panel\n",
        "    n_components_grid: List[int]       # HMM K sensitivity (e.g., [2,3])\n",
        "    covariance_type: str               # \"full\" by default\n",
        "    random_seed: int\n",
        "    # Debounce (used later in 2.4)\n",
        "    min_dwell_days: int\n",
        "    posterior_thresh: float\n",
        "    # Optional finance rule: give more weight to recent samples during HMM fit\n",
        "    recency_weighting: bool\n",
        "    recency_half_life_days: int\n",
        "    # I/O\n",
        "    plots_enabled: bool\n",
        "    save_csv_alongside_parquet: bool\n",
        "    features_path: str = FEATURES_PATH_DEFAULT\n",
        "    universe_path: str = UNIVERSE_PATH\n",
        "    regime_dir: str = REGIME_DIR\n",
        "    plots_dir: str = PLOTS_DIR\n",
        "\n",
        "DEFAULT_CFG = RegimeConfig(\n",
        "    hmm_features=[\"spy_rv_20\", \"vix_close\", \"breadth\"],  # from Section 1 (raw context)\n",
        "    include_dvix=True,\n",
        "    n_components_grid=[2, 3],\n",
        "    covariance_type=\"full\",\n",
        "    random_seed=42,\n",
        "    min_dwell_days=3,\n",
        "    posterior_thresh=0.55,\n",
        "    recency_weighting=False,           # flip to True if enabling in 2.2\n",
        "    recency_half_life_days=90,\n",
        "    plots_enabled=True,\n",
        "    save_csv_alongside_parquet=True,\n",
        ")\n",
        "\n",
        "CONFIG_FILE = \"config.yaml\"\n",
        "user_cfg = {}\n",
        "if os.path.exists(CONFIG_FILE):\n",
        "    try:\n",
        "        with open(CONFIG_FILE, \"r\") as f:\n",
        "            raw_cfg = yaml.safe_load(f) or {}\n",
        "            if isinstance(raw_cfg, dict):\n",
        "                user_cfg = raw_cfg.get(\"regimes\", {}) or {}\n",
        "    except Exception:\n",
        "        user_cfg = {}\n",
        "\n",
        "def merge_cfg(default: RegimeConfig, override: Dict[str, Any]) -> RegimeConfig:\n",
        "    d = asdict(default)\n",
        "    for k, v in override.items():\n",
        "        if k in d and v is not None:\n",
        "            d[k] = v\n",
        "    return RegimeConfig(**d)\n",
        "\n",
        "CFG = merge_cfg(DEFAULT_CFG, user_cfg)\n",
        "\n",
        "# Persist effective config for traceability\n",
        "with open(os.path.join(REGIME_DIR, \"regime_config_effective.json\"), \"w\") as f:\n",
        "    json.dump(asdict(CFG), f, indent=2)\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "# 2) Load Section 1 artifacts and build the market panel stub\n",
        "# NOTE: use RAW context features from Section 1 (no CS-z).\n",
        "# This version auto-detects whether ^VIX exists as a ticker,\n",
        "# or vix_close/breadth/spy_rv_20 are already on every row.\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "assert os.path.exists(CFG.features_path), f\"Missing features file: {CFG.features_path}\"\n",
        "fe = pd.read_parquet(CFG.features_path)\n",
        "fe[\"date\"] = pd.to_datetime(fe[\"date\"], utc=False, errors=\"coerce\")\n",
        "fe = fe.dropna(subset=[\"date\"]).sort_values([\"date\", \"ticker\"])\n",
        "\n",
        "# Required columns present?\n",
        "required_cols = {\"date\", \"ticker\", \"adj_close\", \"spy_rv_20\", \"vix_close\", \"breadth\"}\n",
        "missing = list(required_cols - set(fe.columns))\n",
        "if missing:\n",
        "    raise ValueError(f\"Required columns missing in features file: {sorted(missing)}\")\n",
        "\n",
        "# SPY must exist for returns\n",
        "if not (fe[\"ticker\"] == \"SPY\").any():\n",
        "    raise ValueError(\"SPY rows not found in features file; cannot compute spy_ret.\")\n",
        "\n",
        "# Build SPY returns\n",
        "spy = fe.loc[fe[\"ticker\"] == \"SPY\", [\"date\", \"adj_close\", \"spy_rv_20\"]].copy()\n",
        "spy[\"spy_ret\"] = np.log(spy[\"adj_close\"] / spy[\"adj_close\"].shift(1))\n",
        "\n",
        "# vix_close / breadth / rv_20 may be replicated on every row; prefer unique-by-date view\n",
        "# If ^VIX rows exist, we can still just take unique-by-date—works for both layouts.\n",
        "vix_by_date = fe[[\"date\", \"vix_close\"]].drop_duplicates(\"date\").copy()\n",
        "breadth_by_date = fe[[\"date\", \"breadth\"]].drop_duplicates(\"date\").copy()\n",
        "rv20_by_date = fe[[\"date\", \"spy_rv_20\"]].drop_duplicates(\"date\").copy()\n",
        "\n",
        "# Merge market panel (date-level)\n",
        "mkt = (\n",
        "    spy[[\"date\", \"spy_ret\"]]                     # SPY returns\n",
        "    .merge(rv20_by_date, on=\"date\", how=\"inner\") # realized vol\n",
        "    .merge(vix_by_date, on=\"date\", how=\"inner\")  # VIX level\n",
        "    .merge(breadth_by_date, on=\"date\", how=\"inner\")  # breadth\n",
        "    .sort_values(\"date\")\n",
        ")\n",
        "\n",
        "# Optional ΔVIX (level change)\n",
        "if CFG.include_dvix:\n",
        "    mkt[\"dvix\"] = mkt[\"vix_close\"].diff()\n",
        "\n",
        "# Breadth timing guard: uncomment if your breadth is same-day and should be known-at-t\n",
        "# mkt[\"breadth\"] = mkt[\"breadth\"].shift(1)\n",
        "\n",
        "# Complete-case rows only (HMM requires no NaNs)\n",
        "core_cols = [\"spy_ret\", \"spy_rv_20\", \"vix_close\", \"breadth\"] + ([\"dvix\"] if CFG.include_dvix else [])\n",
        "mkt = mkt.dropna(subset=core_cols).reset_index(drop=True)\n",
        "\n",
        "# Save panel (consumed by 2.1/2.2)\n",
        "panel_path = os.path.join(CFG.regime_dir, \"market_panel.parquet\")\n",
        "mkt.to_parquet(panel_path, index=False)\n",
        "if CFG.save_csv_alongside_parquet:\n",
        "    mkt.to_csv(os.path.join(CFG.regime_dir, \"market_panel.csv\"), index=False)\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "# 5) Finalize & console summary (with robust date handling)\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "def _fmt_date(ts):\n",
        "    return None if pd.isna(ts) else pd.Timestamp(ts).strftime(\"%Y-%m-%d\")\n",
        "\n",
        "if mkt.empty:\n",
        "    # Diagnostics to help you decide if breadth shift is needed, etc.\n",
        "    core_for_diag = [\"spy_ret\", \"spy_rv_20\", \"vix_close\", \"breadth\"] + ([\"dvix\"] if CFG.include_dvix else [])\n",
        "    non_null_counts = {c: int(fe[c].notna().sum()) if c in fe.columns else 0 for c in core_for_diag}\n",
        "    spy_src = fe.loc[fe[\"ticker\"] == \"SPY\", [\"date\", \"adj_close\", \"spy_rv_20\"]].assign(\n",
        "        spy_ret=lambda d: np.log(d[\"adj_close\"] / d[\"adj_close\"].shift(1))\n",
        "    )\n",
        "    coverage_diag = {\n",
        "        \"rows_with_spy_ret_and_rv20\": int(spy_src.dropna(subset=[\"spy_ret\", \"spy_rv_20\"]).shape[0]),\n",
        "        \"unique_dates_with_vix\": int(vix_by_date.dropna(subset=[\"vix_close\"]).shape[0]),\n",
        "        \"unique_dates_with_breadth\": int(breadth_by_date.dropna(subset=[\"breadth\"]).shape[0]),\n",
        "    }\n",
        "    raise ValueError(\n",
        "        \"Market panel is empty after merging/dropping NaNs. \"\n",
        "        f\"Non-null counts (in features file): {non_null_counts}. \"\n",
        "        f\"Coverage by component: {coverage_diag}. \"\n",
        "        \"Common fixes: ensure breadth timing (try shifting breadth by 1), \"\n",
        "        \"or check for gaps in SPY/VIX/breadth date alignment.\"\n",
        "    )\n",
        "\n",
        "summary = {\n",
        "    \"features_file\": CFG.features_path,\n",
        "    \"universe_file\": CFG.universe_path,\n",
        "    \"market_panel_rows\": int(mkt.shape[0]),\n",
        "    \"market_panel_cols\": list(mkt.columns),\n",
        "    \"date_min\": _fmt_date(mkt['date'].min()),\n",
        "    \"date_max\": _fmt_date(mkt['date'].max()),\n",
        "    \"config_effective\": os.path.abspath(os.path.join(CFG.regime_dir, \"regime_config_effective.json\")),\n",
        "    \"panel_path\": os.path.abspath(panel_path),\n",
        "    \"meta_path\": os.path.abspath(os.path.join(CFG.regime_dir, 'regime_meta.json')),\n",
        "}\n",
        "print(json.dumps(summary, indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z59_nRdbzRNn",
        "outputId": "7f9e3be1-8cbd-4fad-f61f-7e3ffe8bb18f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"features_file\": \"features_filtered.parquet\",\n",
            "  \"universe_file\": \"universe.csv\",\n",
            "  \"market_panel_rows\": 4657,\n",
            "  \"market_panel_cols\": [\n",
            "    \"date\",\n",
            "    \"spy_ret\",\n",
            "    \"spy_rv_20\",\n",
            "    \"vix_close\",\n",
            "    \"breadth\",\n",
            "    \"dvix\"\n",
            "  ],\n",
            "  \"date_min\": \"2007-02-06\",\n",
            "  \"date_max\": \"2025-08-08\",\n",
            "  \"config_effective\": \"/content/artifacts/regimes/regime_config_effective.json\",\n",
            "  \"panel_path\": \"/content/artifacts/regimes/market_panel.parquet\",\n",
            "  \"meta_path\": \"/content/artifacts/regimes/regime_meta.json\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Section 2.1 — Data Assembly (windowed extraction + scaling)\n",
        "# Uses artifacts from 2.0; prepares X_train/X_test for HMM.\n",
        "# ============================================================\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import os\n",
        "import json\n",
        "from dataclasses import asdict\n",
        "from typing import Dict, Any, Tuple, List, Optional\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import joblib\n",
        "\n",
        "# Reuse CFG, paths from 2.0\n",
        "REGIME_DIR = CFG.regime_dir\n",
        "PLOTS_DIR = CFG.plots_dir\n",
        "PANEL_PATH = os.path.join(REGIME_DIR, \"market_panel.parquet\")\n",
        "META_PATH = os.path.join(REGIME_DIR, \"regime_meta.json\")\n",
        "\n",
        "assert os.path.exists(PANEL_PATH), f\"Missing market panel: {PANEL_PATH}\"\n",
        "mkt = pd.read_parquet(PANEL_PATH)\n",
        "mkt[\"date\"] = pd.to_datetime(mkt[\"date\"])\n",
        "mkt = mkt.sort_values(\"date\").reset_index(drop=True)\n",
        "\n",
        "# Choose feature list (raw context features only; dvix optional)\n",
        "hmm_feat_cols = list(CFG.hmm_features)\n",
        "if CFG.include_dvix and \"dvix\" not in hmm_feat_cols:\n",
        "    hmm_feat_cols.append(\"dvix\")\n",
        "\n",
        "# Safety: ensure columns exist\n",
        "missing_cols = [c for c in hmm_feat_cols + [\"spy_ret\"] if c not in mkt.columns]\n",
        "if missing_cols:\n",
        "    raise ValueError(f\"Missing required columns in market panel: {missing_cols}\")\n",
        "\n",
        "def make_window_masks(df: pd.DataFrame,\n",
        "                      train_start: str,\n",
        "                      train_end: str,\n",
        "                      test_start: str,\n",
        "                      test_end: str) -> Tuple[pd.Series, pd.Series]:\n",
        "    d = df[\"date\"]\n",
        "    train_mask = (d >= pd.to_datetime(train_start)) & (d <= pd.to_datetime(train_end))\n",
        "    test_mask  = (d >= pd.to_datetime(test_start))  & (d <= pd.to_datetime(test_end))\n",
        "    return train_mask, test_mask\n",
        "\n",
        "def build_hmm_matrices(df: pd.DataFrame,\n",
        "                       features: List[str],\n",
        "                       train_start: str,\n",
        "                       train_end: str,\n",
        "                       test_start: str,\n",
        "                       test_end: str,\n",
        "                       scaler_out_path: Optional[str] = None,\n",
        "                       breadth_shift_days: int = 0) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      {\n",
        "        'X_train': np.ndarray,\n",
        "        'X_test': np.ndarray,\n",
        "        'dates_train': pd.DatetimeIndex,\n",
        "        'dates_test': pd.DatetimeIndex,\n",
        "        'scaler_path': str,\n",
        "        'scaler_mean': list,\n",
        "        'scaler_scale': list,\n",
        "        'qc': dict\n",
        "      }\n",
        "    \"\"\"\n",
        "    dfw = df.copy()\n",
        "\n",
        "    # Optional breadth shift (if you decide breadth should be known-at-t from t-1)\n",
        "    if breadth_shift_days != 0 and \"breadth\" in features:\n",
        "        dfw[\"breadth\"] = dfw[\"breadth\"].shift(breadth_shift_days)\n",
        "\n",
        "    # Drop rows with missing features\n",
        "    dfw = dfw.dropna(subset=features).reset_index(drop=True)\n",
        "\n",
        "    # Window masks\n",
        "    tr_mask, te_mask = make_window_masks(dfw, train_start, train_end, test_start, test_end)\n",
        "\n",
        "    # Slice\n",
        "    train_df = dfw.loc[tr_mask, [\"date\"] + features].dropna()\n",
        "    test_df  = dfw.loc[te_mask, [\"date\"] + features].dropna()\n",
        "\n",
        "    if train_df.empty or test_df.empty:\n",
        "        raise ValueError(\n",
        "            f\"Empty train/test after slicing: \"\n",
        "            f\"train({train_start}→{train_end}) rows={train_df.shape[0]}, \"\n",
        "            f\"test({test_start}→{test_end}) rows={test_df.shape[0]}. \"\n",
        "            f\"Consider adjusting dates or breadth_shift_days.\"\n",
        "        )\n",
        "\n",
        "    # Standardize on TRAIN ONLY; transform TEST with same scaler\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(train_df[features].to_numpy(dtype=float))\n",
        "    X_test  = scaler.transform(test_df[features].to_numpy(dtype=float))\n",
        "\n",
        "    # Persist scaler per window\n",
        "    if scaler_out_path is None:\n",
        "        win_tag = f\"{train_start}_{train_end}__{test_start}_{test_end}\".replace(\"-\", \"\")\n",
        "        scaler_out_path = os.path.join(REGIME_DIR, f\"scaler_{win_tag}.joblib\")\n",
        "    joblib.dump(scaler, scaler_out_path)\n",
        "\n",
        "    # Quick QC: mean/var drift (train vs test) and feature coverage\n",
        "    qc = {\n",
        "        \"train_rows\": int(train_df.shape[0]),\n",
        "        \"test_rows\": int(test_df.shape[0]),\n",
        "        \"features\": features,\n",
        "        \"train_means\": dict(zip(features, np.mean(X_train, axis=0).round(6).tolist())),\n",
        "        \"train_stds\": dict(zip(features, np.std(X_train, axis=0, ddof=0).round(6).tolist())),\n",
        "        \"test_means\": dict(zip(features, np.mean(X_test, axis=0).round(6).tolist())),\n",
        "        \"test_stds\": dict(zip(features, np.std(X_test, axis=0, ddof=0).round(6).tolist())),\n",
        "    }\n",
        "\n",
        "    # Save a tiny per-window QC file\n",
        "    win_qc_path = scaler_out_path.replace(\".joblib\", \"_qc.json\")\n",
        "    with open(win_qc_path, \"w\") as f:\n",
        "        json.dump(qc, f, indent=2)\n",
        "\n",
        "    return {\n",
        "        \"X_train\": X_train,\n",
        "        \"X_test\": X_test,\n",
        "        \"dates_train\": train_df[\"date\"].to_list(),\n",
        "        \"dates_test\": test_df[\"date\"].to_list(),\n",
        "        \"scaler_path\": scaler_out_path,\n",
        "        \"scaler_mean\": scaler.mean_.round(12).tolist(),\n",
        "        \"scaler_scale\": scaler.scale_.round(12).tolist(),\n",
        "        \"qc\": qc,\n",
        "    }\n",
        "\n",
        "# Example: pick a first walk-forward split anchored to your warmup_cutoff\n",
        "# You can replace these with your Section 6 generator later.\n",
        "train_start = \"2007-02-06\"  # day after warmup_cutoff in your QC\n",
        "train_end   = \"2016-12-30\"\n",
        "test_start  = \"2017-01-03\"\n",
        "test_end    = mkt[\"date\"].max().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "window = build_hmm_matrices(\n",
        "    df=mkt,\n",
        "    features=hmm_feat_cols,\n",
        "    train_start=train_start,\n",
        "    train_end=train_end,\n",
        "    test_start=test_start,\n",
        "    test_end=test_end,\n",
        "    scaler_out_path=None,\n",
        "    breadth_shift_days=0,  # set to 1 if you confirm breadth must be known-at-t from t-1\n",
        ")\n",
        "\n",
        "# Persist a small window manifest so later steps (2.2+) can load it\n",
        "manifest = {\n",
        "    \"window\": {\n",
        "        \"train_start\": train_start,\n",
        "        \"train_end\": train_end,\n",
        "        \"test_start\": test_start,\n",
        "        \"test_end\": test_end,\n",
        "    },\n",
        "    \"features\": hmm_feat_cols,\n",
        "    \"scaler_path\": window[\"scaler_path\"],\n",
        "    \"n_train\": len(window[\"dates_train\"]),\n",
        "    \"n_test\": len(window[\"dates_test\"]),\n",
        "}\n",
        "with open(os.path.join(REGIME_DIR, \"window_manifest.json\"), \"w\") as f:\n",
        "    json.dump(manifest, f, indent=2)\n",
        "\n",
        "print(json.dumps({\n",
        "    \"status\": \"2.1 ready\",\n",
        "    \"features_used\": hmm_feat_cols,\n",
        "    \"scaler_saved\": window[\"scaler_path\"],\n",
        "    \"train_rows\": manifest[\"n_train\"],\n",
        "    \"test_rows\": manifest[\"n_test\"],\n",
        "}, indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfBUiQIZvMAS",
        "outputId": "e259e4d0-568a-4b80-f2e5-a317fd5f27ea"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"status\": \"2.1 ready\",\n",
            "  \"features_used\": [\n",
            "    \"spy_rv_20\",\n",
            "    \"vix_close\",\n",
            "    \"breadth\",\n",
            "    \"dvix\"\n",
            "  ],\n",
            "  \"scaler_saved\": \"artifacts/regimes/scaler_20070206_20161230__20170103_20250808.joblib\",\n",
            "  \"train_rows\": 2495,\n",
            "  \"test_rows\": 2162\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hmmlearn --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQf0An0h0TWS",
        "outputId": "18cbef45-40d7-4552-be25-70394b5e6c05"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/165.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m153.6/165.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.9/165.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Section 2.2 — Model Choice & Configuration (Gaussian HMM)\n",
        "# Primary: GaussianHMM (full covariance), K in {2,3}\n",
        "# - Multiple restarts; pick best train log-likelihood\n",
        "# - Sticky transitions (Dirichlet-like persistence) via diagonal bias\n",
        "# - Finance recency weighting: time-decayed sub-sequences (ENABLED)\n",
        "# Reuses:\n",
        "#   - artifacts/regimes/market_panel.parquet (from 2.0)\n",
        "#   - artifacts/regimes/window_manifest.json (from 2.1)\n",
        "#   - scaler_*.joblib (from 2.1)\n",
        "# Outputs:\n",
        "#   - artifacts/regimes/hmm_best.pkl (joblib bundle: model + meta)\n",
        "#   - artifacts/regimes/hmm_kgrid.json (scores by K)\n",
        "# ============================================================\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import os\n",
        "import json\n",
        "from typing import Dict, Any, List, Tuple\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "from hmmlearn.hmm import GaussianHMM\n",
        "\n",
        "# Reuse config and paths from 2.0 / 2.1\n",
        "REGIME_DIR = CFG.regime_dir\n",
        "PANEL_PATH = os.path.join(REGIME_DIR, \"market_panel.parquet\")\n",
        "MANIFEST_PATH = os.path.join(REGIME_DIR, \"window_manifest.json\")\n",
        "META_PATH = os.path.join(REGIME_DIR, \"regime_meta.json\")\n",
        "\n",
        "assert os.path.exists(PANEL_PATH), f\"Missing market panel: {PANEL_PATH}\"\n",
        "assert os.path.exists(MANIFEST_PATH), f\"Missing window manifest: {MANIFEST_PATH}\"\n",
        "\n",
        "with open(MANIFEST_PATH, \"r\") as f:\n",
        "    MAN = json.load(f)\n",
        "\n",
        "mkt = pd.read_parquet(PANEL_PATH).sort_values(\"date\").reset_index(drop=True)\n",
        "mkt[\"date\"] = pd.to_datetime(mkt[\"date\"])\n",
        "\n",
        "scaler = joblib.load(MAN[\"scaler_path\"])\n",
        "features = MAN[\"features\"]\n",
        "assert all(c in mkt.columns for c in features), f\"Panel missing features: {set(features) - set(mkt.columns)}\"\n",
        "\n",
        "# Train/test windows\n",
        "train_start = pd.to_datetime(MAN[\"window\"][\"train_start\"])\n",
        "train_end   = pd.to_datetime(MAN[\"window\"][\"train_end\"])\n",
        "test_start  = pd.to_datetime(MAN[\"window\"][\"test_start\"])\n",
        "test_end    = pd.to_datetime(MAN[\"window\"][\"test_end\"])\n",
        "\n",
        "train_df = mkt[(mkt[\"date\"] >= train_start) & (mkt[\"date\"] <= train_end)][[\"date\"] + features].dropna().reset_index(drop=True)\n",
        "test_df  = mkt[(mkt[\"date\"] >= test_start)  & (mkt[\"date\"] <= test_end)][[\"date\"] + features].dropna().reset_index(drop=True)\n",
        "\n",
        "X_train = scaler.transform(train_df[features].to_numpy(dtype=float))\n",
        "X_test  = scaler.transform(test_df[features].to_numpy(dtype=float))\n",
        "dates_train = train_df[\"date\"].to_numpy()\n",
        "dates_test  = test_df[\"date\"].to_numpy()\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "# Hyperparameters — Test run now, bump for real run (marked TOCHANGE)\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "N_COMPONENTS_GRID = [2]   # TOCHANGE: [3] or [2,3] for real run\n",
        "N_ITER = 200              # TOCHANGE: 1000 for real run\n",
        "N_INIT = 2                # TOCHANGE: 10 for real run\n",
        "RANDOM_STATE = 42\n",
        "COVARIANCE_TYPE = \"full\"\n",
        "TOL = 1e-3                # TOCHANGE: 1e-4 for real run\n",
        "\n",
        "# Sticky transitions strength (Dirichlet-like, diagonal blend post-fit)\n",
        "# Larger -> stickier regimes (longer dwell times)\n",
        "LAMBDA_STICK = 0.15       # TOCHANGE: 0.30–0.50 for real run\n",
        "\n",
        "# Finance recency weighting — ENABLED\n",
        "APPLY_RECENCY = True\n",
        "HALF_LIFE_DAYS = 756      # ~3 years; keeps 2008 meaningful\n",
        "# TOCHANGE: try 504 (~2y, more recency), 756 (~3y, balanced), 1260 (~5y, less recency)\n",
        "\n",
        "EPSILON_FLOOR = 0.10      # ensures old episodes never get <10% of peak weight\n",
        "# TOCHANGE: 0.05–0.15 depending on how protective you want to be\n",
        "\n",
        "# For recency sampler (still lightweight in test; scale for real run)\n",
        "SEG_LEN = 60              # TOCHANGE: 90–120 for real run\n",
        "N_SEGMENTS = 80           # TOCHANGE: 200–400 for real run\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "# Utilities\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "def _diag_sticky_blend(transmat: np.ndarray, lam: float) -> np.ndarray:\n",
        "    k = transmat.shape[0]\n",
        "    T = (1.0 - lam) * transmat + lam * np.eye(k)\n",
        "    T = T / T.sum(axis=1, keepdims=True)\n",
        "    return T\n",
        "\n",
        "def _build_time_decay_weights(dates: np.ndarray, half_life_days: int) -> np.ndarray:\n",
        "    t = np.array([pd.Timestamp(d).toordinal() for d in dates], dtype=float)\n",
        "    age = (t.max() - t)  # newer dates -> smaller age\n",
        "    decay = np.log(2) / max(1, half_life_days)\n",
        "    w = np.exp(-decay * age)\n",
        "    return w / (w.sum() + 1e-12)\n",
        "\n",
        "def _sample_time_weighted_subsequences(\n",
        "    X: np.ndarray,\n",
        "    dates: np.ndarray,\n",
        "    seg_len: int,\n",
        "    n_segments: int,\n",
        "    half_life_days: int,\n",
        "    random_state: int,\n",
        ") -> Tuple[np.ndarray, List[int]]:\n",
        "    rng = np.random.RandomState(random_state)\n",
        "    n = X.shape[0]\n",
        "    if n < seg_len:\n",
        "        return X.copy(), [n]\n",
        "    ends = np.arange(seg_len - 1, n)\n",
        "    p = _build_time_decay_weights(dates[ends], half_life_days)\n",
        "    p = np.maximum(p, EPSILON_FLOOR * p.max())\n",
        "    p = p / p.sum()\n",
        "\n",
        "    chosen = rng.choice(ends, size=min(n_segments, len(ends)), replace=True, p=p)\n",
        "    lengths, chunks = [], []\n",
        "    for e in chosen:\n",
        "        s = e - (seg_len - 1)\n",
        "        chunk = X[s:e+1]\n",
        "        chunks.append(chunk)\n",
        "        lengths.append(len(chunk))\n",
        "    X_concat = np.vstack(chunks)\n",
        "    return X_concat, lengths\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "# Train HMMs; pick best by train log-likelihood\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "best = {\"score\": -np.inf, \"model\": None, \"k\": None, \"seed\": None, \"train_lengths\": None, \"fit_mode\": None}\n",
        "results = []\n",
        "\n",
        "for k in N_COMPONENTS_GRID:\n",
        "    for r in range(N_INIT):\n",
        "        seed = RANDOM_STATE + r\n",
        "\n",
        "        if APPLY_RECENCY:\n",
        "            X_fit, lengths = _sample_time_weighted_subsequences(\n",
        "                X_train, dates_train,\n",
        "                seg_len=SEG_LEN,\n",
        "                n_segments=N_SEGMENTS,\n",
        "                half_life_days=HALF_LIFE_DAYS,\n",
        "                random_state=seed,\n",
        "            )\n",
        "            fit_mode = \"recency\"\n",
        "        else:\n",
        "            X_fit, lengths = X_train, [len(X_train)]\n",
        "            fit_mode = \"plain\"\n",
        "\n",
        "        # Init model\n",
        "        model = GaussianHMM(\n",
        "            n_components=k,\n",
        "            covariance_type=COVARIANCE_TYPE,\n",
        "            n_iter=N_ITER,\n",
        "            tol=TOL,\n",
        "            random_state=seed,\n",
        "            verbose=False,\n",
        "            # IMPORTANT: do not include 's' or 't' here, otherwise your custom\n",
        "            # startprob_/transmat_ get overwritten on init.\n",
        "            init_params=\"mc\",      # means, covars only\n",
        "            params=\"stmc\",         # learn startprob, transmat, means, covars\n",
        "        )\n",
        "\n",
        "        # Sticky-biased initialization (near-diagonal)\n",
        "        trans0 = np.full((k, k), (1.0 - 0.90) / max(1, k - 1))\n",
        "        np.fill_diagonal(trans0, 0.90)\n",
        "        model.transmat_ = trans0\n",
        "\n",
        "        # Uniform start probabilities\n",
        "        model.startprob_ = np.full(k, 1.0 / k)\n",
        "\n",
        "        # Fit\n",
        "        model.fit(X_fit, lengths=lengths)\n",
        "\n",
        "        # Post-fit sticky blend (Dirichlet-like)\n",
        "        model.transmat_ = _diag_sticky_blend(model.transmat_, LAMBDA_STICK)\n",
        "\n",
        "        score = model.score(X_train)  # comparable scoring on original train sequence\n",
        "\n",
        "        results.append({\n",
        "            \"k\": k,\n",
        "            \"seed\": seed,\n",
        "            \"score\": float(score),\n",
        "            \"fit_mode\": fit_mode,\n",
        "            \"transmat\": model.transmat_.tolist(),\n",
        "        })\n",
        "\n",
        "        if score > best[\"score\"]:\n",
        "            best.update({\"score\": score, \"model\": model, \"k\": k, \"seed\": seed, \"train_lengths\": lengths, \"fit_mode\": fit_mode})\n",
        "\n",
        "# Save grid scores\n",
        "with open(os.path.join(REGIME_DIR, \"hmm_kgrid.json\"), \"w\") as f:\n",
        "    json.dump({\"results\": results, \"chosen\": {\"k\": best[\"k\"], \"seed\": best[\"seed\"], \"score\": float(best[\"score\"]), \"fit_mode\": best[\"fit_mode\"]}}, f, indent=2)\n",
        "\n",
        "# Persist best model bundle\n",
        "bundle = {\n",
        "    \"model\": best[\"model\"],\n",
        "    \"k\": best[\"k\"],\n",
        "    \"random_state\": best[\"seed\"],\n",
        "    \"features\": features,\n",
        "    \"scaler_path\": MAN[\"scaler_path\"],\n",
        "    \"train_dates\": [str(d) for d in dates_train],\n",
        "    \"test_dates\": [str(d) for d in dates_test],\n",
        "    \"fit_mode\": best[\"fit_mode\"],\n",
        "    \"sticky_lambda\": LAMBDA_STICK,\n",
        "    \"n_iter\": N_ITER,\n",
        "    \"n_init\": N_INIT,\n",
        "    \"tol\": TOL,\n",
        "    \"covariance_type\": COVARIANCE_TYPE,\n",
        "    \"recency_weighting\": True,  # enabled\n",
        "    \"recency_half_life_days\": HALF_LIFE_DAYS,\n",
        "    \"recency_seg_len\": SEG_LEN,         # TOCHANGE: 90–120\n",
        "    \"recency_n_segments\": N_SEGMENTS,   # TOCHANGE: 200–400\n",
        "    \"created_at\": datetime.utcnow().isoformat() + \"Z\",\n",
        "    \"recency_epsilon_floor\": EPSILON_FLOOR\n",
        "}\n",
        "joblib.dump(bundle, os.path.join(REGIME_DIR, \"hmm_best.pkl\"))\n",
        "\n",
        "joblib.dump(bundle, os.path.join(REGIME_DIR, \"regime_hmm.pkl\"))  # alias for checklist\n",
        "\n",
        "print(json.dumps({\n",
        "    \"status\": \"2.2 trained\",\n",
        "    \"chosen_k\": best[\"k\"],\n",
        "    \"fit_mode\": best[\"fit_mode\"],\n",
        "    \"train_score\": float(best[\"score\"]),\n",
        "    \"n_iter\": N_ITER,\n",
        "    \"n_init\": N_INIT,\n",
        "    \"sticky_lambda\": LAMBDA_STICK,\n",
        "    \"recency_weighting\": True,\n",
        "    \"half_life_days\": HALF_LIFE_DAYS,\n",
        "    \"seg_len\": SEG_LEN,\n",
        "    \"n_segments\": N_SEGMENTS,\n",
        "}, indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3ekvERhym7N",
        "outputId": "d5ddf66f-ba75-4211-caab-20ebfe0ff612"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"status\": \"2.2 trained\",\n",
            "  \"chosen_k\": 2,\n",
            "  \"fit_mode\": \"recency\",\n",
            "  \"train_score\": -8178.605247677998,\n",
            "  \"n_iter\": 200,\n",
            "  \"n_init\": 2,\n",
            "  \"sticky_lambda\": 0.15,\n",
            "  \"recency_weighting\": true,\n",
            "  \"half_life_days\": 756,\n",
            "  \"seg_len\": 60,\n",
            "  \"n_segments\": 80\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick peek at effective sampling weights (fixed timedelta math)\n",
        "ends = np.arange(SEG_LEN - 1, len(dates_train))\n",
        "end_dates = pd.to_datetime(dates_train[ends])\n",
        "\n",
        "pp = _build_time_decay_weights(end_dates, HALF_LIFE_DAYS)\n",
        "pp = np.maximum(pp, EPSILON_FLOOR * pp.max())\n",
        "pp = pp / pp.sum()\n",
        "\n",
        "# Age in *days* relative to the most recent end_date\n",
        "max_date = end_dates.max()\n",
        "ages_days = (max_date - end_dates) / np.timedelta64(1, \"D\")  # float days\n",
        "\n",
        "# Weighted mean age (how \"old\" the typical sampled segment end is)\n",
        "w_mean_age_days = float(np.sum(ages_days * pp))\n",
        "\n",
        "# 95% weight age: age threshold below which 95% of weight lies\n",
        "order = np.argsort(ages_days)                  # youngest → oldest\n",
        "cumw = np.cumsum(pp[order])\n",
        "w95_idx = np.searchsorted(cumw, 0.95)\n",
        "w95_age_days = float(ages_days[order][min(w95_idx, len(ages_days)-1)])\n",
        "\n",
        "print({\n",
        "    \"weights_min\": float(pp.min()),\n",
        "    \"weights_max\": float(pp.max()),\n",
        "    \"weighted_mean_age_days\": w_mean_age_days,\n",
        "    \"weighted_p95_age_days\": w95_age_days,\n",
        "})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKJAsmdi85Ek",
        "outputId": "cc15d11a-6387-448f-846d-a046af0aaaf5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'weights_min': 0.0001335955016895934, 'weights_max': 0.001335955016895934, 'weighted_mean_age_days': 1017.4276078929489, 'weighted_p95_age_days': 2990.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Section 2.3 — State Labeling & Semantics\n",
        "# - Score posteriors for all dates\n",
        "# - Profile states on TRAIN window only (no peeking)\n",
        "# - Label states: Risk-On (↑ret, ↓vol), Risk-Off (↓ret, ↑vol), Transition (rest)\n",
        "# - Persist labels, posteriors, and profiles\n",
        "# Outputs:\n",
        "#   artifacts/regimes/regime_labels.parquet (date, state_id, p0..pK, regime_label)\n",
        "#   artifacts/regimes/state_profiles.csv\n",
        "#   artifacts/regimes/regime_meta.json (updated label map)\n",
        "# ============================================================\n",
        "\n",
        "from __future__ import annotations\n",
        "import os, json\n",
        "from typing import Dict, Any\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from datetime import datetime\n",
        "\n",
        "REGIME_DIR = CFG.regime_dir\n",
        "PANEL_PATH = os.path.join(REGIME_DIR, \"market_panel.parquet\")\n",
        "MANIFEST_PATH = os.path.join(REGIME_DIR, \"window_manifest.json\")\n",
        "BUNDLE_PATH = os.path.join(REGIME_DIR, \"hmm_best.pkl\")\n",
        "META_PATH = os.path.join(REGIME_DIR, \"regime_meta.json\")\n",
        "\n",
        "# Load artifacts\n",
        "mkt = pd.read_parquet(PANEL_PATH).sort_values(\"date\").reset_index(drop=True)\n",
        "mkt[\"date\"] = pd.to_datetime(mkt[\"date\"])\n",
        "with open(MANIFEST_PATH, \"r\") as f:\n",
        "    MAN = json.load(f)\n",
        "bundle = joblib.load(BUNDLE_PATH)\n",
        "model = bundle[\"model\"]\n",
        "features = bundle[\"features\"]\n",
        "scaler = joblib.load(bundle[\"scaler_path\"])\n",
        "\n",
        "# Prepare matrices for ALL dates (but label semantics computed on TRAIN ONLY)\n",
        "X_all = scaler.transform(mkt[features].to_numpy(dtype=float))\n",
        "dates_all = mkt[\"date\"].to_numpy()\n",
        "\n",
        "# Score posteriors\n",
        "post = model.predict_proba(X_all)  # shape: (T, K)\n",
        "states_argmax = post.argmax(axis=1)\n",
        "K = post.shape[1]\n",
        "\n",
        "# Train/test masks\n",
        "train_start = pd.to_datetime(MAN[\"window\"][\"train_start\"])\n",
        "train_end   = pd.to_datetime(MAN[\"window\"][\"train_end\"])\n",
        "test_start  = pd.to_datetime(MAN[\"window\"][\"test_start\"])\n",
        "test_end    = pd.to_datetime(MAN[\"window\"][\"test_end\"])\n",
        "train_mask = (mkt[\"date\"] >= train_start) & (mkt[\"date\"] <= train_end)\n",
        "test_mask  = (mkt[\"date\"] >= test_start)  & (mkt[\"date\"] <= test_end)\n",
        "\n",
        "# Helper: posterior-weighted stats on TRAIN window\n",
        "def weighted_mean(x, w):\n",
        "    w = np.asarray(w, dtype=float)\n",
        "    x = np.asarray(x, dtype=float)\n",
        "    s = w.sum()\n",
        "    return float((x * w).sum() / s) if s > 0 else np.nan\n",
        "\n",
        "def weighted_std(x, w):\n",
        "    mu = weighted_mean(x, w)\n",
        "    w = np.asarray(w, dtype=float)\n",
        "    x = np.asarray(x, dtype=float)\n",
        "    s = w.sum()\n",
        "    if s <= 1: return np.nan\n",
        "    var = ((w * (x - mu)**2).sum()) / s\n",
        "    return float(np.sqrt(max(var, 0.0)))\n",
        "\n",
        "def weighted_quantile(x, w, q=0.05):\n",
        "    x = np.asarray(x, dtype=float)\n",
        "    w = np.asarray(w, dtype=float)\n",
        "    order = np.argsort(x)\n",
        "    x_sorted, w_sorted = x[order], w[order]\n",
        "    cw = np.cumsum(w_sorted)\n",
        "    if cw[-1] == 0: return np.nan\n",
        "    return float(x_sorted[np.searchsorted(cw, q * cw[-1])])\n",
        "\n",
        "# Compute per-state profiles on TRAIN\n",
        "train_ix = np.where(train_mask.values)[0]\n",
        "has_dvix = \"dvix\" in mkt.columns\n",
        "profiles = []\n",
        "for s in range(K):\n",
        "    w = post[train_ix, s]\n",
        "    if w.sum() == 0:\n",
        "        mu_ret = mu_vol = mu_vix = mu_brd = q05 = np.nan\n",
        "        sd_ret = np.nan\n",
        "    else:\n",
        "        mu_ret = weighted_mean(mkt.loc[train_mask, \"spy_ret\"].values, w)\n",
        "        sd_ret = weighted_std(mkt.loc[train_mask, \"spy_ret\"].values, w)\n",
        "        mu_vol = weighted_mean(mkt.loc[train_mask, \"spy_rv_20\"].values, w)\n",
        "        mu_vix = weighted_mean(mkt.loc[train_mask, \"vix_close\"].values, w)\n",
        "        mu_brd = weighted_mean(mkt.loc[train_mask, \"breadth\"].values, w)\n",
        "        mu_dvix = weighted_mean(mkt.loc[train_mask, \"dvix\"].values, w) if has_dvix else np.nan\n",
        "        q05    = weighted_quantile(mkt.loc[train_mask, \"spy_ret\"].values, w, q=0.05)\n",
        "\n",
        "    profiles.append({\n",
        "        \"state_id\": s,\n",
        "        \"ret_mean\": mu_ret,\n",
        "        \"ret_std\": sd_ret,\n",
        "        \"rv20_mean\": mu_vol,\n",
        "        \"vix_mean\": mu_vix,\n",
        "        \"dvix_mean\": mu_dvix if has_dvix else np.nan,\n",
        "        \"breadth_mean\": mu_brd,\n",
        "        \"ret_q05\": q05,\n",
        "    })\n",
        "\n",
        "prof_df = pd.DataFrame(profiles)\n",
        "\n",
        "# Labeling rules (train window only, no peeking):\n",
        "#  - Risk-On: highest mean return, lowest vol (tie-breakers help if ambiguous)\n",
        "#  - Risk-Off: highest vol, lowest mean return (tie-breakers help if ambiguous)\n",
        "#  - Transition: whichever state is not assigned above\n",
        "# Primary ranks\n",
        "ret_rank = prof_df[\"ret_mean\"].rank(method=\"dense\")                        # higher = better\n",
        "# For clarity: choose Risk-Off by *highest* rv20 (vol spike)\n",
        "risk_off_id = int(prof_df[\"rv20_mean\"].idxmax())                           # highest vol\n",
        "risk_on_id  = int(ret_rank.idxmax())                                       # highest return\n",
        "\n",
        "# Tie-breaker refinement (only if they collide or look ambiguous)\n",
        "# Risk-On tie-breakers: breadth↑, VIX↓, tail q05↑\n",
        "# Risk-Off tie-breakers: vol↑, ret↓, ΔVIX↑, breadth↓\n",
        "def _best_risk_on_row(df: pd.DataFrame) -> int:\n",
        "    score = (\n",
        "        df[\"breadth_mean\"].fillna(-1.0).rank(method=\"dense\", ascending=False) +\n",
        "        df[\"vix_mean\"].fillna(np.inf).rank(method=\"dense\", ascending=True) +\n",
        "        df[\"ret_q05\"].fillna(-np.inf).rank(method=\"dense\", ascending=False)\n",
        "    )\n",
        "    return int(score.idxmax())\n",
        "\n",
        "def _best_risk_off_row(df: pd.DataFrame) -> int:\n",
        "    score = (\n",
        "        df[\"rv20_mean\"].fillna(-np.inf).rank(method=\"dense\", ascending=False) +\n",
        "        df[\"ret_mean\"].fillna(np.inf).rank(method=\"dense\", ascending=True) +\n",
        "        (df[\"dvix_mean\"] if \"dvix_mean\" in df.columns else pd.Series(0.0, index=df.index)).fillna(0.0).rank(method=\"dense\", ascending=False) +\n",
        "        df[\"breadth_mean\"].fillna(np.inf).rank(method=\"dense\", ascending=True)\n",
        "    )\n",
        "    return int(score.idxmax())\n",
        "\n",
        "if risk_on_id == risk_off_id:\n",
        "    risk_on_id  = _best_risk_on_row(prof_df)\n",
        "    risk_off_id = _best_risk_off_row(prof_df)\n",
        "    # Safety: if still colliding (extremely rare), force Risk-Off = highest vol, Risk-On = highest return\n",
        "    if risk_on_id == risk_off_id:\n",
        "        risk_off_id = int(prof_df[\"rv20_mean\"].idxmax())\n",
        "        risk_on_id  = int(prof_df[\"ret_mean\"].idxmax())\n",
        "\n",
        "# Final label map\n",
        "label_map = {risk_on_id: \"Risk-On\", risk_off_id: \"Risk-Off\"}\n",
        "for s in range(K):\n",
        "    if s not in label_map:\n",
        "        label_map[s] = \"Transition\"\n",
        "\n",
        "# ---------- Build outputs USING the FINAL label_map ----------\n",
        "out = mkt[[\"date\"]].copy()\n",
        "out[\"state_id\"] = post.argmax(axis=1)\n",
        "for s in range(K):\n",
        "    out[f\"p{s}\"] = post[:, s]\n",
        "out[\"regime_label\"] = out[\"state_id\"].map(label_map)\n",
        "\n",
        "out_path = os.path.join(REGIME_DIR, \"regime_labels.parquet\")\n",
        "out.to_parquet(out_path, index=False)\n",
        "out.to_csv(os.path.join(REGIME_DIR, \"regime_labels.csv\"), index=False)\n",
        "\n",
        "prof_df.to_csv(os.path.join(REGIME_DIR, \"state_profiles.csv\"), index=False)\n",
        "\n",
        "# ---------- Update meta WITH the FINAL label_map ----------\n",
        "if os.path.exists(META_PATH):\n",
        "    with open(META_PATH, \"r\") as f:\n",
        "        meta = json.load(f)\n",
        "else:\n",
        "    meta = {}\n",
        "\n",
        "meta.setdefault(\"created_at\", datetime.utcnow().isoformat() + \"Z\")\n",
        "meta.setdefault(\"config\", {})\n",
        "meta.setdefault(\"diagnostics\", {})\n",
        "meta[\"diagnostics\"][\"state_profiles_train\"] = prof_df.to_dict(orient=\"records\")\n",
        "meta[\"state_label_map\"] = {int(k): v for k, v in label_map.items()}\n",
        "meta.setdefault(\"features_used\", features)\n",
        "meta[\"notes\"] = meta.get(\"notes\", []) + [\n",
        "    \"State labeling computed on train window only (no peeking).\",\n",
        "    \"Risk-On: highest mean ret & lowest vol; Risk-Off: highest vol & lowest ret; else Transition.\",\n",
        "    \"Tie-breakers: breadth↑, VIX↓, tail q05↑ (Risk-On); vol↑, ret↓, ΔVIX↑, breadth↓ (Risk-Off).\",\n",
        "]\n",
        "\n",
        "with open(META_PATH, \"w\") as f:\n",
        "    json.dump(meta, f, indent=2)\n",
        "\n",
        "print(json.dumps({\n",
        "    \"status\": \"2.3 labeled\",\n",
        "    \"k\": K,\n",
        "    \"label_map\": label_map,\n",
        "    \"profiles_path\": os.path.join(REGIME_DIR, \"state_profiles.csv\"),\n",
        "    \"labels_path\": out_path,\n",
        "}, indent=2))\n",
        "\n",
        "# Posteriors sanity\n",
        "assert np.allclose(post.sum(axis=1), 1.0, atol=1e-6), \"Posterior rows must sum to 1.\"\n",
        "assert not pd.isna(out[\"regime_label\"]).any(), \"All states must map to a regime label.\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1sBNFT58pPr",
        "outputId": "6963edf2-638c-45d7-d648-8f49213250bd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"status\": \"2.3 labeled\",\n",
            "  \"k\": 2,\n",
            "  \"label_map\": {\n",
            "    \"0\": \"Risk-On\",\n",
            "    \"1\": \"Risk-Off\"\n",
            "  },\n",
            "  \"profiles_path\": \"artifacts/regimes/state_profiles.csv\",\n",
            "  \"labels_path\": \"artifacts/regimes/regime_labels.parquet\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Section 2.4 — Smoothing, Persistence & Debounce\n",
        "# - Option: Viterbi most-likely path vs. posterior argmax\n",
        "# - Debounce: POSTERIOR_THRESH and MIN_DWELL_DAYS from config\n",
        "# - Gap handling: inherit last known regime (dates already market-days)\n",
        "# Reuses:\n",
        "#   - artifacts/regimes/market_panel.parquet (2.0)\n",
        "#   - artifacts/regimes/window_manifest.json (2.1)\n",
        "#   - artifacts/regimes/hmm_best.pkl (2.2)\n",
        "#   - artifacts/regimes/regime_labels.parquet (2.3)\n",
        "# Outputs:\n",
        "#   - artifacts/regimes/regime_labels.parquet (updated with *_smoothed cols)\n",
        "#   - artifacts/regimes/regime_meta.json (updated diagnostics)\n",
        "#   - console summary of dwell-time stats\n",
        "# ============================================================\n",
        "\n",
        "from __future__ import annotations\n",
        "import os, json\n",
        "from typing import List, Dict, Any, Tuple\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "REGIME_DIR     = CFG.regime_dir\n",
        "PANEL_PATH     = os.path.join(REGIME_DIR, \"market_panel.parquet\")\n",
        "LABELS_PATH    = os.path.join(REGIME_DIR, \"regime_labels.parquet\")\n",
        "META_PATH      = os.path.join(REGIME_DIR, \"regime_meta.json\")\n",
        "BUNDLE_PATH    = os.path.join(REGIME_DIR, \"hmm_best.pkl\")\n",
        "\n",
        "assert os.path.exists(PANEL_PATH),  f\"Missing market panel: {PANEL_PATH}\"\n",
        "assert os.path.exists(LABELS_PATH), f\"Missing labels from 2.3: {LABELS_PATH}\"\n",
        "assert os.path.exists(BUNDLE_PATH), f\"Missing HMM bundle: {BUNDLE_PATH}\"\n",
        "\n",
        "# --- Config knobs (extend 2.0 config if not present) ---\n",
        "P_THRESH   = getattr(CFG, \"posterior_thresh\", 0.55) # TOCHANGE: consider 0.60–0.65 for a stricter switch confirmation.\n",
        "MIN_DWELL  = getattr(CFG, \"min_dwell_days\", 3) #  # TOCHANGE: consider 5–10 to further reduce chattering.\n",
        "SMOOTH_MTH = getattr(CFG, \"smoothing_method\", \"posterior\")  # \"posterior\" | \"viterbi\" # TOCHANGE: try \"viterbi\" for the real run and compare dwell-time stats and chattering.\n",
        "\n",
        "# --- Load artifacts ---\n",
        "labels = pd.read_parquet(LABELS_PATH).sort_values(\"date\").reset_index(drop=True)\n",
        "bundle = joblib.load(BUNDLE_PATH)\n",
        "model  = bundle[\"model\"]\n",
        "features = bundle[\"features\"]\n",
        "scaler  = joblib.load(bundle[\"scaler_path\"])\n",
        "\n",
        "mkt = pd.read_parquet(PANEL_PATH).sort_values(\"date\").reset_index(drop=True)\n",
        "mkt[\"date\"] = pd.to_datetime(mkt[\"date\"])\n",
        "\n",
        "# sanity\n",
        "assert np.array_equal(labels[\"date\"].values, mkt[\"date\"].values), \"Date alignment mismatch between labels and market panel.\"\n",
        "\n",
        "# --- Choose base path: Viterbi vs posterior argmax ---\n",
        "# We need posteriors for thresholding either way; for Viterbi we re-score X_all.\n",
        "X_all = scaler.transform(mkt[features].to_numpy(dtype=float))\n",
        "post  = model.predict_proba(X_all)  # (T, K)\n",
        "K     = post.shape[1]\n",
        "\n",
        "if SMOOTH_MTH.lower() == \"viterbi\":\n",
        "    base_states = model.predict(X_all)     # most-likely state path\n",
        "else:\n",
        "    base_states = post.argmax(axis=1)      # raw posterior argmax (already in 2.3)\n",
        "\n",
        "# --- Debounce step 1: posterior threshold gating (no switch if low confidence) ---\n",
        "maxp = post.max(axis=1)\n",
        "debounce_states = np.array(base_states, dtype=int)\n",
        "for i in range(1, len(debounce_states)):\n",
        "    if debounce_states[i] != debounce_states[i-1]:\n",
        "        # require sufficient posterior confidence on the *new* state\n",
        "        if maxp[i] < P_THRESH:\n",
        "            debounce_states[i] = debounce_states[i-1]\n",
        "\n",
        "# --- Debounce step 2: enforce minimum dwell time by collapsing short runs ---\n",
        "def _runs(state_series: np.ndarray) -> List[Tuple[int,int,int]]:\n",
        "    \"\"\"Return list of (start_idx, end_idx_inclusive, state) runs.\"\"\"\n",
        "    out = []\n",
        "    s = 0\n",
        "    cur = state_series[0]\n",
        "    for i in range(1, len(state_series)):\n",
        "        if state_series[i] != cur:\n",
        "            out.append((s, i-1, cur))\n",
        "            s = i\n",
        "            cur = state_series[i]\n",
        "    out.append((s, len(state_series)-1, cur))\n",
        "    return out\n",
        "\n",
        "def _collapse_short_runs(states: np.ndarray, min_len: int, post: np.ndarray) -> np.ndarray:\n",
        "    arr = states.copy()\n",
        "    changed = True\n",
        "    # iterate until stable (collapsing can merge adjacent runs)\n",
        "    while changed:\n",
        "        changed = False\n",
        "        runs = _runs(arr)\n",
        "        for (s, e, st) in runs:\n",
        "            run_len = e - s + 1\n",
        "            if run_len < min_len:\n",
        "                # Candidate neighbors: previous and next, choose higher avg posterior over this segment\n",
        "                prev_state = runs[runs.index((s, e, st))-1][2] if runs.index((s, e, st)) > 0 else None\n",
        "                next_state = runs[runs.index((s, e, st))+1][2] if runs.index((s, e, st)) < len(runs)-1 else None\n",
        "\n",
        "                # If no neighbors (degenerate), skip\n",
        "                if prev_state is None and next_state is None:\n",
        "                    continue\n",
        "\n",
        "                # Compute average posterior for neighbors over the short segment\n",
        "                best_neighbor = None\n",
        "                best_score = -np.inf\n",
        "                for cand in [prev_state, next_state]:\n",
        "                    if cand is None:\n",
        "                        continue\n",
        "                    score = float(post[s:e+1, cand].mean())\n",
        "                    if score > best_score:\n",
        "                        best_score = score\n",
        "                        best_neighbor = cand\n",
        "                # Relabel the short run to best neighbor\n",
        "                arr[s:e+1] = best_neighbor\n",
        "                changed = True\n",
        "                break  # restart since runs have changed\n",
        "    return arr\n",
        "\n",
        "smoothed_states = _collapse_short_runs(debounce_states, MIN_DWELL, post)\n",
        "\n",
        "# --- Map to labels using the semantics from 2.3 (state_label_map) ---\n",
        "# read label map from meta\n",
        "if os.path.exists(META_PATH):\n",
        "    with open(META_PATH, \"r\") as f:\n",
        "        meta = json.load(f)\n",
        "else:\n",
        "    meta = {}\n",
        "\n",
        "state_label_map = meta.get(\"state_label_map\", None)\n",
        "if state_label_map is None:\n",
        "    # fallback to identity names if meta missing (shouldn't happen)\n",
        "    state_label_map = {int(s): f\"State{s}\" for s in range(K)}\n",
        "\n",
        "# Update labels DataFrame with smoothed outputs\n",
        "labels[\"state_id_smoothed\"] = smoothed_states\n",
        "for s in range(K):\n",
        "    # keep original p0..pK as-is from 2.3; they reflect the raw model posteriors\n",
        "    if f\"p{s}\" not in labels.columns:\n",
        "        labels[f\"p{s}\"] = post[:, s]\n",
        "\n",
        "labels[\"regime_label_smoothed\"] = labels[\"state_id_smoothed\"].map({int(k): v for k, v in state_label_map.items()})\n",
        "\n",
        "# --- Dwell-time diagnostics ---\n",
        "def _dwell_stats(states: np.ndarray) -> pd.DataFrame:\n",
        "    rr = _runs(states)\n",
        "    return pd.DataFrame({\n",
        "        \"state_id\": [st for (_,_,st) in rr],\n",
        "        \"run_len\":  [e - s + 1 for (s,e,_) in rr],\n",
        "    }).groupby(\"state_id\").agg(\n",
        "        median_run_length=(\"run_len\", \"median\"),\n",
        "        mean_run_length  =(\"run_len\", \"mean\"),\n",
        "        n_runs           =(\"run_len\", \"count\"),\n",
        "        max_run_length   =(\"run_len\", \"max\"),\n",
        "    ).reset_index()\n",
        "\n",
        "dwell_df = _dwell_stats(labels[\"state_id_smoothed\"].to_numpy())\n",
        "\n",
        "# --- Save updated labels back to disk ---\n",
        "labels.to_parquet(LABELS_PATH, index=False)\n",
        "labels.to_csv(LABELS_PATH.replace(\".parquet\", \".csv\"), index=False)\n",
        "\n",
        "# --- Update regime_meta.json diagnostics & config snapshot ---\n",
        "meta.setdefault(\"diagnostics\", {})\n",
        "meta[\"diagnostics\"][\"smoothing\"] = {\n",
        "    \"method\": SMOOTH_MTH,\n",
        "    \"posterior_thresh\": P_THRESH,\n",
        "    \"min_dwell_days\": MIN_DWELL,\n",
        "    \"dwell_stats\": dwell_df.to_dict(orient=\"records\"),\n",
        "}\n",
        "meta.setdefault(\"notes\", [])\n",
        "meta[\"notes\"] += [\n",
        "    \"2.4 smoothing applied with debounce (posterior threshold + min dwell).\",\n",
        "    \"If method='viterbi', base path is Viterbi; else posterior argmax.\",\n",
        "]\n",
        "# de-dup notes\n",
        "meta[\"notes\"] = list(dict.fromkeys(meta[\"notes\"]))\n",
        "\n",
        "with open(META_PATH, \"w\") as f:\n",
        "    json.dump(meta, f, indent=2)\n",
        "\n",
        "print(json.dumps({\n",
        "    \"status\": \"2.4 smoothed\",\n",
        "    \"method\": SMOOTH_MTH,\n",
        "    \"posterior_thresh\": P_THRESH,\n",
        "    \"min_dwell_days\": MIN_DWELL,\n",
        "    \"k\": K,\n",
        "    \"median_dwell_by_state\": {\n",
        "        int(r[\"state_id\"]): float(r[\"median_run_length\"]) for r in dwell_df.to_dict(orient=\"records\")\n",
        "    },\n",
        "    \"labels_path\": LABELS_PATH\n",
        "}, indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eo9ZowDyKJ-f",
        "outputId": "9baa87bd-667b-49ba-d227-3ee92fe2822d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"status\": \"2.4 smoothed\",\n",
            "  \"method\": \"posterior\",\n",
            "  \"posterior_thresh\": 0.55,\n",
            "  \"min_dwell_days\": 3,\n",
            "  \"k\": 2,\n",
            "  \"median_dwell_by_state\": {\n",
            "    \"0\": 33.5,\n",
            "    \"1\": 12.0\n",
            "  },\n",
            "  \"labels_path\": \"artifacts/regimes/regime_labels.parquet\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Section 2.5 — Robustness & Sensitivity\n",
        "# - K sensitivity: K ∈ {2,3}\n",
        "# - Feature sensitivity: drop-one/add-one variants\n",
        "# - Era stability: pre/post-2015 and 2020 crisis\n",
        "# - Bootstrap: block bootstrap label stability\n",
        "# Reuses:\n",
        "#   - artifacts/regimes/market_panel.parquet (2.0)\n",
        "#   - artifacts/regimes/window_manifest.json (2.1)\n",
        "#   - scaler_*.joblib (2.1)\n",
        "#   - artifacts/regimes/hmm_best.pkl (2.2 baseline)\n",
        "#   - artifacts/regimes/regime_labels.parquet (2.3 baseline labels)\n",
        "# Outputs:\n",
        "#   - artifacts/regimes/regime_sensitivity.json\n",
        "# Notes:\n",
        "#   This is a light test pass; heavier settings are tagged with # TOCHANGE\n",
        "# ============================================================\n",
        "\n",
        "from __future__ import annotations\n",
        "import os, json\n",
        "from typing import Dict, Any, List, Tuple\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from hmmlearn.hmm import GaussianHMM\n",
        "\n",
        "REGIME_DIR  = CFG.regime_dir\n",
        "PANEL_PATH  = os.path.join(REGIME_DIR, \"market_panel.parquet\")\n",
        "MAN_PATH    = os.path.join(REGIME_DIR, \"window_manifest.json\")\n",
        "BUNDLE_PATH = os.path.join(REGIME_DIR, \"hmm_best.pkl\")\n",
        "LABELS_PATH = os.path.join(REGIME_DIR, \"regime_labels.parquet\")\n",
        "OUT_PATH    = os.path.join(REGIME_DIR, \"regime_sensitivity.json\")\n",
        "\n",
        "assert os.path.exists(PANEL_PATH) and os.path.exists(MAN_PATH) and os.path.exists(BUNDLE_PATH)\n",
        "mkt = pd.read_parquet(PANEL_PATH).sort_values(\"date\").reset_index(drop=True)\n",
        "mkt[\"date\"] = pd.to_datetime(mkt[\"date\"])\n",
        "\n",
        "with open(MAN_PATH, \"r\") as f:\n",
        "    MAN = json.load(f)\n",
        "\n",
        "bundle   = joblib.load(BUNDLE_PATH)\n",
        "features_base = bundle[\"features\"]\n",
        "scaler   = joblib.load(bundle[\"scaler_path\"])\n",
        "k_base   = int(bundle[\"k\"])\n",
        "recency  = bool(bundle.get(\"recency_weighting\", True))\n",
        "hl_days  = int(bundle.get(\"recency_half_life_days\", 756))\n",
        "seg_len  = int(bundle.get(\"recency_seg_len\", 60))\n",
        "n_segs   = int(bundle.get(\"recency_n_segments\", 80))\n",
        "tol      = float(bundle.get(\"tol\", 1e-3))\n",
        "n_iter   = int(bundle.get(\"n_iter\", 200))         # TOCHANGE: 1000 for real run\n",
        "n_init   = int(bundle.get(\"n_init\", 2))           # TOCHANGE: 10 for real run\n",
        "covtype  = bundle.get(\"covariance_type\", \"full\")\n",
        "lam_stick= float(bundle.get(\"sticky_lambda\", 0.15))  # TOCHANGE: 0.30–0.50 for real run\n",
        "rand0    = int(bundle.get(\"random_state\", 42))\n",
        "\n",
        "train_start = pd.to_datetime(MAN[\"window\"][\"train_start\"])\n",
        "train_end   = pd.to_datetime(MAN[\"window\"][\"train_end\"])\n",
        "test_start  = pd.to_datetime(MAN[\"window\"][\"test_start\"])\n",
        "test_end    = pd.to_datetime(MAN[\"window\"][\"test_end\"])\n",
        "\n",
        "mask_train = (mkt[\"date\"] >= train_start) & (mkt[\"date\"] <= train_end)\n",
        "mask_test  = (mkt[\"date\"] >= test_start)  & (mkt[\"date\"] <= test_end)\n",
        "\n",
        "# ⬇️ ADD: tiny helper to fit a local scaler on the train (or era) subset\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def _fit_local_scaler(feats: List[str], subset_mask: pd.Series) -> StandardScaler:\n",
        "    df = mkt.loc[subset_mask, feats].dropna()\n",
        "    scaler_local = StandardScaler()\n",
        "    scaler_local.fit(df.to_numpy(dtype=float))\n",
        "    return scaler_local\n",
        "\n",
        "def _diag_sticky_blend(T: np.ndarray, lam: float) -> np.ndarray:\n",
        "    k = T.shape[0]\n",
        "    out = (1.0 - lam) * T + lam * np.eye(k)\n",
        "    return out / out.sum(axis=1, keepdims=True)\n",
        "\n",
        "def _build_time_decay_weights(dates: np.ndarray, half_life_days: int) -> np.ndarray:\n",
        "    t = np.array([pd.Timestamp(d).toordinal() for d in dates], dtype=float)\n",
        "    age = (t.max() - t)\n",
        "    decay = np.log(2) / max(1, half_life_days)\n",
        "    w = np.exp(-decay * age)\n",
        "    return w / (w.sum() + 1e-12)\n",
        "\n",
        "# Canonical recency-sampling params (align with 2.2 bundle keys)\n",
        "REC_SEG_LEN    = int(bundle.get(\"recency_seg_len\", 60))\n",
        "REC_N_SEGMENTS = int(bundle.get(\"recency_n_segments\", 80))\n",
        "REC_HALF_LIFE  = int(bundle.get(\"recency_half_life_days\", 756))\n",
        "REC_EPS        = float(bundle.get(\"recency_epsilon_floor\", 0.10))  # matches 2.2 key\n",
        "\n",
        "def _sample_time_weighted_subsequences(\n",
        "    X: np.ndarray,\n",
        "    dates: np.ndarray,\n",
        "    seg_len: int = REC_SEG_LEN,\n",
        "    n_segments: int = REC_N_SEGMENTS,\n",
        "    half_life_days: int = REC_HALF_LIFE,\n",
        "    seed: int = 42,\n",
        "):\n",
        "    rng = np.random.RandomState(seed)\n",
        "    n = X.shape[0]\n",
        "    if n < seg_len:\n",
        "        return X.copy(), [n]\n",
        "    ends = np.arange(seg_len - 1, n)\n",
        "    p = _build_time_decay_weights(dates[ends], half_life_days)\n",
        "    p = np.maximum(p, REC_EPS * p.max())\n",
        "    p = p / p.sum()\n",
        "    chosen = rng.choice(ends, size=min(n_segments, len(ends)), replace=True, p=p)\n",
        "    chunks, lengths = [], []\n",
        "    for e in chosen:\n",
        "        s = e - (seg_len - 1)\n",
        "        chunks.append(X[s:e+1])\n",
        "        lengths.append(seg_len)\n",
        "    return np.vstack(chunks), lengths\n",
        "\n",
        "# ⬇️ MODIFY: _fit_hmm_for_features now fits and returns a local scaler,\n",
        "# and uses it for both training and scoring.\n",
        "def _fit_hmm_for_features(feats: List[str], k: int, rs: int, subset_mask: pd.Series) -> Dict[str, Any]:\n",
        "    # Fit local scaler on the subset (train or era) to avoid feature-count mismatch\n",
        "    scaler_local = _fit_local_scaler(feats, subset_mask)\n",
        "\n",
        "    df = mkt.loc[subset_mask, [\"date\"] + feats].dropna().reset_index(drop=True)\n",
        "    dates = df[\"date\"].to_numpy()\n",
        "    X = scaler_local.transform(df[feats].to_numpy(dtype=float))\n",
        "\n",
        "    if recency:\n",
        "        X_fit, lengths = _sample_time_weighted_subsequences(\n",
        "            X, dates, seg_len=seg_len, n_segments=n_segs, half_life_days=hl_days, seed=rs\n",
        "        )\n",
        "    else:\n",
        "        X_fit, lengths = X, [len(X)]\n",
        "\n",
        "    model = GaussianHMM(\n",
        "        n_components=k,\n",
        "        covariance_type=covtype,\n",
        "        n_iter=n_iter,\n",
        "        tol=tol,\n",
        "        random_state=rs,\n",
        "        verbose=False,\n",
        "        init_params=\"mc\",   # means, covars\n",
        "        params=\"stmc\",      # learn startprob/transmat as well\n",
        "    )\n",
        "    # sticky-ish init\n",
        "    T0 = np.full((k, k), (1.0 - 0.90) / max(1, k - 1)); np.fill_diagonal(T0, 0.90)\n",
        "    model.transmat_ = T0\n",
        "    model.startprob_ = np.full(k, 1.0 / k)\n",
        "\n",
        "    model.fit(X_fit, lengths=lengths)\n",
        "    model.transmat_ = _diag_sticky_blend(model.transmat_, lam_stick)\n",
        "\n",
        "    # score on original (non-sampled) sequence\n",
        "    score = float(model.score(X))\n",
        "\n",
        "    return {\n",
        "        \"model\": model,\n",
        "        \"score\": score,\n",
        "        \"dates\": dates,\n",
        "        \"feats\": feats,\n",
        "        \"scaler\": scaler_local  # ⬅️ return it\n",
        "    }\n",
        "\n",
        "# ⬇️ MODIFY: _profile_and_label takes the local scaler we fit above\n",
        "def _profile_and_label(model, feats: List[str], scaler_local: StandardScaler) -> Dict[str, Any]:\n",
        "    X_all = scaler_local.transform(mkt[feats].to_numpy(dtype=float))\n",
        "    post  = model.predict_proba(X_all)\n",
        "    K = post.shape[1]\n",
        "\n",
        "    # compute profiles on TRAIN ONLY (no peeking)\n",
        "    train_ix = np.where(mask_train.values)[0]\n",
        "\n",
        "    def wmean(x, w):\n",
        "        w = np.asarray(w); x = np.asarray(x); s = w.sum()\n",
        "        return float((x*w).sum()/s) if s>0 else np.nan\n",
        "    def wstd(x, w):\n",
        "        mu = wmean(x, w); w = np.asarray(w); x = np.asarray(x); s=w.sum()\n",
        "        if s<=1: return np.nan\n",
        "        return float(np.sqrt(max(((w*(x-mu)**2).sum()/s), 0.0)))\n",
        "    def wq05(x, w):\n",
        "        x=np.asarray(x); w=np.asarray(w); o=np.argsort(x); xs,ws=x[o],w[o]; cw=np.cumsum(ws)\n",
        "        return float(xs[np.searchsorted(cw, 0.05*cw[-1])]) if cw[-1]>0 else np.nan\n",
        "\n",
        "    prof = []\n",
        "    for s in range(K):\n",
        "        w = post[train_ix, s]\n",
        "        prof.append({\n",
        "            \"state_id\": s,\n",
        "            \"ret_mean\": wmean(mkt.loc[mask_train,\"spy_ret\"].values, w),\n",
        "            \"ret_std\":  wstd (mkt.loc[mask_train,\"spy_ret\"].values, w),\n",
        "            \"rv20_mean\": wmean(mkt.loc[mask_train,\"spy_rv_20\"].values, w),\n",
        "            \"vix_mean\":  wmean(mkt.loc[mask_train,\"vix_close\"].values, w),\n",
        "            \"dvix_mean\": wmean(mkt.loc[mask_train,\"dvix\"].values, w) if \"dvix\" in mkt.columns else np.nan,\n",
        "            \"breadth_mean\": wmean(mkt.loc[mask_train,\"breadth\"].values, w),\n",
        "            \"ret_q05\":  wq05 (mkt.loc[mask_train,\"spy_ret\"].values, w),\n",
        "        })\n",
        "    prof_df = pd.DataFrame(prof)\n",
        "\n",
        "    risk_off_id = int(prof_df[\"rv20_mean\"].idxmax())\n",
        "    risk_on_id  = int(prof_df[\"ret_mean\"].idxmax())\n",
        "    label_map = {risk_on_id: \"Risk-On\", risk_off_id: \"Risk-Off\"}\n",
        "    for s in range(K):\n",
        "        if s not in label_map:\n",
        "            label_map[s] = \"Transition\"\n",
        "\n",
        "    return {\n",
        "        \"profiles\": prof_df.to_dict(orient=\"records\"),\n",
        "        \"label_map\": {int(k): v for k,v in label_map.items()},\n",
        "        \"posteriors_shape\": list(post.shape),\n",
        "        \"transmat\": model.transmat_.tolist(),\n",
        "    }\n",
        "\n",
        "def _agreement_vs_baseline(new_states: np.ndarray, baseline_states: np.ndarray) -> float:\n",
        "    # simple percent agreement\n",
        "    if len(new_states) != len(baseline_states):\n",
        "        return np.nan\n",
        "    return float((new_states == baseline_states).mean())\n",
        "\n",
        "# --- Load baseline label sequence (we'll compare to *smoothed* if present) ---\n",
        "base_labels = pd.read_parquet(LABELS_PATH).sort_values(\"date\")\n",
        "base_labels[\"date\"] = pd.to_datetime(base_labels[\"date\"])  # <- add this\n",
        "\n",
        "if \"state_id_smoothed\" in base_labels.columns:\n",
        "    base_states = base_labels[\"state_id_smoothed\"].to_numpy()\n",
        "else:\n",
        "    base_states = base_labels[\"state_id\"].to_numpy()\n",
        "\n",
        "results: Dict[str, Any] = {\"k_sensitivity\": [], \"feature_sensitivity\": [], \"era_stability\": [], \"bootstrap\": {}}\n",
        "\n",
        "def _score_states_on_valid_dates(model, feats, scaler_local):\n",
        "    full_df = mkt[[\"date\"] + feats].dropna().reset_index(drop=True)\n",
        "    Xa = scaler_local.transform(full_df[feats].to_numpy(dtype=float))\n",
        "    states = model.predict_proba(Xa).argmax(axis=1)\n",
        "    return full_df[\"date\"].to_numpy(), states\n",
        "\n",
        "def _agreement_on_intersection(dates_new, states_new, base_labels_df) -> float:\n",
        "    df_new = pd.DataFrame({\"date\": dates_new, \"state_new\": states_new})\n",
        "    df_join = df_new.merge(\n",
        "        base_labels_df[[\"date\", \"state_id_smoothed\" if \"state_id_smoothed\" in base_labels_df.columns else \"state_id\"]]\n",
        "        .rename(columns={\"state_id_smoothed\":\"state_base\",\"state_id\":\"state_base\"}),\n",
        "        on=\"date\", how=\"inner\"\n",
        "    )\n",
        "    if len(df_join) == 0:\n",
        "        return np.nan\n",
        "    return float((df_join[\"state_new\"].to_numpy() == df_join[\"state_base\"].to_numpy()).mean())\n",
        "\n",
        "# 1) K sensitivity ------------------------------------------------------------\n",
        "K_GRID = [2, 3]  # TOCHANGE: can expand to [2,3] in real run if currently narrowed\n",
        "for k in K_GRID:\n",
        "    best = {\"score\": -np.inf, \"meta\": None}\n",
        "    for r in range(n_init):\n",
        "        rs = rand0 + r\n",
        "        fit = _fit_hmm_for_features(features_base, k, rs, mask_train)\n",
        "        meta = _profile_and_label(fit[\"model\"], features_base, fit[\"scaler\"])\n",
        "        dates_scored, states_all = _score_states_on_valid_dates(fit[\"model\"], features_base, fit[\"scaler\"])\n",
        "        agree = _agreement_on_intersection(dates_scored, states_all, base_labels)\n",
        "\n",
        "        entry = {\n",
        "            \"k\": k, \"seed\": rs, \"score\": fit[\"score\"], \"agreement_vs_baseline\": agree,\n",
        "            \"profiles\": meta[\"profiles\"], \"label_map\": meta[\"label_map\"],\n",
        "            \"transmat\": meta[\"transmat\"]\n",
        "        }\n",
        "        if fit[\"score\"] > best[\"score\"]:\n",
        "            best = {\"score\": fit[\"score\"], \"meta\": entry}\n",
        "    results[\"k_sensitivity\"].append(best[\"meta\"])\n",
        "\n",
        "# 2) Feature sensitivity -------------------------------------------------------\n",
        "# Define variants relative to baseline features\n",
        "fsets = []\n",
        "fsets.append((\"baseline\", features_base))\n",
        "if \"vix_close\" in features_base: fsets.append((\"no_vix\", [f for f in features_base if f!=\"vix_close\"]))\n",
        "if \"breadth\"   in features_base: fsets.append((\"no_breadth\", [f for f in features_base if f!=\"breadth\"]))\n",
        "if \"dvix\"      in features_base: fsets.append((\"no_dvix\", [f for f in features_base if f!=\"dvix\"]))\n",
        "# minimal core set\n",
        "core = [f for f in [\"spy_rv_20\",\"vix_close\"] if f in mkt.columns]\n",
        "if core: fsets.append((\"core_rv_vix\", core))\n",
        "\n",
        "for name, feats in fsets:\n",
        "    k = k_base\n",
        "    best = {\"score\": -np.inf, \"meta\": None}\n",
        "    for r in range(n_init):\n",
        "        rs = rand0 + 100 + r\n",
        "        fit = _fit_hmm_for_features(feats, k, rs, mask_train)\n",
        "        meta = _profile_and_label(fit[\"model\"], feats, fit[\"scaler\"])\n",
        "        dates_scored, states_all = _score_states_on_valid_dates(fit[\"model\"], feats, fit[\"scaler\"])\n",
        "        agree = _agreement_on_intersection(dates_scored, states_all, base_labels)\n",
        "        entry = {\n",
        "            \"feature_set\": name, \"k\": k, \"seed\": rs, \"feats\": feats,\n",
        "            \"score\": fit[\"score\"], \"agreement_vs_baseline\": agree,\n",
        "            \"profiles\": meta[\"profiles\"], \"label_map\": meta[\"label_map\"],\n",
        "            \"transmat\": meta[\"transmat\"]\n",
        "        }\n",
        "        if fit[\"score\"] > best[\"score\"]:\n",
        "            best = {\"score\": fit[\"score\"], \"meta\": entry}\n",
        "    results[\"feature_sensitivity\"].append(best[\"meta\"])\n",
        "\n",
        "\n",
        "# 3) Era stability -------------------------------------------------------------\n",
        "def _fit_on_era(start: str, end: str, k: int, seed: int, feats: List[str], name: str) -> Dict[str, Any]:\n",
        "    mask = (mkt[\"date\"] >= pd.to_datetime(start)) & (mkt[\"date\"] <= pd.to_datetime(end))\n",
        "    fit  = _fit_hmm_for_features(feats, k, seed, mask)\n",
        "    meta = _profile_and_label(fit[\"model\"], feats, fit[\"scaler\"])\n",
        "    return {\n",
        "        \"era\": name, \"k\": k, \"seed\": seed, \"score\": fit[\"score\"],\n",
        "        \"profiles\": meta[\"profiles\"], \"label_map\": meta[\"label_map\"],\n",
        "        \"transmat\": meta[\"transmat\"], \"start\": str(start), \"end\": str(end)\n",
        "    }\n",
        "\n",
        "\n",
        "# ⬇️ OPTIONAL TIDY-UP — Bootstrap: use a local scaler for baseline features too\n",
        "df_tr = mkt.loc[mask_train, [\"date\"] + features_base].dropna().reset_index(drop=True)\n",
        "scaler_base_local = StandardScaler().fit(df_tr[features_base].to_numpy(dtype=float))  # local\n",
        "X_tr  = scaler_base_local.transform(df_tr[features_base].to_numpy(dtype=float))\n",
        "dt_tr = df_tr[\"date\"].to_numpy()\n",
        "\n",
        "eras = [\n",
        "    (\"pre_2015\",  \"2007-02-06\", \"2014-12-31\"),\n",
        "    (\"post_2015\", \"2015-01-01\", str(train_end.date())),\n",
        "    (\"crisis_2020\",\"2020-02-15\",\"2020-12-31\"),\n",
        "]\n",
        "for name, s, e in eras:\n",
        "    rs = rand0 + hash(name) % 1000\n",
        "    results[\"era_stability\"].append(_fit_on_era(s, e, k_base, rs, features_base, name))\n",
        "\n",
        "\n",
        "# 4) Bootstrap (block) ---------------------------------------------------------\n",
        "def _block_bootstrap_indices(n: int, block: int, n_blocks: int, rng: np.random.RandomState):\n",
        "    starts = rng.randint(0, n, size=n_blocks)\n",
        "    idx = []\n",
        "    for st in starts:\n",
        "        idx.extend([(st + j) % n for j in range(block)])\n",
        "    return np.array(idx[:n])\n",
        "\n",
        "BOOT_REPS   = 5    # TOCHANGE: 100–300 for real run\n",
        "BLOCK_DAYS  = 20   # TOCHANGE: 20–60 depending on serial corr\n",
        "rng = np.random.RandomState(rand0+999)\n",
        "\n",
        "# Build train matrix for bootstrap using local scaler\n",
        "df_tr = mkt.loc[mask_train, [\"date\"] + features_base].dropna().reset_index(drop=True)\n",
        "scaler_base_local = StandardScaler().fit(df_tr[features_base].to_numpy(dtype=float))\n",
        "X_tr  = scaler_base_local.transform(df_tr[features_base].to_numpy(dtype=float))\n",
        "dt_tr = df_tr[\"date\"].to_numpy()\n",
        "\n",
        "boot_summ = {\"k\": k_base, \"reps\": BOOT_REPS, \"block_days\": BLOCK_DAYS, \"agreement_vs_baseline\": []}\n",
        "for b in range(BOOT_REPS):\n",
        "    idx = _block_bootstrap_indices(len(dt_tr), BLOCK_DAYS, max(1, len(dt_tr)//BLOCK_DAYS), rng)\n",
        "    Xb  = X_tr[idx]\n",
        "\n",
        "    model = GaussianHMM(\n",
        "        n_components=k_base, covariance_type=covtype, n_iter=n_iter, tol=tol,\n",
        "        random_state=rand0 + 500 + b, verbose=False, init_params=\"mc\", params=\"stmc\"\n",
        "    )\n",
        "    T0 = np.full((k_base, k_base), (1.0 - 0.90) / max(1, k_base - 1)); np.fill_diagonal(T0, 0.90)\n",
        "    model.transmat_ = T0; model.startprob_ = np.full(k_base, 1.0 / k_base)\n",
        "    model.fit(Xb, lengths=[len(Xb)])\n",
        "    model.transmat_ = _diag_sticky_blend(model.transmat_, lam_stick)\n",
        "\n",
        "    # Score on valid dates & align to baseline\n",
        "    dates_scored, states_all = _score_states_on_valid_dates(model, features_base, scaler_base_local)\n",
        "    agree = _agreement_on_intersection(dates_scored, states_all, base_labels)\n",
        "    boot_summ[\"agreement_vs_baseline\"].append(agree)\n",
        "\n",
        "boot_summ[\"agreement_mean\"] = float(np.mean(boot_summ[\"agreement_vs_baseline\"]))\n",
        "boot_summ[\"agreement_std\"]  = float(np.std (boot_summ[\"agreement_vs_baseline\"]))\n",
        "results[\"bootstrap\"] = boot_summ\n",
        "\n",
        "# Save results\n",
        "with open(OUT_PATH, \"w\") as f:\n",
        "    json.dump({\n",
        "        \"created_at\": datetime.utcnow().isoformat() + \"Z\",\n",
        "        \"inputs\": {\n",
        "            \"features_base\": features_base,\n",
        "            \"k_base\": k_base,\n",
        "            \"recency_weighting\": recency,\n",
        "            \"half_life_days\": hl_days,\n",
        "            \"lam_stick\": lam_stick,\n",
        "            \"n_iter\": n_iter, \"n_init\": n_init, \"tol\": tol, \"covariance_type\": covtype\n",
        "        },\n",
        "        \"results\": results\n",
        "    }, f, indent=2)\n",
        "\n",
        "print(json.dumps({\n",
        "    \"status\": \"2.5 done\",\n",
        "    \"out\": OUT_PATH,\n",
        "    \"k_choices\": [2,3],\n",
        "    \"feature_sets_tested\": [fs[0] for fs in fsets],\n",
        "    \"bootstrap\": {\"reps\": BOOT_REPS, \"agreement_mean\": boot_summ[\"agreement_mean\"], \"agreement_std\": boot_summ[\"agreement_std\"]},\n",
        "}, indent=2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGWBeVKSMGWt",
        "outputId": "f39d21d6-e362-43fa-f181-3274f5950225"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"status\": \"2.5 done\",\n",
            "  \"out\": \"artifacts/regimes/regime_sensitivity.json\",\n",
            "  \"k_choices\": [\n",
            "    2,\n",
            "    3\n",
            "  ],\n",
            "  \"feature_sets_tested\": [\n",
            "    \"baseline\",\n",
            "    \"no_vix\",\n",
            "    \"no_breadth\",\n",
            "    \"no_dvix\",\n",
            "    \"core_rv_vix\"\n",
            "  ],\n",
            "  \"bootstrap\": {\n",
            "    \"reps\": 5,\n",
            "    \"agreement_mean\": 0.5312862357741035,\n",
            "    \"agreement_std\": 0.247818080468718\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Section 2.6 — Diagnostics & QA\n",
        "# Plots:\n",
        "#   • Timeline with regime shading over SPY price (rebased) & drawdown\n",
        "#   • Posterior probabilities (stacked area)\n",
        "#   • State return histograms, QQ plots\n",
        "#   • Transition matrix heatmap, dwell-time distribution\n",
        "# Tables:\n",
        "#   • State profiles (load from 2.3), transition matrix & steady-state\n",
        "#   • Switch frequency & chattering metrics\n",
        "# Alerts:\n",
        "#   • Inconsistent semantics (e.g., positive mean but highest vol)\n",
        "#   • Very short dwell (median < 3d)\n",
        "#   • Mapping flips / excessive chattering\n",
        "# Reuses (do not recompute):\n",
        "#   - artifacts/regimes/market_panel.parquet (2.0)\n",
        "#   - artifacts/regimes/window_manifest.json (2.1)\n",
        "#   - artifacts/regimes/hmm_best.pkl (2.2)\n",
        "#   - artifacts/regimes/regime_labels.parquet (2.3/2.4)\n",
        "#   - artifacts/regimes/state_profiles.csv (2.3)\n",
        "#   - artifacts/regimes/regime_meta.json (2.3/2.4)\n",
        "# Outputs:\n",
        "#   - artifacts/regimes/diagnostics/*.png\n",
        "#   - artifacts/regimes/diagnostics/*.csv / *.json\n",
        "#   - console summary + alerts\n",
        "# Notes:\n",
        "#   - #TOCHANGE marks spots to bump for real run (heavier plots/points)\n",
        "# ============================================================\n",
        "\n",
        "from __future__ import annotations\n",
        "import os, json\n",
        "from typing import Dict, Any, List, Tuple\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "REGIME_DIR  = CFG.regime_dir\n",
        "DIAG_DIR    = os.path.join(REGIME_DIR, \"diagnostics\")\n",
        "os.makedirs(DIAG_DIR, exist_ok=True)\n",
        "\n",
        "PANEL_PATH  = os.path.join(REGIME_DIR, \"market_panel.parquet\")\n",
        "MAN_PATH    = os.path.join(REGIME_DIR, \"window_manifest.json\")\n",
        "BUNDLE_PATH = os.path.join(REGIME_DIR, \"hmm_best.pkl\")\n",
        "LABELS_PATH = os.path.join(REGIME_DIR, \"regime_labels.parquet\")\n",
        "META_PATH   = os.path.join(REGIME_DIR, \"regime_meta.json\")\n",
        "PROF_PATH   = os.path.join(REGIME_DIR, \"state_profiles.csv\")\n",
        "\n",
        "# --- Load artifacts\n",
        "assert os.path.exists(PANEL_PATH) and os.path.exists(MAN_PATH) and os.path.exists(BUNDLE_PATH) and os.path.exists(LABELS_PATH)\n",
        "\n",
        "mkt = pd.read_parquet(PANEL_PATH).sort_values(\"date\").reset_index(drop=True)\n",
        "mkt[\"date\"] = pd.to_datetime(mkt[\"date\"])\n",
        "labels = pd.read_parquet(LABELS_PATH).sort_values(\"date\").reset_index(drop=True)\n",
        "labels[\"date\"] = pd.to_datetime(labels[\"date\"])\n",
        "\n",
        "with open(MAN_PATH, \"r\") as f:\n",
        "    MAN = json.load(f)\n",
        "\n",
        "bundle = joblib.load(BUNDLE_PATH)\n",
        "features = bundle[\"features\"]\n",
        "k = int(bundle[\"k\"])\n",
        "\n",
        "meta = {}\n",
        "if os.path.exists(META_PATH):\n",
        "    with open(META_PATH, \"r\") as f:\n",
        "        meta = json.load(f)\n",
        "\n",
        "# --- Derived / convenience\n",
        "date_equal = np.array_equal(mkt[\"date\"].values, labels[\"date\"].values)\n",
        "if not date_equal:\n",
        "    # Align by inner-join on date (some rows may be dropped if any side had NA)\n",
        "    labels = labels.merge(mkt[[\"date\"]], on=\"date\", how=\"inner\").sort_values(\"date\").reset_index(drop=True)\n",
        "    mkt    = mkt.merge(labels[[\"date\"]], on=\"date\", how=\"inner\").sort_values(\"date\").reset_index(drop=True)\n",
        "\n",
        "# Prefer smoothed series if present\n",
        "state_col  = \"state_id_smoothed\" if \"state_id_smoothed\" in labels.columns else \"state_id\"\n",
        "label_col  = \"regime_label_smoothed\" if \"regime_label_smoothed\" in labels.columns else \"regime_label\"\n",
        "\n",
        "# K from posterior columns p0..pK-1 (robust to re-fits)\n",
        "p_cols = [c for c in labels.columns if c.startswith(\"p\")]\n",
        "K = len(p_cols) if len(p_cols) > 0 else k\n",
        "\n",
        "# --- Helpers\n",
        "def _runs(series: np.ndarray) -> List[Tuple[int,int,int]]:\n",
        "    \"\"\"Return (start_idx, end_idx, value) runs for integer state series.\"\"\"\n",
        "    out = []\n",
        "    s = 0; cur = series[0]\n",
        "    for i in range(1, len(series)):\n",
        "        if series[i] != cur:\n",
        "            out.append((s, i-1, int(cur)))\n",
        "            s = i; cur = series[i]\n",
        "    out.append((s, len(series)-1, int(cur)))\n",
        "    return out\n",
        "\n",
        "def _transition_matrix(states: np.ndarray, K: int) -> np.ndarray:\n",
        "    T = np.zeros((K, K), dtype=float)\n",
        "    for i in range(len(states)-1):\n",
        "        T[states[i], states[i+1]] += 1.0\n",
        "    row_sums = T.sum(axis=1, keepdims=True)\n",
        "    row_sums[row_sums==0] = 1.0\n",
        "    return T / row_sums\n",
        "\n",
        "def _steady_state(T: np.ndarray) -> np.ndarray:\n",
        "    # Empirical steady-state as left eigenvector (or fallback to state freq)\n",
        "    try:\n",
        "        vals, vecs = np.linalg.eig(T.T)\n",
        "        i = np.argmin(np.abs(vals - 1.0))\n",
        "        v = np.real(vecs[:, i]); v = np.maximum(v, 0)\n",
        "        if v.sum() == 0: raise ValueError\n",
        "        return v / v.sum()\n",
        "    except Exception:\n",
        "        return np.ones(T.shape[0]) / T.shape[0]\n",
        "\n",
        "def _rebase_price_from_returns(rets: np.ndarray, start=100.0) -> np.ndarray:\n",
        "    # Assumes rets are simple daily returns (e.g., spy_ret). If logrets, replace with exp(cumsum).\n",
        "    out = np.empty_like(rets, dtype=float); out[0] = start * (1.0 + np.nan_to_num(rets[0], nan=0.0))\n",
        "    for i in range(1, len(rets)):\n",
        "        out[i] = out[i-1] * (1.0 + np.nan_to_num(rets[i], nan=0.0))\n",
        "    return out\n",
        "\n",
        "def _drawdown(price: np.ndarray) -> np.ndarray:\n",
        "    cummax = np.maximum.accumulate(price)\n",
        "    dd = price / np.where(cummax==0, 1.0, cummax) - 1.0\n",
        "    return dd\n",
        "\n",
        "# --- Compute core diagnostics\n",
        "states = labels[state_col].to_numpy(dtype=int)\n",
        "Tmat   = _transition_matrix(states, K)\n",
        "ss_emp = _steady_state(Tmat)\n",
        "runs   = _runs(states)\n",
        "dwell  = pd.DataFrame({\n",
        "    \"state_id\": [st for (s,e,st) in runs],\n",
        "    \"run_len\":  [e-s+1 for (s,e,st) in runs],\n",
        "})\n",
        "\n",
        "# --- Switch/chattering metrics\n",
        "switches = (states[1:] != states[:-1]).sum()\n",
        "switch_rate = switches / max(1, len(states)-1)\n",
        "one_day_runs = (dwell[\"run_len\"] == 1).mean()  # fraction single-day\n",
        "lt3_runs = (dwell[\"run_len\"] < 3).mean()\n",
        "\n",
        "# --- Load state profiles (2.3) if present, else compute from TRAIN weights\n",
        "profiles_df = None\n",
        "if os.path.exists(PROF_PATH):\n",
        "    profiles_df = pd.read_csv(PROF_PATH)\n",
        "else:\n",
        "    # Fallback: rough unweighted per-state profiles on all data (not ideal, but avoids recompute)\n",
        "    tmp = []\n",
        "    for s in range(K):\n",
        "        mask = (states == s)\n",
        "        tmp.append({\n",
        "            \"state_id\": s,\n",
        "            \"ret_mean\": float(np.nanmean(mkt.loc[mask,\"spy_ret\"])),\n",
        "            \"ret_std\":  float(np.nanstd (mkt.loc[mask,\"spy_ret\"])),\n",
        "            \"rv20_mean\": float(np.nanmean(mkt.loc[mask,\"spy_rv_20\"])),\n",
        "            \"vix_mean\":  float(np.nanmean(mkt.loc[mask,\"vix_close\"])),\n",
        "            \"dvix_mean\": float(np.nanmean(mkt.loc[mask,\"dvix\"])) if \"dvix\" in mkt.columns else np.nan,\n",
        "            \"breadth_mean\": float(np.nanmean(mkt.loc[mask,\"breadth\"])),\n",
        "            \"ret_q05\":  float(np.nanquantile(mkt.loc[mask,\"spy_ret\"], 0.05)),\n",
        "        })\n",
        "    profiles_df = pd.DataFrame(tmp)\n",
        "profiles_df.to_csv(os.path.join(DIAG_DIR, \"state_profiles_table.csv\"), index=False)\n",
        "\n",
        "# --- Transition matrix & steady-state tables\n",
        "pd.DataFrame(Tmat, columns=[f\"to_{i}\" for i in range(K)], index=[f\"from_{i}\" for i in range(K)]) \\\n",
        "  .to_csv(os.path.join(DIAG_DIR, \"transition_matrix.csv\"))\n",
        "pd.DataFrame({\"state_id\": list(range(K)), \"steady_state_prob\": ss_emp}) \\\n",
        "  .to_csv(os.path.join(DIAG_DIR, \"steady_state.csv\"), index=False)\n",
        "\n",
        "# --- Switch frequency table (yearly)\n",
        "lab = labels[[\"date\", state_col]].copy()\n",
        "lab[\"year\"] = lab[\"date\"].dt.year\n",
        "lab[\"sw\"] = (lab[state_col].shift(-1) != lab[state_col]).astype(int)\n",
        "switch_by_year = lab.groupby(\"year\")[\"sw\"].sum().reset_index().rename(columns={\"sw\":\"n_switches\"})\n",
        "switch_by_year.to_csv(os.path.join(DIAG_DIR, \"switches_by_year.csv\"), index=False)\n",
        "\n",
        "# ============================================================\n",
        "# PLOTS\n",
        "# ============================================================\n",
        "\n",
        "# 1) Price timeline with regime shading & drawdown\n",
        "spy_price = _rebase_price_from_returns(mkt[\"spy_ret\"].to_numpy(dtype=float), start=100.0)\n",
        "spy_dd    = _drawdown(spy_price)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 5))\n",
        "ax.plot(mkt[\"date\"], spy_price, lw=1.25)\n",
        "# Shade by regime\n",
        "for (s,e,st) in runs:\n",
        "    ax.axvspan(mkt[\"date\"].iloc[s], mkt[\"date\"].iloc[e], alpha=0.15, label=f\"State {st}\" if s==runs[0][0] else None)\n",
        "ax.set_title(\"SPY (rebased) with Regime Shading\")\n",
        "ax.set_xlabel(\"Date\"); ax.set_ylabel(\"Rebased Price\")\n",
        "fig.tight_layout()\n",
        "fig.savefig(os.path.join(DIAG_DIR, \"timeline_regime_shading.png\"), dpi=150)\n",
        "plt.close(fig)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 3))\n",
        "ax.plot(mkt[\"date\"], spy_dd, lw=1.0)\n",
        "ax.set_title(\"SPY Drawdown\")\n",
        "ax.set_xlabel(\"Date\"); ax.set_ylabel(\"Drawdown\")\n",
        "fig.tight_layout()\n",
        "fig.savefig(os.path.join(DIAG_DIR, \"timeline_drawdown.png\"), dpi=150)\n",
        "plt.close(fig)\n",
        "\n",
        "# 2) Posterior probabilities (stacked area)\n",
        "if len(p_cols) == K:\n",
        "    fig, ax = plt.subplots(figsize=(12, 4))\n",
        "    ax.stackplot(labels[\"date\"], *(labels[c].to_numpy() for c in p_cols))\n",
        "    ax.set_ylim(0,1); ax.set_title(\"Posterior Probabilities (Stacked)\")\n",
        "    ax.set_xlabel(\"Date\"); ax.set_ylabel(\"Probability\")\n",
        "    fig.tight_layout()\n",
        "    fig.savefig(os.path.join(DIAG_DIR, \"posteriors_stacked.png\"), dpi=150)\n",
        "    plt.close(fig)\n",
        "\n",
        "\n",
        "# (erfinv helper without SciPy)\n",
        "def erfinv(y):\n",
        "    # Approximation (Winitzki) good enough for QQ visual; replace with SciPy in real run\n",
        "    a = 0.147\n",
        "    sign = np.sign(y)\n",
        "    x = np.clip(y, -0.999999, 0.999999)\n",
        "    ln = np.log(1 - x**2)\n",
        "    first = 2/(np.pi*a) + ln/2\n",
        "    return sign * np.sqrt( np.sqrt(first**2 - ln/a) - first )\n",
        "\n",
        "# 3) State return histograms + QQ plots\n",
        "# TOCHANGE: bump N_QQ_POINTS to 1000 for real run\n",
        "N_QQ_POINTS = 200\n",
        "qs = np.linspace(0.01, 0.99, N_QQ_POINTS)\n",
        "for s in range(K):\n",
        "    mask = (states == s)\n",
        "    r = mkt.loc[mask, \"spy_ret\"].dropna().to_numpy()\n",
        "    if len(r) == 0:\n",
        "        continue\n",
        "\n",
        "    # Histogram\n",
        "    fig, ax = plt.subplots(figsize=(5,3))\n",
        "    ax.hist(r, bins=40, alpha=0.8)  # #TOCHANGE: 80 bins for real run\n",
        "    ax.set_title(f\"State {s} return histogram\")\n",
        "    fig.tight_layout()\n",
        "    fig.savefig(os.path.join(DIAG_DIR, f\"state_{s}_ret_hist.png\"), dpi=150)\n",
        "    plt.close(fig)\n",
        "\n",
        "    # QQ vs normal\n",
        "    mu, sd = float(np.mean(r)), float(np.std(r, ddof=0))\n",
        "    if sd <= 0:\n",
        "        sd = 1e-9\n",
        "    emp_q = np.quantile(r, qs)\n",
        "    nor_q = mu + sd * np.sqrt(2) * erfinv(2*qs - 1)  # inverse CDF via erfinv\n",
        "    fig, ax = plt.subplots(figsize=(5,3))\n",
        "    ax.scatter(nor_q, emp_q, s=6, alpha=0.7)\n",
        "    lims = [min(nor_q.min(), emp_q.min()), max(nor_q.max(), emp_q.max())]\n",
        "    ax.plot(lims, lims, lw=1.0)\n",
        "    ax.set_title(f\"State {s} QQ vs Normal\")\n",
        "    ax.set_xlabel(\"Theoretical quantiles\"); ax.set_ylabel(\"Empirical quantiles\")\n",
        "    fig.tight_layout()\n",
        "    fig.savefig(os.path.join(DIAG_DIR, f\"state_{s}_qq.png\"), dpi=150)\n",
        "    plt.close(fig)\n",
        "\n",
        "\n",
        "# 4) Transition heatmap\n",
        "fig, ax = plt.subplots(figsize=(5,4))\n",
        "im = ax.imshow(Tmat, aspect=\"auto\", vmin=0, vmax=np.max(Tmat))\n",
        "ax.set_title(\"Transition Matrix\")\n",
        "ax.set_xlabel(\"to\"); ax.set_ylabel(\"from\")\n",
        "ax.set_xticks(range(K)); ax.set_yticks(range(K))\n",
        "fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
        "fig.tight_layout()\n",
        "fig.savefig(os.path.join(DIAG_DIR, \"transition_matrix_heatmap.png\"), dpi=150)\n",
        "plt.close(fig)\n",
        "\n",
        "# 5) Dwell-time distribution per state\n",
        "fig, ax = plt.subplots(figsize=(6,4))\n",
        "for s in range(K):\n",
        "    ax.hist(dwell.loc[dwell[\"state_id\"]==s, \"run_len\"], bins=range(1,51), alpha=0.6, label=f\"State {s}\")\n",
        "ax.legend()\n",
        "ax.set_title(\"Dwell-time distribution (days)\")\n",
        "ax.set_xlabel(\"Run length (days)\"); ax.set_ylabel(\"Count\")\n",
        "fig.tight_layout()\n",
        "fig.savefig(os.path.join(DIAG_DIR, \"dwell_time_distribution.png\"), dpi=150)\n",
        "plt.close(fig)\n",
        "\n",
        "# ============================================================\n",
        "# ALERTS\n",
        "# ============================================================\n",
        "alerts = []\n",
        "\n",
        "# Semantics: positive mean but highest vol -> suspicious \"Risk-On\"\n",
        "if profiles_df is not None and len(profiles_df) >= K:\n",
        "    vol_rank = profiles_df[\"rv20_mean\"].rank(ascending=True)  # 1 = lowest vol\n",
        "    hi_vol_state = int(profiles_df.loc[profiles_df[\"rv20_mean\"].idxmax(), \"state_id\"])\n",
        "    pos_mean_states = profiles_df.loc[profiles_df[\"ret_mean\"] > 0, \"state_id\"].astype(int).tolist()\n",
        "    if hi_vol_state in pos_mean_states and K >= 2:\n",
        "        alerts.append(f\"State {hi_vol_state}: positive mean return but highest realized vol (check semantics).\")\n",
        "\n",
        "# Dwell-time < 3 days median\n",
        "dwell_median = dwell.groupby(\"state_id\")[\"run_len\"].median()\n",
        "for s, med in dwell_median.items():\n",
        "    if med < 3:\n",
        "        alerts.append(f\"State {s}: median dwell {med}d < 3 (too chatty).\")\n",
        "\n",
        "# Chattering: high switch rate or many single-day runs\n",
        "if switch_rate > 0.15:  # #TOCHANGE: tighten to 0.10 for real run\n",
        "    alerts.append(f\"High switch rate: {switch_rate:.2%}\")\n",
        "if one_day_runs > 0.10:  # #TOCHANGE: tighten to 0.05 for real run\n",
        "    alerts.append(f\"Single-day run fraction elevated: {one_day_runs:.2%}\")\n",
        "\n",
        "# Mapping flips heuristic: compare label continuity around major drawdowns\n",
        "# (simple heuristic: if label changes >3 times within any 20-day window)\n",
        "# #TOCHANGE: widen window to 60 days for real run\n",
        "WINDOW = 20\n",
        "roll_switch = pd.Series((states[1:] != states[:-1]).astype(int)).rolling(WINDOW).sum().fillna(0)\n",
        "if (roll_switch > 3).any():\n",
        "    alerts.append(\"Frequent label flips in short windows (potential mapping instability).\")\n",
        "\n",
        "# Save alerts\n",
        "with open(os.path.join(DIAG_DIR, \"alerts.json\"), \"w\") as f:\n",
        "    json.dump({\"alerts\": alerts}, f, indent=2)\n",
        "\n",
        "# Save a compact summary CSV\n",
        "pd.DataFrame({\n",
        "    \"metric\": [\"K\", \"switches\", \"switch_rate\", \"one_day_runs_frac\", \"lt3_runs_frac\"],\n",
        "    \"value\": [K, switches, switch_rate, one_day_runs, lt3_runs]\n",
        "}).to_csv(os.path.join(DIAG_DIR, \"summary_metrics.csv\"), index=False)\n",
        "\n",
        "print(json.dumps({\n",
        "    \"status\": \"2.6 diagnostics complete\",\n",
        "    \"plots_dir\": DIAG_DIR,\n",
        "    \"alerts_count\": len(alerts),\n",
        "    \"notes\": [\n",
        "        \"State profiles loaded from 2.3 if available; else quick fallback was used.\",\n",
        "        \"Semantics checks are heuristics; confirm with 2.3 profiles and 2.5 sensitivity.\"\n",
        "    ]\n",
        "}, indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWuaoM4eZ57C",
        "outputId": "41c6295a-ab53-4724-e07a-61dd71537e4e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"status\": \"2.6 diagnostics complete\",\n",
            "  \"plots_dir\": \"artifacts/regimes/diagnostics\",\n",
            "  \"alerts_count\": 1,\n",
            "  \"notes\": [\n",
            "    \"State profiles loaded from 2.3 if available; else quick fallback was used.\",\n",
            "    \"Semantics checks are heuristics; confirm with 2.3 profiles and 2.5 sensitivity.\"\n",
            "  ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Section 2.7 — Regime-Aware Policy Hooks (Interfaces to Sec 3–5)\n",
        "# Reuses:\n",
        "#   - artifacts/regimes/regime_labels.parquet (2.3/2.4)\n",
        "#   - artifacts/regimes/regime_meta.json (2.3/2.4)\n",
        "#   - artifacts/regimes/hmm_best.pkl (2.2)  [fallback if p-cols missing]\n",
        "#   - artifacts/regimes/window_manifest.json (2.1)\n",
        "#   - artifacts/regimes/market_panel.parquet (2.0)  [fallback scoring]\n",
        "# Outputs:\n",
        "#   - artifacts/regimes/regime_policy_map.json\n",
        "# Notes:\n",
        "#   - This file is the single interface consumed by Sections 3–5.\n",
        "#   - #TOCHANGE marks values to tune for the real run.\n",
        "# ============================================================\n",
        "\n",
        "from __future__ import annotations\n",
        "import os, json, hashlib\n",
        "from typing import Dict, Any\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "REGIME_DIR  = CFG.regime_dir\n",
        "PANEL_PATH  = os.path.join(REGIME_DIR, \"market_panel.parquet\")\n",
        "LABELS_PATH = os.path.join(REGIME_DIR, \"regime_labels.parquet\")\n",
        "META_PATH   = os.path.join(REGIME_DIR, \"regime_meta.json\")\n",
        "MAN_PATH    = os.path.join(REGIME_DIR, \"window_manifest.json\")\n",
        "BUNDLE_PATH = os.path.join(REGIME_DIR, \"hmm_best.pkl\")\n",
        "OUT_PATH    = os.path.join(REGIME_DIR, \"regime_policy_map.json\")\n",
        "\n",
        "# --- Load essentials\n",
        "labels = pd.read_parquet(LABELS_PATH).sort_values(\"date\").reset_index(drop=True)\n",
        "labels[\"date\"] = pd.to_datetime(labels[\"date\"])\n",
        "with open(MAN_PATH, \"r\") as f: MAN = json.load(f)\n",
        "bundle = joblib.load(BUNDLE_PATH)\n",
        "\n",
        "# state→label semantics\n",
        "state_label_map = None\n",
        "if os.path.exists(META_PATH):\n",
        "    with open(META_PATH, \"r\") as f:\n",
        "        meta = json.load(f)\n",
        "    state_label_map = meta.get(\"state_label_map\", None)\n",
        "else:\n",
        "    meta = {}\n",
        "\n",
        "# infer K and get posteriors\n",
        "p_cols = [c for c in labels.columns if c.startswith(\"p\")]\n",
        "K = len(p_cols) if p_cols else int(bundle[\"k\"])\n",
        "\n",
        "# fallback: if no p-cols, score from model on all dates\n",
        "if not p_cols:\n",
        "    feats = bundle[\"features\"]\n",
        "    scaler = joblib.load(bundle[\"scaler_path\"])\n",
        "    mkt = pd.read_parquet(PANEL_PATH).sort_values(\"date\").reset_index(drop=True)\n",
        "    X_all = scaler.transform(mkt[feats].to_numpy(dtype=float))\n",
        "    post = bundle[\"model\"].predict_proba(X_all)\n",
        "    for s in range(post.shape[1]):\n",
        "        labels[f\"p{s}\"] = post[:, s]\n",
        "    p_cols = [f\"p{s}\" for s in range(K)]\n",
        "\n",
        "# choose smoothed ids/labels if available\n",
        "state_col = \"state_id_smoothed\" if \"state_id_smoothed\" in labels.columns else \"state_id\"\n",
        "label_col = \"regime_label_smoothed\" if \"regime_label_smoothed\" in labels.columns else \"regime_label\"\n",
        "\n",
        "# if meta has mapping but label_col missing, map on the fly\n",
        "if label_col not in labels.columns and state_label_map is not None:\n",
        "    labels[label_col] = labels[state_col].map({int(k): v for k, v in state_label_map.items()})\n",
        "\n",
        "# --- Confidence proxies\n",
        "def entropy(p: np.ndarray) -> float:\n",
        "    p = np.clip(p, 1e-12, 1.0)\n",
        "    return float(-(p*np.log(p)).sum() / np.log(len(p)))  # normalized to [0,1]\n",
        "\n",
        "def aggressiveness_from_confidence(p: np.ndarray) -> Dict[str, float]:\n",
        "    # Proxy 1: max posterior\n",
        "    c_max = float(p.max())\n",
        "    # Proxy 2: 1 - normalized entropy (higher -> more certain)\n",
        "    c_ent = 1.0 - entropy(p)\n",
        "    # Combine (simple mean)  #TOCHANGE: use weighted combo or monotone spline\n",
        "    c = 0.5 * (c_max + c_ent)\n",
        "    # Map to aggressiveness scalar g ∈ [g_min, g_max]\n",
        "    g_min, g_max = 0.35, 1.00     #TOCHANGE: (0.25,1.00) if you want deeper throttling\n",
        "    g = g_min + (g_max - g_min) * c\n",
        "    return {\"c_max\": c_max, \"c_entropy\": c_ent, \"c\": c, \"g\": g}\n",
        "\n",
        "# --- Latest regime & confidence (optionally smooth over last N days)\n",
        "#TOCHANGE: set N_SMOOTH=5–10 for prod; 1 for fast test\n",
        "N_SMOOTH = 3\n",
        "tail = labels.tail(N_SMOOTH)\n",
        "p_tail = tail[p_cols].to_numpy(dtype=float)\n",
        "p_mean = p_tail.mean(axis=0)\n",
        "latest_row = labels.iloc[-1]\n",
        "latest_label = str(latest_row[label_col]) if label_col in labels.columns else f\"State{int(latest_row[state_col])}\"\n",
        "conf = aggressiveness_from_confidence(p_mean)\n",
        "\n",
        "# --- Per-regime policy defaults (edit for your stack)\n",
        "# Use intuitive names; downstream can match by these labels\n",
        "# Turnover caps are relative (e.g., fraction of portfolio eligible to trade)\n",
        "policy_by_regime: Dict[str, Dict[str, Any]] = {\n",
        "    \"Risk-On\": {\n",
        "        \"weights_multipliers\": {          #TOCHANGE: tailor to your factors\n",
        "            \"momentum\": 1.20,\n",
        "            \"quality\":  1.00,\n",
        "            \"value\":    1.00,\n",
        "            \"low_vol\":  0.85,\n",
        "        },\n",
        "        \"turnover_cap\": 0.20,             #TOCHANGE: 0.25\n",
        "        \"risk_target_vol_annual\": 0.10,   # 10%\n",
        "        \"hedge_intensity\": 0.0,           # baseline hedge ratio\n",
        "    },\n",
        "    \"Transition\": {\n",
        "        \"weights_multipliers\": {\n",
        "            \"momentum\": 0.95,\n",
        "            \"quality\":  1.05,\n",
        "            \"value\":    1.05,\n",
        "            \"low_vol\":  1.05,\n",
        "        },\n",
        "        \"turnover_cap\": 0.15,\n",
        "        \"risk_target_vol_annual\": 0.08,   # 8%\n",
        "        \"hedge_intensity\": 0.15,\n",
        "    },\n",
        "    \"Risk-Off\": {\n",
        "        \"weights_multipliers\": {\n",
        "            \"momentum\": 0.70,             # throttle momo\n",
        "            \"quality\":  1.15,             # upweight quality/defensive\n",
        "            \"value\":    1.05,\n",
        "            \"low_vol\":  1.25,\n",
        "        },\n",
        "        \"turnover_cap\": 0.10,\n",
        "        \"risk_target_vol_annual\": 0.06,   # 6%\n",
        "        \"hedge_intensity\": 0.35,\n",
        "    },\n",
        "}\n",
        "\n",
        "# --- If our label universe differs (e.g., only 2 states), coerce keys\n",
        "present_labels = set(labels[label_col].dropna().astype(str).unique()) if label_col in labels.columns else set()\n",
        "for lbl in list(policy_by_regime.keys()):\n",
        "    if lbl not in present_labels and present_labels:\n",
        "        # map missing labels to a reasonable fallback  #TOCHANGE: make explicit mapping per run\n",
        "        del policy_by_regime[lbl]\n",
        "# If states are only numeric (no semantic labels), synthesize keys\n",
        "if not policy_by_regime and state_label_map is None:\n",
        "    unique_states = sorted(labels[state_col].unique())\n",
        "    for s in unique_states:\n",
        "        policy_by_regime[f\"State{s}\"] = {\n",
        "            \"weights_multipliers\": {\"momentum\":1.0,\"quality\":1.0,\"value\":1.0,\"low_vol\":1.0},\n",
        "            \"turnover_cap\": 0.15, \"risk_target_vol_annual\": 0.08, \"hedge_intensity\": 0.15,\n",
        "        }\n",
        "\n",
        "# --- Global scaling by confidence g (downstream can apply this linearly)\n",
        "# We expose both the raw confidence and recommend common scalings.\n",
        "scaling = {\n",
        "    \"aggressiveness_scalar_g\": conf[\"g\"],\n",
        "    \"confidence\": conf,                           # contains c_max, c_entropy, c (combined)\n",
        "    \"recommendations\": {\n",
        "        # Downstream usage suggestions\n",
        "        \"scale_position_sizes_by_g\": True,\n",
        "        \"scale_turnover_cap_by_g\": True,\n",
        "        \"scale_hedge_intensity_by_(1-g)\": True\n",
        "    }\n",
        "}\n",
        "\n",
        "# --- Package the full map\n",
        "out = {\n",
        "    \"created_at\": pd.Timestamp.utcnow().isoformat() + \"Z\",\n",
        "    \"latest_date\": str(latest_row[\"date\"].date()),\n",
        "    \"k\": int(K),\n",
        "    \"latest_regime_label\": latest_label,\n",
        "    \"latest_state_id\": int(latest_row[state_col]),\n",
        "    \"latest_posteriors\": {f\"p{s}\": float(latest_row.get(f\"p{s}\", np.nan)) for s in range(K)},\n",
        "    \"confidence\": scaling,\n",
        "    \"policy_by_regime\": policy_by_regime,\n",
        "    \"inputs\": {\n",
        "        \"labels_path\": LABELS_PATH,\n",
        "        \"meta_path\": META_PATH,\n",
        "        \"bundle_path\": BUNDLE_PATH,\n",
        "        \"scaler_path\": MAN[\"scaler_path\"],\n",
        "        \"features\": bundle[\"features\"],\n",
        "        \"window\": MAN.get(\"window\", {}),\n",
        "        \"smoothing_window_days\": N_SMOOTH,  #TOCHANGE\n",
        "    },\n",
        "}\n",
        "\n",
        "# include sensitivity & diagnostics pointers if present\n",
        "sens_path = os.path.join(REGIME_DIR, \"regime_sensitivity.json\")\n",
        "diag_dir  = os.path.join(REGIME_DIR, \"diagnostics\")\n",
        "if os.path.exists(sens_path):\n",
        "    out[\"inputs\"][\"sensitivity_path\"] = sens_path\n",
        "if os.path.isdir(diag_dir):\n",
        "    out[\"inputs\"][\"diagnostics_dir\"] = diag_dir\n",
        "\n",
        "# hash a minimal signature (useful for caching / auditing)\n",
        "sig = hashlib.sha256(json.dumps({\n",
        "    \"features\": out[\"inputs\"][\"features\"],\n",
        "    \"window\": out[\"inputs\"][\"window\"],\n",
        "    \"k\": out[\"k\"]\n",
        "}, sort_keys=True).encode()).hexdigest()\n",
        "out[\"signature\"] = sig\n",
        "\n",
        "with open(OUT_PATH, \"w\") as f:\n",
        "    json.dump(out, f, indent=2)\n",
        "\n",
        "print(json.dumps({\n",
        "    \"status\": \"2.7 policy hooks exported\",\n",
        "    \"out\": OUT_PATH,\n",
        "    \"latest_label\": latest_label,\n",
        "    \"g_scalar\": round(out[\"confidence\"][\"aggressiveness_scalar_g\"], 4),\n",
        "}, indent=2))"
      ],
      "metadata": {
        "id": "xLOwjNqprmtB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WjpztXYb_NuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Alpha Layer (Signals)"
      ],
      "metadata": {
        "id": "j3s49iJvw3VU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ErnR5GetzRjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Portfolio Construction & Risk"
      ],
      "metadata": {
        "id": "GB56azskw8x_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XIo2hIhCzR14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. RL Sizing Policy (PPO)"
      ],
      "metadata": {
        "id": "OVTV8iNaxcly"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nR9-doWYzSSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Backtesting (Backward Testing) — Rigor"
      ],
      "metadata": {
        "id": "MxL1szv9x19g"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_8u_WWZvzStF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Forward Testing (No Orders; Shadow Runs)"
      ],
      "metadata": {
        "id": "24zaJ3GOx5RJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "45bA-KpizTFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Cost Model & Execution Assumptions"
      ],
      "metadata": {
        "id": "o_yX0Y24x8qb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H-8_FoZ5zTl-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Reproducibility & Testability"
      ],
      "metadata": {
        "id": "ic1uXdOax_ak"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jTVyA9gyzT-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. Visualization & Reporting"
      ],
      "metadata": {
        "id": "NJVGj0mKyBZX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VYhvh69GzUaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11. Automation Options (Optional, no trading)"
      ],
      "metadata": {
        "id": "cL8XA8jqyDfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t1tL4b7EzU3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12. Optional Alpaca Integration (disabled by default)"
      ],
      "metadata": {
        "id": "0Ywf60UCyFbB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ab8-P8WEzWq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 13. File/Module Structure (Colab-friendly)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "/project\n",
        "  config.yaml\n",
        "  data/\n",
        "    universe.csv\n",
        "    features.parquet\n",
        "    regime_labels.parquet\n",
        "  models/\n",
        "    lstm_*.pt / .h5\n",
        "    gbm_*.txt\n",
        "    stacker_*.pkl\n",
        "    rl_policy_*.pkl\n",
        "  runs/YYYY-MM-DD/\n",
        "    signals.parquet\n",
        "    weights.parquet\n",
        "    hedges.parquet\n",
        "    daily_pnl.csv\n",
        "    risk.json\n",
        "  reports/\n",
        "    backtest_tearsheet.html\n",
        "    forward_tearsheet_YYYY-MM.html\n",
        "  src/\n",
        "    data_loader.py\n",
        "    feature_engineering.py\n",
        "    regime.py\n",
        "    models_lstm.py\n",
        "    models_tabular.py\n",
        "    stacking.py\n",
        "    uncertainty.py\n",
        "    portfolio_bl_rp.py\n",
        "    hedging.py\n",
        "    rl_policy.py\n",
        "    backtest.py\n",
        "    forward_shadow.py\n",
        "    risk_metrics.py\n",
        "    stats_tests.py  # DM, SPA/White RC, Sharpe inference\n",
        "    monte_carlo.py  # block bootstrap\n",
        "    reporting.py    # plots & HTML/PDF\n",
        "  main.py          # CLI: daily-shadow / weekly-train / monthly-report\n",
        "  notebook.ipynb   # Colab master: end-to-end run with toggles\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "Jb1M33e5yJKy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 14. More info\n",
        "\n",
        "- Suggested stack: pandas, numpy, scikit-learn, lightgbm, xgboost, tensorflow/PyTorch (choose one for LSTM), hmmlearn, stable-baselines3, cvxpy (for BL/optimization), arch (optional), statsmodels, scipy, matplotlib/plotly.\n",
        "\n",
        "Compute plan (fits $50–$100):\n",
        "\n",
        "- S&P 100, 5–8 walk-forward windows.\n",
        "\n",
        "- LSTM 1–2 layers (64–128 units), MC-dropout 20 samples.\n",
        "\n",
        "- PPO with modest timesteps per window.\n",
        "\n",
        "- 200–400 Monte Carlo bootstrap paths.\n",
        "\n",
        "- 1–3 GPU hours on Colab Pro/Pro+; RAM < 24GB."
      ],
      "metadata": {
        "id": "LYsWzmxjyXy8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 15. Build Order (fastest to value)\n",
        "\n",
        "1. Data + Features + Regimes → validate leakage & plots.\n",
        "\n",
        "2. Multifactor composite → baseline cross-sec L/S backtest.\n",
        "\n",
        "3. GBM/MLP + LSTM → stacking + uncertainty; re-run backtest.\n",
        "\n",
        "4. BL + RP + Dynamic hedge → re-run backtest & stress.\n",
        "\n",
        "5. RL sizing → ablation vs no-RL; finalize backtest.\n",
        "\n",
        "6. Forward shadow loop (daily), weekly retrain, monthly reports.\n",
        "\n",
        "7. Automation (Actions/cron), optional Alpaca paper stub (off)."
      ],
      "metadata": {
        "id": "KZCqpKiHynK6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 16. What you'll see in the first results\n",
        "- Backtest tear sheet with OO-S equity curve, MC bands, by-regime tables, SPA/DM outcomes, VaR/CVaR & stress.\n",
        "\n",
        "- Ablation:\n",
        "\n",
        "  - Multifactor only → +ML → +ML+RL;\n",
        "\n",
        "  - Market-neutral vs long-only w/ hedging;\n",
        "\n",
        "  - Cost sensitivity 5–20 bps.\n",
        "\n",
        "- A live forward dashboard (from Day 1) accumulating daily PnL + monthly report.\n",
        "\n"
      ],
      "metadata": {
        "id": "GR2YWGzNy6Ip"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 17. Forward-Testing Duration Recommendation\n",
        "\n",
        "- Run at least 4 weeks forward shadow to confirm plumbing & stability.\n",
        "\n",
        "- Prefer 8–12 weeks to evaluate regime adaptation, RL sizing behavior under drawdowns, and cost realism.\n",
        "\n",
        "- Only after the forward period matches backtest risk/return within expected error bands should you consider paper-trading execution.\n",
        "\n"
      ],
      "metadata": {
        "id": "2Y4YzjnzzF_r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2rj3UeBraWs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary><strong>Outline Details</strong></summary>\n",
        "\n",
        "# Project Outline — Regime-Aware Multifactor + LSTM/Ensembles + RL (with rigorous back & forward testing)\n",
        "\n",
        "## 0) Objectives & Success Criteria\n",
        "**Primary objective:** Generate statistically significant pure alpha (market-neutral) with controlled drawdowns after transaction costs.  \n",
        "\n",
        "**Secondary objective:** Build a repeatable process capable of ongoing, unattended forward testing that outputs monthly tear sheets.  \n",
        "\n",
        "**Pass/Fail gates (OO-S):**  \n",
        "- Annualized Sharpe ≥ 1.0 (cost-adjusted) across walk-forward windows.  \n",
        "- SPA/White Reality Check non-rejection vs family of alternatives at 5–10% level.  \n",
        "- Max DD ≤ 15–20% (tunable) in backtests.  \n",
        "- Forward test (4–8+ weeks): positive return, rolling Sharpe > 0.8, tail losses consistent with backtest VaR/CVaR.  \n",
        "\n",
        "---\n",
        "\n",
        "## 1) Data & Universe\n",
        "\n",
        "### 1.1 Universe\n",
        "- S&P 100 equities (liquid, keeps compute sane).  \n",
        "- Hedging instruments: SPY + sector ETFs (XLY, XLF, XLV, XLK, XLI, XLE, XLP, XLB, XLU, XLRE).  \n",
        "- Source: Yahoo Finance (daily bars).  \n",
        "- Lookback: 10–15 years if available (train 2012→, test recent).  \n",
        "\n",
        "### 1.2 Features\n",
        "- **Returns/vol:** log returns (1–60d lags), realized vol, ATR.  \n",
        "- **Momentum:** 12–1, 6–1, 20d, trend filters (e.g., SMA cross, slope).  \n",
        "- **Value:** B/P, E/P, CF/P, shareholder yield (latest available; forward-fill monthly/quarterly).  \n",
        "- **Quality:** gross profitability, ROE, accruals, leverage, F-Score-like composite.  \n",
        "- **Market context:** VIX, SPY vol, market breadth (% advancers, optional).  \n",
        "- Leakage controls: strictly lag all features, align to t-1; winsorize & z-score cross-sectionally.  \n",
        "\n",
        "### 1.3 Data Hygiene\n",
        "- Survivorship-bias approach: use current S&P 100 for practicality; (optional) point-in-time later.  \n",
        "- Corporate actions: use adjusted prices.  \n",
        "- Missing fundamentals: impute conservatively or drop; record masks for model.  \n",
        "- **Deliverables:** `features.parquet`, `universe.csv`, `meta.yaml`.  \n",
        "\n",
        "---\n",
        "\n",
        "## 2) Regime Modeling\n",
        "\n",
        "### 2.1 HMM (2–3 states)\n",
        "- Inputs: SPY daily returns/vol, VIX level/change, market breadth.  \n",
        "- States: Risk-On, Risk-Off, Transition (labeled by average return/vol).  \n",
        "- **Output:** daily regime label + posterior probabilities.  \n",
        "\n",
        "### 2.2 Usage\n",
        "- Regime-specific ensemble weights, turnover caps, and risk targets.  \n",
        "- Momentum throttled in Risk-Off; quality emphasized.  \n",
        "- **Deliverables:** `regime_labels.parquet`, regime plot.  \n",
        "\n",
        "---\n",
        "\n",
        "## 3) Alpha Layer (Signals)\n",
        "\n",
        "### 3.1 Multifactor Composite\n",
        "- Value/Momentum/Quality composites (winsorized, z-scored).  \n",
        "- Per-regime blend fit with ridge.  \n",
        "- **Output:** factor alpha score per asset/day.  \n",
        "\n",
        "### 3.2 ML Overlays\n",
        "- **LSTM:** 60-day sequences → t+5/t+10 returns; MC-dropout for uncertainty.  \n",
        "- **Tabular ensembles:** LightGBM (primary), XGBoost, small MLP; also quantile versions.  \n",
        "- **Stacking meta-learner:** ridge/LightGBM; OOF training within walk-forward train window.  \n",
        "- **Output:** final forecast (mean) + uncertainty proxy.  \n",
        "\n",
        "### 3.3 Uncertainty → Confidence\n",
        "- Expected Sharpe proxy = mean / std_hat.  \n",
        "- Bucket confidence for analytics.  \n",
        "- **Deliverables:** `alpha_raw.parquet`, `alpha_ensemble.parquet`, feature importance charts.  \n",
        "\n",
        "---\n",
        "\n",
        "## 4) Portfolio Construction & Risk\n",
        "\n",
        "### 4.1 Baseline Weights\n",
        "- Cross-sectional L/S: long top decile, short bottom decile by forecasted Sharpe.  \n",
        "- Beta-neutral, per-name and sector caps.  \n",
        "\n",
        "### 4.2 Black–Litterman (BL)\n",
        "- Prior: market-cap weights → implied μ.  \n",
        "- Views: ensemble alphas scaled by uncertainty.  \n",
        "- Posterior μ̂ → mean-variance with L2 & turnover penalty.  \n",
        "\n",
        "### 4.3 Risk Parity & Vol Target\n",
        "- Equalize risk across sector/factor clusters.  \n",
        "- Target portfolio vol (8–12% ann.).  \n",
        "\n",
        "### 4.4 Dynamic Hedging\n",
        "- Daily orthogonalization vs SPY + sectors; hedge ratios adjustable by RL.  \n",
        "- **Deliverables:** weights, exposures, hedge plots.  \n",
        "\n",
        "---\n",
        "\n",
        "## 5) RL Sizing Policy (PPO)\n",
        "\n",
        "### 5.1 Role\n",
        "- Scales risk target and tunes hedges.  \n",
        "\n",
        "### 5.2 State\n",
        "- Regime, vol, drawdown, alpha strength, uncertainty, turnover, betas, cost model.  \n",
        "\n",
        "### 5.3 Reward\n",
        "- PnL – costs – λ·CVaR_tail – κ·Δdrawdown – penalties.  \n",
        "\n",
        "### 5.4 Training\n",
        "- Train within walk-forward segments; fixed seeds.  \n",
        "- **Deliverables:** `rl_policy.pkl`, diagnostics.  \n",
        "\n",
        "---\n",
        "\n",
        "## 6) Backtesting (Backward Testing) — Rigor\n",
        "\n",
        "### 6.1 Walk-Forward Engine\n",
        "- Rolling/expanding windows; purged & embargoed CV.  \n",
        "- Refit all models per window; test daily with costs.  \n",
        "\n",
        "### 6.2 Significance & Reality Checks\n",
        "- DM test, SPA/White RC, Sharpe inference.  \n",
        "\n",
        "### 6.3 Tail Risk & Stress\n",
        "- VaR/CVaR; stress tests (2008/2020, vol shocks, liquidity cuts).  \n",
        "\n",
        "### 6.4 Monte Carlo Robustness\n",
        "- Block bootstrap; output PnL envelopes.  \n",
        "- **Deliverables:** equity curves, DD charts, ablations.  \n",
        "\n",
        "---\n",
        "\n",
        "## 7) Forward Testing (Shadow Mode)\n",
        "\n",
        "### 7.1 Daily Shadow Run\n",
        "- No backfill; use latest models; log all artifacts.  \n",
        "\n",
        "### 7.2 Retraining Cadence\n",
        "- Weekly or bi-weekly; strict forward-only.  \n",
        "\n",
        "### 7.3 Monthly Auto-Report\n",
        "- Tear sheets with returns, Sharpe, DD, risk, regime PnL, VaR/CVaR.  \n",
        "\n",
        "### 7.4 Duration\n",
        "- Min: 4 weeks; Pref: 8–12 weeks.  \n",
        "- **Deliverables:** daily run files, monthly reports.  \n",
        "\n",
        "---\n",
        "\n",
        "## 8) Cost Model & Execution Assumptions\n",
        "- Costs: 10 bps round-trip (sweep 5–20).  \n",
        "- Slippage: 1–2 bps; higher in Risk-Off.  \n",
        "- Short borrow: 10–50 bps ann.  \n",
        "- Liquidity caps: ≤5–10% ADV.  \n",
        "\n",
        "---\n",
        "\n",
        "## 9) Reproducibility & Testability\n",
        "- Config-driven (`config.yaml`); fixed seeds.  \n",
        "- Unit/integration tests for leakage, CV folds, NaNs, RL bounds.  \n",
        "- Experiment tracking with CSV/JSON + git hash.  \n",
        "\n",
        "---\n",
        "\n",
        "## 10) Visualization & Reporting\n",
        "- Equity curves with regime shading, rolling metrics, exposures, attribution, bucket PnL, by-regime performance, risk dashboards.  \n",
        "\n",
        "---\n",
        "\n",
        "## 11) Automation Options\n",
        "- **Colab:** manual or scheduled;  \n",
        "- **GitHub Actions:** nightly, weekly, monthly;  \n",
        "- **VM + cron:** low-budget option.  \n",
        "\n",
        "---\n",
        "\n",
        "## 12) Optional Alpaca Integration\n",
        "- Disabled by default; forward test never sends orders; later optional paper fills.  \n",
        "\n",
        "</details>\n"
      ],
      "metadata": {
        "id": "Wh8ahohLNbIu"
      }
    }
  ]
}