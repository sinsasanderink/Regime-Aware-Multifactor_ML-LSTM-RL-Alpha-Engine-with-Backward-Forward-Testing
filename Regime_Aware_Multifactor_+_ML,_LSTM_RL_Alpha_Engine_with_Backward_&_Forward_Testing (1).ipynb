{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Regime-Aware Multifactor + ML/RL Alpha Engine with Backward & Forward Testing\n",
        "\n",
        "## Project Description\n",
        "\n",
        "This project is a **modular trading research system** designed to generate **pure alpha** (market-neutral returns independent of market beta) by combining **proven multifactor investing principles** with **modern machine learning and reinforcement learning techniques**, and testing them with **rigorous statistical validation**.\n",
        "\n",
        "The strategy’s profit engine comes from exploiting cross-sectional mispricings in a broad large-cap U.S. universe (**S&P 500 training set with dynamic top-N selection by confidence**) by identifying which stocks are likely to outperform or underperform others over the next 5–10 days. This is achieved through:\n",
        "\n",
        "- **Multifactor Alpha Layer:**  \n",
        "  - **Value** (cheap stocks with potential to mean-revert up)  \n",
        "  - **Momentum** (stocks in persistent trends)  \n",
        "  - **Quality** (financially strong, operationally robust companies)  \n",
        "  - Per-regime factor blending with shrinkage to avoid overfitting.\n",
        "\n",
        "- **Machine Learning Overlays:**  \n",
        "  - **LSTM** (sequence model) to capture time-series patterns in returns, volatility, and technicals.  \n",
        "  - **LightGBM/XGBoost/MLP** (tabular models) to detect nonlinear interactions in cross-sectional features.  \n",
        "  - **Stacking meta-learner** to optimally blend factor and ML outputs.  \n",
        "  - **Uncertainty quantification** via MC-dropout and quantile models to control position sizing.\n",
        "\n",
        "- **Regime Detection:**  \n",
        "  - Hidden Markov Model (HMM) to classify markets as **Risk-On**, **Risk-Off**, or **Transition**, adjusting model weights and risk accordingly.\n",
        "\n",
        "- **Portfolio Construction & Risk Management:**  \n",
        "  - **Black–Litterman optimization** to integrate model views with market-implied returns.  \n",
        "  - **Risk parity** to balance sector/factor exposures.  \n",
        "  - **Dynamic hedging** against SPY/sector ETFs to maintain market neutrality.\n",
        "\n",
        "- **Reinforcement Learning (PPO):**  \n",
        "  - Learns a sizing and hedging policy that adapts risk-taking to forecast strength, uncertainty, and current market regime, maximizing return per unit of tail risk (CVaR-aware reward).\n",
        "\n",
        "## Testing & Validation\n",
        "\n",
        "The project integrates **both backward and forward testing** to ensure robustness:\n",
        "\n",
        "- **Backward Testing (Historical):**  \n",
        "  - Walk-forward analysis with purged cross-validation to avoid look-ahead bias.  \n",
        "  - Statistical significance tests (Diebold–Mariano, SPA/White Reality Check) to confirm non-randomness.  \n",
        "  - Monte Carlo block bootstrap to estimate confidence intervals and failure probabilities.  \n",
        "  - VaR/CVaR analysis and stress testing against historical crisis scenarios.\n",
        "\n",
        "- **Forward Testing (Shadow, No Trades):**  \n",
        "  - Daily simulation using only forward data, logging PnL and risk metrics without sending orders.  \n",
        "  - Weekly retraining and monthly auto-generated tear sheets to track live performance against backtest expectations.  \n",
        "  - Recommended forward-testing period: 4–12 weeks before considering paper/live execution.\n",
        "\n",
        "## Goal\n",
        "\n",
        "The system’s goal is to produce **consistent, statistically validated alpha** with low correlation to the market and controlled drawdowns, using a combination of **factor investing, machine learning, and reinforcement learning**. This approach maximizes the probability of sustainable profitability before any real capital is risked.\n",
        "\n"
      ],
      "metadata": {
        "id": "Lh0jFF7ysB3v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Objectives & Success Criteria\n",
        "- Primary objective: Generate statistically significant pure alpha (market-neutral) with controlled drawdowns after transaction costs.\n",
        "\n",
        "- Secondary objective: Build a repeatable process capable of ongoing, unattended forward testing that outputs monthly tear sheets.\n",
        "\n",
        "- Pass/Fail gates (OO-S):\n",
        "  - Annualized Sharpe ≥ 1.0 (cost-adjusted) across walk-forward windows.\n",
        "  - SPA/White Reality Check non-rejection vs family of alternatives at 5–10% level.\n",
        "  - Max DD ≤ 15–20% (tunable) in backtests.\n",
        "  - Forward test (4–8+ weeks): positive return, rolling Sharpe > 0.8, tail losses consistent with backtest VaR/CVaR.\n",
        "\n"
      ],
      "metadata": {
        "id": "33IwP6ukt9c3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Data & Universe"
      ],
      "metadata": {
        "id": "dc5X57hiwx36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "if device == \"cuda\":\n",
        "    print(torch.cuda.get_device_name(0))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mwrsmOX39bI",
        "outputId": "fe3f21a8-eedf-423b-c315-4bf9bf5d3742"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip -q install yfinance pandas numpy PyYAML pyarrow statsmodels tenacity"
      ],
      "metadata": {
        "id": "8t8ZL73mzgiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 1.1 UNIVERSE (UPDATED)\n",
        "# S&P 500 training set with dynamic top-N selection by confidence (later in pipeline).\n",
        "# Hedging instruments: SPY + sector ETFs.\n",
        "# Source: Yahoo Finance (daily bars). Lookback from 2006-01-01 to today.\n",
        "# Saves: universe.csv and raw_prices.parquet (OHLCV + Adj Close for all tickers incl. hedges + ^VIX)\n",
        "# ============================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yfinance as yf\n",
        "from datetime import datetime\n",
        "\n",
        "START_DATE = \"2006-01-01\"\n",
        "END_DATE = datetime.today().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "def to_fmp_symbol(sym: str) -> str:\n",
        "    # map Yahoo/WSJ style class tickers to FMP\n",
        "    return sym.replace(\"-\", \".\") if \"-\" in sym else sym\n",
        "\n",
        "def is_index_like(sym: str) -> bool:\n",
        "    # skip ^VIX and other index-style series for FMP backfill\n",
        "    return sym.startswith(\"^\")\n",
        "\n",
        "# --- Get S&P 500 constituents from Wikipedia (survivorship bias acknowledged) ---\n",
        "sp500_url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
        "tables = pd.read_html(sp500_url)\n",
        "sp500 = tables[0]  # first table\n",
        "tickers_raw = sp500[\"Symbol\"].tolist()\n",
        "# Some tickers on Wikipedia have periods; yfinance uses dashes for certain cases\n",
        "tickers = [t.replace(\".\", \"-\") for t in tickers_raw]\n",
        "\n",
        "# --- Hedging instruments (market & sector ETFs) ---\n",
        "hedges = [\"SPY\", \"XLY\", \"XLF\", \"XLV\", \"XLK\", \"XLI\", \"XLE\", \"XLP\", \"XLB\", \"XLU\", \"XLRE\"]\n",
        "context_symbols = [\"^VIX\"]  # market context series\n",
        "\n",
        "universe = sorted(set(tickers))\n",
        "universe_all = sorted(set(universe + hedges + context_symbols))\n",
        "\n",
        "# --- Save universe to CSV ---\n",
        "pd.DataFrame({\"ticker\": universe}).to_csv(f\"universe_{END_DATE}.csv\", index=False)\n",
        "pd.DataFrame({\"ticker\": universe}).to_csv(\"universe.csv\", index=False)  # pointer\n",
        "\n",
        "\n",
        "# --- Download daily OHLCV for all symbols ---\n",
        "# yfinance handles adjusted prices; we’ll keep both Close & Adj Close.\n",
        "data = yf.download(\n",
        "    universe_all,\n",
        "    start=START_DATE,\n",
        "    end=END_DATE,\n",
        "    auto_adjust=False,\n",
        "    group_by=\"ticker\",\n",
        "    progress=False,\n",
        "    threads=True,\n",
        ")\n",
        "\n",
        "if data is None or getattr(data, \"empty\", False):\n",
        "    raise RuntimeError(\"yfinance returned no data — try rerunning or chunking the request.\")\n",
        "\n",
        "def top_level_symbols(df):\n",
        "    # Handles both MultiIndex (normal multi-ticker) and flat columns (edge cases)\n",
        "    if isinstance(df.columns, pd.MultiIndex):\n",
        "        return set(df.columns.get_level_values(0))\n",
        "    # flat columns -> we can only have one symbol; yfinance puts OHLCV names as columns\n",
        "    return set()  # treat as empty to trigger backfill logic safely\n",
        "\n",
        "# added: tells us if yfinance skipped any tickers\n",
        "available = top_level_symbols(data)\n",
        "missing = [sym for sym in universe_all if sym not in available]\n",
        "if missing:\n",
        "    pd.Series(missing, name=\"missing_symbols\").to_csv(\"missing_symbols.csv\", index=False)\n",
        "    print(f\"WARNING: {len(missing)} symbols missing from download. Saved to missing_symbols.csv\")\n",
        "\n",
        "# Normalize to tidy format: MultiIndex -> long DataFrame\n",
        "frames = []\n",
        "if isinstance(data.columns, pd.MultiIndex):\n",
        "    for sym in universe_all:\n",
        "        if sym not in available:\n",
        "            continue\n",
        "        df = data[sym].copy()\n",
        "        df.columns = [c.lower().replace(\" \", \"_\") for c in df.columns]\n",
        "        df[\"ticker\"] = sym\n",
        "        frames.append(df.reset_index().rename(columns={\"Date\": \"date\"}))\n",
        "else:\n",
        "    # Edge: flat columns — shouldn't happen with many symbols, but keep it safe\n",
        "    df = data.copy()\n",
        "    df.columns = [c.lower().replace(\" \", \"_\") for c in df.columns]\n",
        "    df[\"ticker\"] = universe_all[0]\n",
        "    frames.append(df.reset_index().rename(columns={\"Date\": \"date\"}))\n",
        "\n",
        "prices = pd.concat(frames, ignore_index=True).sort_values([\"ticker\", \"date\"])\n",
        "prices[\"date\"] = pd.to_datetime(prices[\"date\"])\n",
        "\n",
        "# Basic sanity: drop rows with all NaNs for OHLCV\n",
        "keep_cols = [\"open\", \"high\", \"low\", \"close\", \"adj_close\", \"volume\"]\n",
        "prices = prices.dropna(subset=keep_cols, how=\"all\")\n",
        "\n",
        "# Save raw prices\n",
        "prices.to_parquet(\"raw_prices.parquet\", index=False)\n",
        "\n",
        "print(f\"Universe size (S&P 500): {len(universe)} tickers\")\n",
        "print(f\"Total symbols incl. hedges/context: {len(universe_all)}\")\n",
        "print(\"Saved: universe.csv, raw_prices.parquet\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7kItZpBzQyM",
        "outputId": "e4e2d911-7ca1-4471-c0da-1721ec696816"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Universe size (S&P 500): 503 tickers\n",
            "Total symbols incl. hedges/context: 515\n",
            "Saved: universe.csv, raw_prices.parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- Optional: Backfill any missing tickers with FMP (skip ^VIX etc.) ----\n",
        "import os, requests, time\n",
        "from getpass import getpass\n",
        "\n",
        "if os.path.exists(\"missing_symbols.csv\"):\n",
        "    missing = pd.read_csv(\"missing_symbols.csv\")[\"missing_symbols\"].tolist()\n",
        "else:\n",
        "    uni = pd.read_csv(\"universe.csv\")[\"ticker\"].tolist()\n",
        "    hedges = [\"SPY\",\"XLY\",\"XLF\",\"XLV\",\"XLK\",\"XLI\",\"XLE\",\"XLP\",\"XLB\",\"XLU\",\"XLRE\"]\n",
        "    context = [\"^VIX\"]\n",
        "    universe_all = sorted(set(uni + hedges + context))\n",
        "    base_prices = pd.read_parquet(\"raw_prices.parquet\")\n",
        "    present = set(base_prices[\"ticker\"].unique())\n",
        "    missing = [s for s in universe_all if s not in present]\n",
        "\n",
        "missing = [s for s in missing if not is_index_like(s)]\n",
        "if not missing:\n",
        "    print(\"No missing symbols to backfill.\")\n",
        "else:\n",
        "    print(f\"Backfilling {len(missing)} symbols from FMP (skipping indexes):\", missing[:8], \"...\")\n",
        "    FMP_API_KEY = os.environ.get(\"FMP_API_KEY\", \"\").strip() or getpass(\"Enter FMP API key for price backfill: \").strip()\n",
        "    if not FMP_API_KEY:\n",
        "        raise RuntimeError(\"FMP_API_KEY required for backfill.\")\n",
        "\n",
        "    base_url = \"https://financialmodelingprep.com/api/v3/historical-price-full\"\n",
        "    def fetch_fmp_prices(sym):\n",
        "        fmp_sym = to_fmp_symbol(sym)\n",
        "        url = f\"{base_url}/{fmp_sym}?from={START_DATE}&to={END_DATE}&serietype=line&apikey={FMP_API_KEY}\"\n",
        "        r = requests.get(url, timeout=30)\n",
        "        r.raise_for_status()\n",
        "        js = r.json()\n",
        "        hist = js.get(\"historical\", [])\n",
        "        if not hist:\n",
        "            return None\n",
        "        df = pd.DataFrame(hist)\n",
        "        df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
        "        # Map columns; fall back if adjClose missing\n",
        "        df = df.rename(columns={\"adjClose\":\"adj_close\"})\n",
        "        if \"adj_close\" not in df.columns:\n",
        "            df[\"adj_close\"] = df[\"close\"]\n",
        "        cols = [\"date\",\"open\",\"high\",\"low\",\"close\",\"adj_close\",\"volume\"]\n",
        "        for c in cols:\n",
        "            if c not in df.columns: df[c] = np.nan\n",
        "        df = df[cols]\n",
        "        df[\"ticker\"] = sym\n",
        "        return df.sort_values(\"date\")\n",
        "\n",
        "    filled = []\n",
        "    for i, sym in enumerate(missing, 1):\n",
        "        try:\n",
        "            df = fetch_fmp_prices(sym)\n",
        "            if df is not None and len(df):\n",
        "                filled.append(df)\n",
        "        except Exception:\n",
        "            pass\n",
        "        if i % 10 == 0:\n",
        "            time.sleep(0.5)  # be polite\n",
        "\n",
        "    if filled:\n",
        "        add = pd.concat(filled, ignore_index=True)\n",
        "        base_prices = pd.read_parquet(\"raw_prices.parquet\")\n",
        "        prices_fixed = pd.concat([base_prices, add], ignore_index=True).sort_values([\"ticker\",\"date\"])\n",
        "        prices_fixed.to_parquet(\"raw_prices.parquet\", index=False)\n",
        "        print(f\"Backfilled {add['ticker'].nunique()} symbols and re-saved raw_prices.parquet\")\n",
        "    else:\n",
        "        print(\"FMP backfill returned no data; proceeding without these tickers.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcFnrcEUU52H",
        "outputId": "a5e532ad-6891-4ce2-e76f-c0cc28fa2253"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No missing symbols to backfill.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 1.2 FEATURES (FMP Premium, no hard-coded key)\n",
        "# ------------------------------------------------------------\n",
        "# Builds:\n",
        "#   • Price/technical features (returns/vol/ATR/momentum/trend)\n",
        "#   • Market context (SPY vol, ^VIX, breadth)\n",
        "#   • Fundamentals via FMP (quarterly BS/IS/CF), cached per ticker,\n",
        "#     forward-filled to daily, and ratio metrics (Value + Quality)\n",
        "# Post-merge:\n",
        "#   • Leakage control (shift all predictive features by 1 day)\n",
        "#   • Winsorize & cross-sectional z-score (by date)\n",
        "#   • Fundamentals imputation + missing masks\n",
        "# Saves:\n",
        "#   • features.parquet\n",
        "#   • funda_quarterly.parquet, funda_daily.parquet\n",
        "#   • cache/funda_q_<TICKER>.parquet (per-ticker cache)\n",
        "# Notes:\n",
        "#   - API key is taken from env var FMP_API_KEY or prompted securely.\n",
        "# ============================================================\n",
        "\n",
        "# %pip -q install yfinance pyarrow tenacity\n",
        "\n",
        "import os, time, random, gc\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yfinance as yf\n",
        "from getpass import getpass\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n",
        "\n",
        "# ---------- Config / Toggles ----------\n",
        "COMPUTE_SLOPE = True      # slope_20 via vectorized method\n",
        "SLOPE_WINDOW = 20\n",
        "RV_WIN = 20\n",
        "ATR_WIN = 14\n",
        "\n",
        "# Fundamentals provider config (FMP Premium)\n",
        "PROVIDER = \"fmp\"          # fixed to FMP for reliability\n",
        "FMP_API_KEY = os.environ.get(\"FMP_API_KEY\", \"\").strip()\n",
        "if not FMP_API_KEY:\n",
        "    # Prompt securely; not echoed, not written to disk\n",
        "    FMP_API_KEY = getpass(\"Enter your FMP API key (kept in-memory for this session): \").strip()\n",
        "if not FMP_API_KEY:\n",
        "    raise RuntimeError(\"FMP_API_KEY is required. Set env var FMP_API_KEY or enter it when prompted.\")\n",
        "\n",
        "def to_fmp_symbol(sym: str) -> str:\n",
        "    \"\"\"\n",
        "    Convert Yahoo-style tickers to FMP-style.\n",
        "    Yahoo uses '-' for class/shared tickers (e.g., BRK-B),\n",
        "    while FMP uses '.' (e.g., BRK.B). Everything else stays the same.\n",
        "    \"\"\"\n",
        "    # common class/delimiter cases\n",
        "    # e.g., BRK-B, BF-B, FOXA (no change), META (no change)\n",
        "    if \"-\" in sym:\n",
        "        return sym.replace(\"-\", \".\")\n",
        "    return sym\n",
        "\n",
        "# Chunking: Premium can fetch all at once. If you ever need throttling, set CHUNK_TICKERS to an int.\n",
        "CHUNK_TICKERS = 20        # TOCHANGE: None = process entire universe in one go > change this later to None to check all stocks instead of just 100\n",
        "START_AT = 0              # offset if chunking\n",
        "SKIP_IF_CACHED = True     # skip ticker if cache exists\n",
        "\n",
        "MAX_WORKERS = 4           # Premium can handle more concurrency; tune 4–12 as you like\n",
        "RETRY_ATTEMPTS = 5\n",
        "BATCH_SLEEP = (0.2, 0.6)  # polite jitter between HTTP calls\n",
        "CACHE_DIR = \"cache\"\n",
        "os.makedirs(CACHE_DIR, exist_ok=True)\n",
        "\n",
        "# ---------- Load raw prices & universe (from 1.1) ----------\n",
        "prices = pd.read_parquet(\"raw_prices.parquet\")\n",
        "universe_full = list(pd.read_csv(\"universe.csv\")[\"ticker\"])\n",
        "hedges = {\"SPY\", \"XLY\", \"XLF\", \"XLV\", \"XLK\", \"XLI\", \"XLE\", \"XLP\", \"XLB\", \"XLU\", \"XLRE\"}\n",
        "context_symbols = {\"^VIX\"}\n",
        "\n",
        "# ============================================================\n",
        "# A) PRICE / TECHNICAL FEATURES\n",
        "# ============================================================\n",
        "\n",
        "def compute_atr(df, window=ATR_WIN):\n",
        "    high, low, close = df[\"high\"], df[\"low\"], df[\"close\"]\n",
        "    prev_close = close.shift(1)\n",
        "    tr = pd.concat([(high - low),\n",
        "                    (high - prev_close).abs(),\n",
        "                    (low - prev_close).abs()], axis=1).max(axis=1)\n",
        "    return tr.rolling(window).mean()\n",
        "\n",
        "def vectorized_rolling_slope(y: pd.Series, window=SLOPE_WINDOW) -> pd.Series:\n",
        "    N = window\n",
        "    if N <= 1:\n",
        "        return pd.Series(np.nan, index=y.index, dtype=float)\n",
        "    x = np.arange(N, dtype=float)\n",
        "    Sx = x.sum()\n",
        "    Sxx = (x**2).sum()\n",
        "    yv = y.to_numpy(dtype=float)\n",
        "    yv = np.where(np.isfinite(yv), yv, 0.0)\n",
        "    k = np.ones(N, dtype=float)\n",
        "    Sy  = np.convolve(yv, k[::-1], mode=\"full\")[N-1:len(yv)+N-1]\n",
        "    Sxy = np.convolve(yv, x[::-1], mode=\"full\")[N-1:len(yv)+N-1]\n",
        "    denom = N * Sxx - Sx * Sx + 1e-12\n",
        "    slope = (N * Sxy - Sx * Sy) / denom\n",
        "    out = pd.Series(np.nan, index=y.index, dtype=float)\n",
        "    out.iloc[N-1:] = slope[N-1:]\n",
        "    return out\n",
        "\n",
        "def mom_over_n(adj_close, n):\n",
        "    return np.log(adj_close / adj_close.shift(n))\n",
        "\n",
        "feat_frames = []\n",
        "tickers = sorted(prices[\"ticker\"].unique())\n",
        "total = len(tickers)\n",
        "\n",
        "for i, (sym, df_sym) in enumerate(prices.groupby(\"ticker\"), start=1):\n",
        "    if sym in context_symbols:\n",
        "        continue\n",
        "    if i % 25 == 0:\n",
        "        print(f\"[Features] {i}/{total} processed… ({sym})\")\n",
        "\n",
        "    df = df_sym.sort_values(\"date\").copy()\n",
        "    df[\"ret_1d\"] = np.log(df[\"adj_close\"] / df[\"adj_close\"].shift(1))\n",
        "    for l in range(1, 61):\n",
        "        df[f\"ret_lag_{l}\"] = df[\"ret_1d\"].shift(l)\n",
        "\n",
        "    df[\"rv_20\"] = df[\"ret_1d\"].rolling(RV_WIN).std() * np.sqrt(252)\n",
        "    df[\"atr_14\"] = compute_atr(df, ATR_WIN)\n",
        "\n",
        "    df[\"mom_20\"]  = mom_over_n(df[\"adj_close\"], 20)\n",
        "    df[\"mom_6m\"]  = mom_over_n(df[\"adj_close\"], 126)\n",
        "    df[\"mom_12m\"] = mom_over_n(df[\"adj_close\"], 252)\n",
        "    df[\"mom_12_1\"] = np.log(df[\"adj_close\"].shift(21) / df[\"adj_close\"].shift(252))\n",
        "    df[\"mom_6_1\"]  = np.log(df[\"adj_close\"].shift(21) / df[\"adj_close\"].shift(126))\n",
        "\n",
        "    df[\"sma_20\"] = df[\"adj_close\"].rolling(20).mean()\n",
        "    df[\"sma_50\"] = df[\"adj_close\"].rolling(50).mean()\n",
        "    df[\"sma_20_gt_50\"] = (df[\"sma_20\"] > df[\"sma_50\"]).astype(\"float32\")\n",
        "    df[\"slope_20\"] = vectorized_rolling_slope(df[\"adj_close\"], window=SLOPE_WINDOW) if COMPUTE_SLOPE else np.nan\n",
        "\n",
        "    df[\"mom_20_vs_vol\"] = df[\"mom_20\"] / (df[\"ret_1d\"].rolling(20).std() + 1e-8)\n",
        "\n",
        "    feat_frames.append(df)\n",
        "\n",
        "features = pd.concat(feat_frames, ignore_index=True)\n",
        "\n",
        "# ============================================================\n",
        "# B) MARKET CONTEXT (SPY vol, VIX, breadth)\n",
        "# ============================================================\n",
        "\n",
        "vix = prices[prices[\"ticker\"] == \"^VIX\"][[\"date\", \"adj_close\"]].rename(columns={\"adj_close\": \"vix_close\"})\n",
        "spy = prices[prices[\"ticker\"] == \"SPY\"].copy()\n",
        "spy[\"spy_ret\"] = np.log(spy[\"adj_close\"] / spy[\"adj_close\"].shift(1))\n",
        "spy[\"spy_rv_20\"] = spy[\"spy_ret\"].rolling(20).std() * np.sqrt(252)\n",
        "ctx = spy[[\"date\", \"spy_rv_20\"]].merge(vix, on=\"date\", how=\"left\")\n",
        "\n",
        "rets = features.pivot(index=\"date\", columns=\"ticker\", values=\"ret_1d\")\n",
        "advancers = (rets > 0).sum(axis=1)\n",
        "# Fixed denominator for stability = full S&P 500 count from universe.csv\n",
        "breadth = (advancers / len(universe_full)).rename(\"breadth\")\n",
        "ctx = ctx.merge(breadth.reset_index(), on=\"date\", how=\"left\")\n",
        "\n",
        "features = features.merge(ctx, on=\"date\", how=\"left\")\n",
        "\n",
        "# ============================================================\n",
        "# C) FUNDAMENTALS (FMP Premium primary; cached per ticker)\n",
        "# ============================================================\n",
        "\n",
        "px_daily_all = prices[prices[\"ticker\"].isin(universe_full)][[\"date\", \"ticker\", \"adj_close\"]].copy()\n",
        "px_daily_all[\"date\"] = pd.to_datetime(px_daily_all[\"date\"])\n",
        "dates_all = px_daily_all[[\"date\"]].drop_duplicates().sort_values(\"date\")\n",
        "\n",
        "def _tidy_quarterly_df(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    if \"date\" in df.columns:\n",
        "        df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
        "    elif \"fillingDate\" in df.columns:\n",
        "        df[\"date\"] = pd.to_datetime(df[\"fillingDate\"])\n",
        "    return df\n",
        "\n",
        "def _coalesce_cols(df: pd.DataFrame, cols: list[str], default=np.nan) -> pd.Series:\n",
        "    avail = [c for c in cols if c in df.columns]\n",
        "    if not avail:\n",
        "        return pd.Series(default, index=df.index)\n",
        "    tmp = df[avail].apply(pd.to_numeric, errors=\"coerce\")\n",
        "    # first non-null across the candidate columns\n",
        "    s = tmp.bfill(axis=1).iloc[:, 0]\n",
        "    return s\n",
        "\n",
        "def _fetch_quarterly_funda_fmp(ticker: str) -> pd.DataFrame:\n",
        "    import requests\n",
        "    base = \"https://financialmodelingprep.com/api/v3\"\n",
        "    fmp_ticker = to_fmp_symbol(ticker)   # BRK-B -> BRK.B\n",
        "\n",
        "    def jget(url):\n",
        "        r = requests.get(url, timeout=30)\n",
        "        r.raise_for_status()\n",
        "        return r.json()\n",
        "\n",
        "    # pull a long history (Premium supports it)\n",
        "    bs = _tidy_quarterly_df(pd.DataFrame(jget(f\"{base}/balance-sheet-statement/{fmp_ticker}?period=quarter&limit=120&apikey={FMP_API_KEY}\")))\n",
        "    is_ = _tidy_quarterly_df(pd.DataFrame(jget(f\"{base}/income-statement/{fmp_ticker}?period=quarter&limit=120&apikey={FMP_API_KEY}\")))\n",
        "    cf = _tidy_quarterly_df(pd.DataFrame(jget(f\"{base}/cash-flow-statement/{fmp_ticker}?period=quarter&limit=120&apikey={FMP_API_KEY}\")))\n",
        "    if bs.empty and is_.empty and cf.empty:\n",
        "        raise RuntimeError(f\"FMP fundamentals empty for {ticker} (queried as {fmp_ticker})\")\n",
        "\n",
        "    out = bs.merge(is_, on=\"date\", how=\"outer\").merge(cf, on=\"date\", how=\"outer\")\n",
        "\n",
        "    # Coalesce across schema variants\n",
        "    out[\"book_equity\"]  = _coalesce_cols(out, [\"totalStockholdersEquity\",\"totalShareholderEquity\",\"totalEquity\"]).astype(float)\n",
        "    out[\"net_income\"]   = _coalesce_cols(out, [\"netIncome\",\"netIncomeApplicableToCommonShares\"]).astype(float)\n",
        "    out[\"ocf\"]          = _coalesce_cols(out, [\n",
        "        \"netCashProvidedByOperatingActivities\",\n",
        "        \"netCashProvidedByUsedInOperatingActivities\",\n",
        "        \"netCashProvidedByUsedInOperatingActivitiesContinuingOperations\"\n",
        "    ]).astype(float)\n",
        "    out[\"gross_profit\"] = _coalesce_cols(out, [\"grossProfit\"]).astype(float)\n",
        "    out[\"total_assets\"] = _coalesce_cols(out, [\"totalAssets\"]).astype(float)\n",
        "\n",
        "    # total_debt: prefer totalDebt; else short + long\n",
        "    td = _coalesce_cols(out, [\"totalDebt\"])\n",
        "    if td.isna().all():\n",
        "        short = _coalesce_cols(out, [\"shortTermDebt\",\"shortLongTermDebtTotal\"])\n",
        "        long  = _coalesce_cols(out, [\"longTermDebt\"])\n",
        "        td = (short.fillna(0) + long.fillna(0)).replace({0: np.nan})\n",
        "    out[\"total_debt\"] = td.astype(float)\n",
        "\n",
        "    # dividends / buybacks (raw signs as provided by FMP)\n",
        "    out[\"dividends\"] = _coalesce_cols(out, [\"dividendsPaid\",\"dividendsPaidCashFlow\"]).astype(float)\n",
        "    out[\"buybacks\"]  = _coalesce_cols(out, [\"commonStockRepurchased\",\"purchaseOfCommonStock\"]).astype(float)\n",
        "\n",
        "    out[\"ticker\"] = ticker  # keep Yahoo-style symbol for our dataset\n",
        "\n",
        "    cols = [\"date\",\"ticker\",\"book_equity\",\"net_income\",\"ocf\",\"gross_profit\",\n",
        "            \"total_assets\",\"total_debt\",\"dividends\",\"buybacks\"]\n",
        "    return out[cols].dropna(subset=[\"date\"])\n",
        "\n",
        "\n",
        "def fetch_or_load_cached_quarterly(ticker: str) -> pd.DataFrame | None:\n",
        "    path = os.path.join(CACHE_DIR, f\"funda_q_{ticker}.parquet\")\n",
        "    if SKIP_IF_CACHED and os.path.exists(path):\n",
        "        try:\n",
        "            return pd.read_parquet(path)\n",
        "        except Exception:\n",
        "            pass\n",
        "    try:\n",
        "        df = _fetch_quarterly_funda_fmp(ticker)\n",
        "        if df is None or df.empty:\n",
        "            return None\n",
        "        df.to_parquet(path, index=False)\n",
        "        time.sleep(random.uniform(*BATCH_SLEEP))  # polite pause\n",
        "        return df\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "# ---- SMOKE TEST (run once, then you can comment it out) ----\n",
        "try:\n",
        "    from IPython.display import display\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "test_syms = [\"AAPL\", \"MSFT\", \"BRK-B\", \"BF-B\"]\n",
        "for s in test_syms:\n",
        "    try:\n",
        "        df = _fetch_quarterly_funda_fmp(s)\n",
        "        # 👇 trim preview to match your price history\n",
        "        df = df[df[\"date\"] >= pd.to_datetime(START_DATE)]\n",
        "        print(s, \"→\", to_fmp_symbol(s), \"rows:\", len(df))\n",
        "        try:\n",
        "            display(df.head(2))\n",
        "        except Exception:\n",
        "            print(df.head(2))\n",
        "    except Exception as e:\n",
        "        print(\"ERR\", s, e)\n",
        "\n",
        "# Determine chunk (or all)\n",
        "if CHUNK_TICKERS:\n",
        "    end_at = min(len(universe_full), START_AT + CHUNK_TICKERS)\n",
        "    tickers_chunk = sorted(universe_full[START_AT:end_at])\n",
        "    print(f\"[FMP] Processing chunk {START_AT}:{end_at} (size={len(tickers_chunk)})\")\n",
        "else:\n",
        "    tickers_chunk = sorted(universe_full)\n",
        "    print(f\"[FMP] Processing entire universe (size={len(tickers_chunk)})\")\n",
        "\n",
        "# Parallel fetch with caching\n",
        "funda_parts, successes = [], 0\n",
        "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
        "    futs = {ex.submit(fetch_or_load_cached_quarterly, t): t for t in tickers_chunk}\n",
        "    for j, fut in enumerate(as_completed(futs), start=1):\n",
        "        df = fut.result()\n",
        "        if df is not None and len(df):\n",
        "            funda_parts.append(df)\n",
        "            successes += 1\n",
        "        if j % 25 == 0:\n",
        "            print(f\"[Fundamentals/FMP] {j}/{len(tickers_chunk)} processed… (successes: {successes})\")\n",
        "\n",
        "if successes == 0:\n",
        "    raise RuntimeError(\"No fundamentals fetched. Check your FMP key or try a smaller CHUNK_TICKERS with fewer workers.\")\n",
        "\n",
        "# Merge chunk with existing quarterly file (so multiple runs accumulate)\n",
        "q_path = \"funda_quarterly.parquet\"\n",
        "funda_q_chunk = pd.concat(funda_parts, ignore_index=True).sort_values([\"ticker\",\"date\"])\n",
        "\n",
        "# after you build funda_q_chunk\n",
        "cutoff = pd.to_datetime(px_daily_all[\"date\"].min())\n",
        "funda_q_chunk = funda_q_chunk[funda_q_chunk[\"date\"] >= cutoff]\n",
        "\n",
        "if os.path.exists(q_path):\n",
        "    old = pd.read_parquet(q_path)\n",
        "    old[\"date\"] = pd.to_datetime(old[\"date\"])\n",
        "    old = old[old[\"date\"] >= cutoff]  # <- trim the old file too\n",
        "    funda_q = (\n",
        "        pd.concat([old, funda_q_chunk], ignore_index=True)\n",
        "          .drop_duplicates([\"ticker\",\"date\"], keep=\"last\")\n",
        "          .sort_values([\"ticker\",\"date\"])\n",
        "    )\n",
        "else:\n",
        "    funda_q = funda_q_chunk\n",
        "\n",
        "funda_q = funda_q.sort_values([\"ticker\",\"date\"])\n",
        "funda_q.to_parquet(q_path, index=False)\n",
        "print(\n",
        "    f\"Saved: {q_path}  \"\n",
        "    f\"(tickers with funda total: {funda_q['ticker'].nunique()}, \"\n",
        "    f\"rows: {len(funda_q)})\"\n",
        ")\n",
        "\n",
        "# Quarterly → daily (forward-fill per ticker) across ALL tickers collected so far\n",
        "ff = []\n",
        "for sym, grp in funda_q.groupby(\"ticker\"):\n",
        "    g = dates_all.merge(grp, on=\"date\", how=\"left\")\n",
        "    g[\"ticker\"] = sym\n",
        "    g = g.sort_values(\"date\").ffill()\n",
        "    ff.append(g)\n",
        "funda_daily = pd.concat(ff, ignore_index=True)\n",
        "\n",
        "# Ratios (Value & Quality)\n",
        "fd = funda_daily.merge(px_daily_all, on=[\"date\",\"ticker\"], how=\"left\")\n",
        "price = fd[\"adj_close\"].replace(0, np.nan)\n",
        "\n",
        "fd[\"book_to_price\"]     = fd[\"book_equity\"] / price\n",
        "fd[\"earnings_yield\"]    = fd[\"net_income\"]  / price\n",
        "fd[\"cf_yield\"]          = fd[\"ocf\"]         / price\n",
        "fd[\"shareholder_yield\"] = (fd[\"dividends\"].fillna(0) * -1 + fd[\"buybacks\"].fillna(0)) / price\n",
        "\n",
        "fd[\"gross_profitability\"] = fd[\"gross_profit\"] / fd[\"total_assets\"].replace(0, np.nan)\n",
        "fd[\"roe\"]                 = fd[\"net_income\"] / fd[\"book_equity\"].replace(0, np.nan)\n",
        "fd[\"accruals\"]            = (fd[\"net_income\"] - fd[\"ocf\"]) / fd[\"total_assets\"].replace(0, np.nan)\n",
        "fd[\"leverage\"]            = fd[\"total_debt\"] / fd[\"total_assets\"].replace(0, np.nan)\n",
        "\n",
        "funda_daily = fd[[\n",
        "    \"date\",\"ticker\",\"book_to_price\",\"earnings_yield\",\"cf_yield\",\"shareholder_yield\",\n",
        "    \"gross_profitability\",\"roe\",\"accruals\",\"leverage\"\n",
        "]]\n",
        "funda_daily.to_parquet(\"funda_daily.parquet\", index=False)\n",
        "print(\"Saved: funda_daily.parquet\")\n",
        "\n",
        "# Merge fundamentals into features\n",
        "features = features.merge(funda_daily, on=[\"date\",\"ticker\"], how=\"left\")\n",
        "\n",
        "# ============================================================\n",
        "# D) POST-MERGE HYGIENE\n",
        "# ============================================================\n",
        "\n",
        "# 1) Leakage control\n",
        "non_feature_cols = {\"date\",\"ticker\",\"open\",\"high\",\"low\",\"close\",\"adj_close\",\"volume\"}\n",
        "cols_to_shift = [c for c in features.columns if c not in non_feature_cols]\n",
        "features[cols_to_shift] = features.groupby(\"ticker\")[cols_to_shift].shift(1)\n",
        "\n",
        "# 2) Winsorize & cross-sectional z-score\n",
        "def winsorize_cs(s, lo=0.01, hi=0.99):\n",
        "    ql, qh = s.quantile(lo), s.quantile(hi)\n",
        "    return s.clip(ql, qh)\n",
        "\n",
        "# --- choose features for cross-sectional standardization (exclude context & raw SMAs) ---\n",
        "cs_cols = [\n",
        "    \"rv_20\",\"atr_14\",\"mom_20\",\"mom_6m\",\"mom_12m\",\"mom_12_1\",\"mom_6_1\",\n",
        "    \"sma_20_gt_50\",\"slope_20\",\"mom_20_vs_vol\",\n",
        "    # fundamentals\n",
        "    \"book_to_price\",\"earnings_yield\",\"cf_yield\",\"shareholder_yield\",\n",
        "    \"gross_profitability\",\"roe\",\"accruals\",\"leverage\"\n",
        "] + [f\"ret_lag_{l}\" for l in range(1,61)]\n",
        "\n",
        "# keep context raw (no CS z-score)\n",
        "context_keep_raw = [\"spy_rv_20\",\"vix_close\",\"breadth\"]\n",
        "\n",
        "present = [c for c in cs_cols if c in features.columns]\n",
        "\n",
        "print(f\"[Standardize] Cross-sectional z-score on {len(present)} features\")\n",
        "\n",
        "def cs_standardize_fast(df, cols, lo=0.01, hi=0.99):\n",
        "    out = df.copy()\n",
        "    out[cols] = out[cols].astype(\"float32\")\n",
        "\n",
        "    d = out[\"date\"]\n",
        "    for c in cols:\n",
        "        s = out[c]\n",
        "\n",
        "        ql = s.groupby(d).transform(lambda x: x.quantile(lo))\n",
        "        qh = s.groupby(d).transform(lambda x: x.quantile(hi))\n",
        "        s_clip = s.clip(ql, qh)\n",
        "\n",
        "        mu = s_clip.groupby(d).transform(\"mean\")\n",
        "        sd = s_clip.groupby(d).transform(\"std\")\n",
        "\n",
        "        # if std is 0 or NaN (date-constant or all-NaN), set denom=1 to avoid blowing up / NaNs\n",
        "        denom = sd.fillna(0.0).replace(0.0, 1.0)\n",
        "\n",
        "        out[c] = ((s_clip - mu) / (denom + 1e-9)).astype(\"float32\")\n",
        "\n",
        "    return out\n",
        "\n",
        "features = cs_standardize_fast(features, present)\n",
        "\n",
        "# 3) Fundamentals imputation + masks\n",
        "funda_cols = [\"book_to_price\",\"earnings_yield\",\"cf_yield\",\"shareholder_yield\",\n",
        "              \"gross_profitability\",\"roe\",\"accruals\",\"leverage\"]\n",
        "for c in funda_cols:\n",
        "    if c in features.columns:\n",
        "        features[f\"{c}_is_missing\"] = features[c].isna().astype(int)\n",
        "        features[c] = features.groupby(\"date\")[c].transform(lambda s: s.fillna(s.median()))\n",
        "\n",
        "# Save final\n",
        "features.to_parquet(\"features.parquet\", index=False)\n",
        "print(\"Saved: features.parquet (lagged, winsorized, cross-sectional z-scored)\")\n",
        "print(\"Artifacts: funda_quarterly.parquet, funda_daily.parquet, cache/funda_q_*.parquet\")\n",
        "\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 998
        },
        "id": "5qa3M9pAzvtv",
        "outputId": "8134a8e4-b36a-4e9f-ce95-045d76ecffb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your FMP API key (kept in-memory for this session): ··········\n",
            "[Features] 25/515 processed… (AMAT)\n",
            "[Features] 50/515 processed… (BA)\n",
            "[Features] 75/515 processed… (CARR)\n",
            "[Features] 100/515 processed… (CNP)\n",
            "[Features] 125/515 processed… (DAL)\n",
            "[Features] 150/515 processed… (EA)\n",
            "[Features] 175/515 processed… (EXE)\n",
            "[Features] 200/515 processed… (GEHC)\n",
            "[Features] 225/515 processed… (HON)\n",
            "[Features] 250/515 processed… (IT)\n",
            "[Features] 275/515 processed… (LDOS)\n",
            "[Features] 300/515 processed… (MCO)\n",
            "[Features] 325/515 processed… (MTCH)\n",
            "[Features] 350/515 processed… (OKE)\n",
            "[Features] 375/515 processed… (PNR)\n",
            "[Features] 400/515 processed… (RSG)\n",
            "[Features] 425/515 processed… (SWKS)\n",
            "[Features] 450/515 processed… (TSN)\n",
            "[Features] 475/515 processed… (VST)\n",
            "[Features] 500/515 processed… (XLF)\n",
            "AAPL → AAPL rows: 78\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "         date ticker   book_equity  net_income           ocf  gross_profit  \\\n",
              "42 2006-04-01   AAPL  8.682000e+09         NaN -1.250000e+08  1.297000e+09   \n",
              "43 2006-07-01   AAPL  9.330000e+09         NaN  1.007000e+09  1.325000e+09   \n",
              "\n",
              "    total_assets  total_debt  dividends   buybacks  \n",
              "42  1.391100e+10         0.0        0.0        0.0  \n",
              "43  1.511400e+10         0.0        0.0 -1000000.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bef3319f-6130-4121-8e35-34fb86dcaf63\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>ticker</th>\n",
              "      <th>book_equity</th>\n",
              "      <th>net_income</th>\n",
              "      <th>ocf</th>\n",
              "      <th>gross_profit</th>\n",
              "      <th>total_assets</th>\n",
              "      <th>total_debt</th>\n",
              "      <th>dividends</th>\n",
              "      <th>buybacks</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>2006-04-01</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>8.682000e+09</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-1.250000e+08</td>\n",
              "      <td>1.297000e+09</td>\n",
              "      <td>1.391100e+10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>2006-07-01</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>9.330000e+09</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.007000e+09</td>\n",
              "      <td>1.325000e+09</td>\n",
              "      <td>1.511400e+10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1000000.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bef3319f-6130-4121-8e35-34fb86dcaf63')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-bef3319f-6130-4121-8e35-34fb86dcaf63 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-bef3319f-6130-4121-8e35-34fb86dcaf63');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-4362dc17-de34-4654-b296-eafb1cd3a7f9\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4362dc17-de34-4654-b296-eafb1cd3a7f9')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-4362dc17-de34-4654-b296-eafb1cd3a7f9 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"gc\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"date\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2006-04-01 00:00:00\",\n        \"max\": \"2006-07-01 00:00:00\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"2006-07-01 00:00:00\",\n          \"2006-04-01 00:00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ticker\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"AAPL\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"book_equity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 458205194.2088828,\n        \"min\": 8682000000.0,\n        \"max\": 9330000000.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          9330000000.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"net_income\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ocf\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 800444876.3031718,\n        \"min\": -125000000.0,\n        \"max\": 1007000000.0,\n        \"num_unique_values\": 2,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"gross_profit\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 19798989.87322333,\n        \"min\": 1297000000.0,\n        \"max\": 1325000000.0,\n        \"num_unique_values\": 2,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"total_assets\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 850649457.7674167,\n        \"min\": 13911000000.0,\n        \"max\": 15114000000.0,\n        \"num_unique_values\": 2,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"total_debt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dividends\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"buybacks\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 707106.7811865475,\n        \"min\": -1000000.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 2,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSFT → MSFT rows: 78\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "         date ticker   book_equity  net_income           ocf  gross_profit  \\\n",
              "42 2006-03-31   MSFT  4.203800e+10         NaN  4.563000e+09  8.872000e+09   \n",
              "43 2006-06-30   MSFT  4.010400e+10         NaN  3.281000e+09  9.674000e+09   \n",
              "\n",
              "    total_assets  total_debt    dividends      buybacks  \n",
              "42  6.685400e+10         0.0 -925000000.0 -4.675000e+09  \n",
              "43  6.959700e+10         0.0 -917000000.0 -3.981000e+09  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cac3e1fc-334a-4d90-876e-88c1a4bca4ad\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>ticker</th>\n",
              "      <th>book_equity</th>\n",
              "      <th>net_income</th>\n",
              "      <th>ocf</th>\n",
              "      <th>gross_profit</th>\n",
              "      <th>total_assets</th>\n",
              "      <th>total_debt</th>\n",
              "      <th>dividends</th>\n",
              "      <th>buybacks</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>2006-03-31</td>\n",
              "      <td>MSFT</td>\n",
              "      <td>4.203800e+10</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.563000e+09</td>\n",
              "      <td>8.872000e+09</td>\n",
              "      <td>6.685400e+10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-925000000.0</td>\n",
              "      <td>-4.675000e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>2006-06-30</td>\n",
              "      <td>MSFT</td>\n",
              "      <td>4.010400e+10</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.281000e+09</td>\n",
              "      <td>9.674000e+09</td>\n",
              "      <td>6.959700e+10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-917000000.0</td>\n",
              "      <td>-3.981000e+09</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cac3e1fc-334a-4d90-876e-88c1a4bca4ad')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-cac3e1fc-334a-4d90-876e-88c1a4bca4ad button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-cac3e1fc-334a-4d90-876e-88c1a4bca4ad');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-fce8a1be-2142-446f-90f2-33a9ebabdcff\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-fce8a1be-2142-446f-90f2-33a9ebabdcff')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-fce8a1be-2142-446f-90f2-33a9ebabdcff button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"gc\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"date\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2006-03-31 00:00:00\",\n        \"max\": \"2006-06-30 00:00:00\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"2006-06-30 00:00:00\",\n          \"2006-03-31 00:00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ticker\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"MSFT\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"book_equity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1367544514.8147829,\n        \"min\": 40104000000.0,\n        \"max\": 42038000000.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          40104000000.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"net_income\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ocf\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 906510893.481154,\n        \"min\": 3281000000.0,\n        \"max\": 4563000000.0,\n        \"num_unique_values\": 2,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"gross_profit\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 567099638.5116111,\n        \"min\": 8872000000.0,\n        \"max\": 9674000000.0,\n        \"num_unique_values\": 2,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"total_assets\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1939593900.7947,\n        \"min\": 66854000000.0,\n        \"max\": 69597000000.0,\n        \"num_unique_values\": 2,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"total_debt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dividends\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5656854.24949238,\n        \"min\": -925000000.0,\n        \"max\": -917000000.0,\n        \"num_unique_values\": 2,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"buybacks\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 490732106.14346397,\n        \"min\": -4675000000.0,\n        \"max\": -3981000000.0,\n        \"num_unique_values\": 2,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BRK-B → BRK.B rows: 78\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "         date ticker   book_equity  net_income           ocf  gross_profit  \\\n",
              "42 2006-03-31  BRK-B  9.534900e+10         NaN  2.359000e+09  6.162000e+09   \n",
              "43 2006-06-30  BRK-B  9.761300e+10         NaN  1.092000e+09  8.197000e+09   \n",
              "\n",
              "    total_assets    total_debt  dividends  buybacks  \n",
              "42  2.302060e+11  3.047900e+10        0.0       0.0  \n",
              "43  2.323310e+11  3.055700e+10        0.0       0.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-57a8485a-28dc-4600-a204-b248d42f37a1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>ticker</th>\n",
              "      <th>book_equity</th>\n",
              "      <th>net_income</th>\n",
              "      <th>ocf</th>\n",
              "      <th>gross_profit</th>\n",
              "      <th>total_assets</th>\n",
              "      <th>total_debt</th>\n",
              "      <th>dividends</th>\n",
              "      <th>buybacks</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>2006-03-31</td>\n",
              "      <td>BRK-B</td>\n",
              "      <td>9.534900e+10</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.359000e+09</td>\n",
              "      <td>6.162000e+09</td>\n",
              "      <td>2.302060e+11</td>\n",
              "      <td>3.047900e+10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>2006-06-30</td>\n",
              "      <td>BRK-B</td>\n",
              "      <td>9.761300e+10</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.092000e+09</td>\n",
              "      <td>8.197000e+09</td>\n",
              "      <td>2.323310e+11</td>\n",
              "      <td>3.055700e+10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-57a8485a-28dc-4600-a204-b248d42f37a1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-57a8485a-28dc-4600-a204-b248d42f37a1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-57a8485a-28dc-4600-a204-b248d42f37a1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-999c6759-bf6d-4448-bb03-793a1c8710b3\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-999c6759-bf6d-4448-bb03-793a1c8710b3')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-999c6759-bf6d-4448-bb03-793a1c8710b3 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"gc\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"date\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2006-03-31 00:00:00\",\n        \"max\": \"2006-06-30 00:00:00\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"2006-06-30 00:00:00\",\n          \"2006-03-31 00:00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ticker\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"BRK-B\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"book_equity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1600889752.6063435,\n        \"min\": 95349000000.0,\n        \"max\": 97613000000.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          97613000000.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"net_income\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ocf\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 895904291.7633557,\n        \"min\": 1092000000.0,\n        \"max\": 2359000000.0,\n        \"num_unique_values\": 2,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"gross_profit\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1438962299.7146242,\n        \"min\": 6162000000.0,\n        \"max\": 8197000000.0,\n        \"num_unique_values\": 2,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"total_assets\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1502601910.0214136,\n        \"min\": 230206000000.0,\n        \"max\": 232331000000.0,\n        \"num_unique_values\": 2,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"total_debt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 55154328.932550706,\n        \"min\": 30479000000.0,\n        \"max\": 30557000000.0,\n        \"num_unique_values\": 2,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dividends\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"buybacks\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERR BF-B FMP fundamentals empty for BF-B (queried as BF.B)\n",
            "[FMP] Processing chunk 0:20 (size=20)\n",
            "Saved: funda_quarterly.parquet  (tickers with funda total: 20, rows: 1487)\n",
            "Saved: funda_daily.parquet\n",
            "[Standardize] Cross-sectional z-score on 78 features\n",
            "Saved: features.parquet (lagged, winsorized, cross-sectional z-scored)\n",
            "Artifacts: funda_quarterly.parquet, funda_daily.parquet, cache/funda_q_*.parquet\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Feature Inventory & Dictionary ===\n",
        "# Point this to your artifact; adjust if you saved elsewhere.\n",
        "FEATURES_PATH = \"features.parquet\"\n",
        "\n",
        "import os, re, pandas as pd\n",
        "\n",
        "if not os.path.exists(FEATURES_PATH):\n",
        "    raise FileNotFoundError(f\"Could not find {FEATURES_PATH}. \"\n",
        "                            \"Run your Section 1.2 pipeline first or update FEATURES_PATH.\")\n",
        "\n",
        "df = pd.read_parquet(FEATURES_PATH)\n",
        "\n",
        "# Columns that are not predictive \"features\" (aligns with your Section 1.2 script)\n",
        "NON_FEATURE_COLS = {\"date\",\"ticker\",\"open\",\"high\",\"low\",\"close\",\"adj_close\",\"volume\"}\n",
        "\n",
        "# Known groups (from your build)\n",
        "CONTEXT_COLS = {\"spy_rv_20\",\"vix_close\",\"breadth\"}\n",
        "TECH_BASE = {\n",
        "    \"ret_1d\",\"rv_20\",\"atr_14\",\"mom_20\",\"mom_6m\",\"mom_12m\",\"mom_12_1\",\"mom_6_1\",\n",
        "    \"sma_20\",\"sma_50\",\"sma_20_gt_50\",\"slope_20\",\"mom_20_vs_vol\"\n",
        "}\n",
        "FUNDAMENTAL_COLS = {\n",
        "    \"book_to_price\",\"earnings_yield\",\"cf_yield\",\"shareholder_yield\",\n",
        "    \"gross_profitability\",\"roe\",\"accruals\",\"leverage\"\n",
        "}\n",
        "\n",
        "# 1) Figure out which columns are in the file\n",
        "all_cols = list(df.columns)\n",
        "feature_cols = [c for c in all_cols if c not in NON_FEATURE_COLS]\n",
        "\n",
        "# 2) Detect buckets\n",
        "lags = sorted([c for c in feature_cols if re.fullmatch(r\"ret_lag_\\d+\", c)], key=lambda x: int(x.split(\"_\")[-1]))\n",
        "context = [c for c in feature_cols if c in CONTEXT_COLS]\n",
        "fundas = [c for c in feature_cols if c in FUNDAMENTAL_COLS]\n",
        "funda_masks = sorted([c for c in feature_cols if c.endswith(\"_is_missing\") and c.replace(\"_is_missing\",\"\") in FUNDAMENTAL_COLS])\n",
        "\n",
        "# Technicals include the base tech set + anything that looks like SMA/trend/ATR/vol/momentum beyond the lags\n",
        "tech_known = sorted([c for c in feature_cols if c in TECH_BASE])\n",
        "# capture any extra tech-style columns you might add later (prefix match heuristics)\n",
        "TECH_PREFIXES = (\"rv_\", \"atr_\", \"mom_\", \"sma_\", \"slope_\")\n",
        "tech_extra = sorted([\n",
        "    c for c in feature_cols\n",
        "    if c not in tech_known\n",
        "    and c not in lags\n",
        "    and c not in context\n",
        "    and (c.startswith(TECH_PREFIXES) or c in {\"ret_1d\"})\n",
        "])\n",
        "\n",
        "# 3) Anything not covered falls into \"other\"\n",
        "covered = set(lags) | set(context) | set(fundas) | set(funda_masks) | set(tech_known) | set(tech_extra)\n",
        "other = sorted([c for c in feature_cols if c not in covered])\n",
        "\n",
        "# 4) Build a compact report\n",
        "def hdr(title, items):\n",
        "    return f\"{title} ({len(items)}):\\n\" + (\", \".join(items) if items else \"—\")\n",
        "\n",
        "report = \"\\n\".join([\n",
        "    f\"Total columns: {len(all_cols)}\",\n",
        "    f\"Predictive feature columns: {len(feature_cols)}\",\n",
        "    \"\",\n",
        "    hdr(\"Market context\", context),\n",
        "    hdr(\"Price/technical (known)\", tech_known),\n",
        "    hdr(\"Price/technical (extra detected)\", tech_extra),\n",
        "    hdr(\"Return lags\", lags),\n",
        "    hdr(\"Fundamentals\", fundas),\n",
        "    hdr(\"Fundamentals — missing masks\", funda_masks),\n",
        "    hdr(\"Other\", other),\n",
        "])\n",
        "\n",
        "print(report)\n",
        "\n",
        "# 5) Also emit a Markdown dictionary to disk for teammates\n",
        "def sample_stats(cols):\n",
        "    if not cols: return \"\"\n",
        "    sub = df[cols]\n",
        "    # % non-null and dtype summary\n",
        "    nn = sub.notna().mean().rename(\"non_null_frac\")\n",
        "    dtypes = sub.dtypes.rename(\"dtype\").astype(str)\n",
        "    return pd.concat([dtypes, nn.round(4)], axis=1).sort_index()\n",
        "\n",
        "sections = [\n",
        "    (\"Market context\", context),\n",
        "    (\"Price/technical (known)\", tech_known),\n",
        "    (\"Price/technical (extra detected)\", tech_extra),\n",
        "    (\"Return lags\", lags),\n",
        "    (\"Fundamentals\", fundas),\n",
        "    (\"Fundamentals — missing masks\", funda_masks),\n",
        "    (\"Other\", other),\n",
        "]\n",
        "\n",
        "lines = [\"# Feature Dictionary\\n\",\n",
        "         f\"- Source file: `{FEATURES_PATH}`\",\n",
        "         f\"- Total columns: **{len(all_cols)}**\",\n",
        "         f\"- Predictive feature columns: **{len(feature_cols)}**\",\n",
        "         \"\"]\n",
        "\n",
        "for title, cols in sections:\n",
        "    lines.append(f\"## {title} ({len(cols)})\")\n",
        "    if cols:\n",
        "        lines.append(\", \".join(cols))\n",
        "        stats = sample_stats(cols)\n",
        "        lines.append(\"\\n<details><summary>Schema & coverage</summary>\\n\\n\")\n",
        "        lines.append(stats.to_markdown())\n",
        "        lines.append(\"\\n</details>\\n\")\n",
        "    else:\n",
        "        lines.append(\"—\\n\")\n",
        "\n",
        "md_path = \"feature_dictionary.md\"\n",
        "with open(md_path, \"w\") as f:\n",
        "    f.write(\"\\n\".join(lines))\n",
        "\n",
        "print(f\"\\nSaved detailed dictionary → {md_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IUXCfU6FE4g",
        "outputId": "bc5a39db-349c-4f04-a967-3d40dd903083"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total columns: 100\n",
            "Predictive feature columns: 92\n",
            "\n",
            "Market context (3):\n",
            "spy_rv_20, vix_close, breadth\n",
            "Price/technical (known) (13):\n",
            "atr_14, mom_12_1, mom_12m, mom_20, mom_20_vs_vol, mom_6_1, mom_6m, ret_1d, rv_20, slope_20, sma_20, sma_20_gt_50, sma_50\n",
            "Price/technical (extra detected) (0):\n",
            "—\n",
            "Return lags (60):\n",
            "ret_lag_1, ret_lag_2, ret_lag_3, ret_lag_4, ret_lag_5, ret_lag_6, ret_lag_7, ret_lag_8, ret_lag_9, ret_lag_10, ret_lag_11, ret_lag_12, ret_lag_13, ret_lag_14, ret_lag_15, ret_lag_16, ret_lag_17, ret_lag_18, ret_lag_19, ret_lag_20, ret_lag_21, ret_lag_22, ret_lag_23, ret_lag_24, ret_lag_25, ret_lag_26, ret_lag_27, ret_lag_28, ret_lag_29, ret_lag_30, ret_lag_31, ret_lag_32, ret_lag_33, ret_lag_34, ret_lag_35, ret_lag_36, ret_lag_37, ret_lag_38, ret_lag_39, ret_lag_40, ret_lag_41, ret_lag_42, ret_lag_43, ret_lag_44, ret_lag_45, ret_lag_46, ret_lag_47, ret_lag_48, ret_lag_49, ret_lag_50, ret_lag_51, ret_lag_52, ret_lag_53, ret_lag_54, ret_lag_55, ret_lag_56, ret_lag_57, ret_lag_58, ret_lag_59, ret_lag_60\n",
            "Fundamentals (8):\n",
            "book_to_price, earnings_yield, cf_yield, shareholder_yield, gross_profitability, roe, accruals, leverage\n",
            "Fundamentals — missing masks (8):\n",
            "accruals_is_missing, book_to_price_is_missing, cf_yield_is_missing, earnings_yield_is_missing, gross_profitability_is_missing, leverage_is_missing, roe_is_missing, shareholder_yield_is_missing\n",
            "Other (0):\n",
            "—\n",
            "\n",
            "Saved detailed dictionary → feature_dictionary.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 1.3 DATA HYGIENE / QC (non-destructive)\n",
        "# ------------------------------------------------------------\n",
        "# - Summarize coverage & missingness (post 1.2)\n",
        "# - Optional pruning: drop early warmup dates & low-coverage dates\n",
        "# - Write meta.yaml and QC CSVs\n",
        "# ============================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yaml\n",
        "\n",
        "FEATURES_PATH = \"features.parquet\"\n",
        "UNIVERSE_PATH = \"universe.csv\"\n",
        "\n",
        "features = pd.read_parquet(FEATURES_PATH)\n",
        "universe_df = pd.read_csv(UNIVERSE_PATH)\n",
        "\n",
        "# ---------- QC: basics ----------\n",
        "min_date = pd.to_datetime(features[\"date\"]).min()\n",
        "max_date = pd.to_datetime(features[\"date\"]).max()\n",
        "n_rows = len(features)\n",
        "n_tickers = features[\"ticker\"].nunique()\n",
        "\n",
        "# Columns we standardized in 1.2 (will exist if 1.2 ran)\n",
        "feature_cols = [c for c in features.columns\n",
        "                if c not in {\"date\",\"ticker\",\"open\",\"high\",\"low\",\"close\",\"adj_close\",\"volume\"}]\n",
        "\n",
        "# Per-column missingness (after 1.2; should be low except earliest windows)\n",
        "missing_pct = (1.0 - features[feature_cols].notna().mean()).sort_values(ascending=False)\n",
        "missing_pct.to_csv(\"qc_missing_by_feature.csv\", header=[\"missing_pct\"])\n",
        "\n",
        "# Coverage by date (# of tickers with at least 1 valid feature on that date)\n",
        "valid_any = features[feature_cols].notna().sum(axis=1) > 0\n",
        "coverage_by_date = (features.assign(valid_any=valid_any)\n",
        "                             .groupby(\"date\")[\"ticker\"]\n",
        "                             .nunique()\n",
        "                             .rename(\"n_tickers\"))\n",
        "coverage_by_date.to_csv(\"qc_coverage_by_date.csv\")\n",
        "\n",
        "# ---------- Optional: pruning rules (non-destructive by default) ----------\n",
        "# 1) Warmup: many features need long windows (max ≈ 252 + 21). Keep dates after first 273 trading days.\n",
        "#    We'll infer a warmup cutoff from SPY availability to be robust.\n",
        "spy_dates = features.loc[features[\"ticker\"]==\"SPY\", \"date\"].sort_values().unique()\n",
        "if len(spy_dates) > 300:\n",
        "    warmup_cutoff = pd.to_datetime(spy_dates[min(273, len(spy_dates)-1)])\n",
        "else:\n",
        "    warmup_cutoff = min_date  # fallback\n",
        "\n",
        "# 2) Low coverage: drop dates with very few names (e.g., <300) — tweak if you want.\n",
        "COVERAGE_MIN = 300\n",
        "low_cov_dates = coverage_by_date[coverage_by_date < COVERAGE_MIN].index\n",
        "\n",
        "# We don’t mutate features here; write a recommended mask so training can filter.\n",
        "date_mask_keep = (~pd.Series(features[\"date\"]).isin(low_cov_dates)) & (features[\"date\"] >= warmup_cutoff)\n",
        "keep_rate = date_mask_keep.mean()\n",
        "pd.DataFrame({\n",
        "    \"warmup_cutoff\":[warmup_cutoff],\n",
        "    \"coverage_min\":[COVERAGE_MIN],\n",
        "    \"keep_rate\":[float(keep_rate)]\n",
        "}).to_csv(\"qc_recommendations.csv\", index=False)\n",
        "\n",
        "# ---------- Meta ----------\n",
        "meta = {\n",
        "    \"universe\": {\n",
        "        \"description\": \"S&P 500 (current constituents; survivorship bias acknowledged).\",\n",
        "        \"count\": int(len(universe_df)),\n",
        "        \"hedges\": [\"SPY\",\"XLY\",\"XLF\",\"XLV\",\"XLK\",\"XLI\",\"XLE\",\"XLP\",\"XLB\",\"XLU\",\"XLRE\"],\n",
        "        \"context_symbols\": [\"^VIX\"],\n",
        "        \"lookback\": {\"start\": str(min_date.date()), \"end\": str(max_date.date())}\n",
        "    },\n",
        "    \"pricing\": {\n",
        "        \"source\": \"Yahoo Finance via yfinance\",\n",
        "        \"adjusted_prices_used\": True,\n",
        "        \"file\": \"raw_prices.parquet\"\n",
        "    },\n",
        "    \"features\": {\n",
        "        \"file\": \"features.parquet\",\n",
        "        \"rows\": int(n_rows),\n",
        "        \"tickers\": int(n_tickers),\n",
        "        \"leakage_control\": \"All predictive features shifted by 1 day.\",\n",
        "        \"cross_sectional_processing\": \"Winsorized [1%,99%] & z-scored by date (see 1.2).\",\n",
        "        \"imputation\": \"Fundamentals imputed (cross-sectional median) in 1.2; *_is_missing masks present.\"\n",
        "    },\n",
        "    \"qc\": {\n",
        "        \"missing_by_feature_csv\": \"qc_missing_by_feature.csv\",\n",
        "        \"coverage_by_date_csv\": \"qc_coverage_by_date.csv\",\n",
        "        \"recommendations_csv\": \"qc_recommendations.csv\",\n",
        "        \"warmup_cutoff\": str(warmup_cutoff.date()),\n",
        "        \"coverage_min\": COVERAGE_MIN,\n",
        "        \"recommendation\": \"Filter training rows to dates >= warmup_cutoff and dates with coverage >= coverage_min.\"\n",
        "    },\n",
        "    \"deliverables\": [\"universe.csv\", \"raw_prices.parquet\", \"features.parquet\",\n",
        "                     \"funda_quarterly.parquet\", \"funda_daily.parquet\", \"meta.yaml\",\n",
        "                     \"qc_missing_by_feature.csv\", \"qc_coverage_by_date.csv\", \"qc_recommendations.csv\"]\n",
        "}\n",
        "\n",
        "with open(\"meta.yaml\", \"w\") as f:\n",
        "    yaml.safe_dump(meta, f, sort_keys=False)\n",
        "\n",
        "print(\"Saved: meta.yaml + QC CSVs\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXCXZLXEzw6j",
        "outputId": "83c70d96-a2d8-4fed-9a9d-0b925123744b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: meta.yaml + QC CSVs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 1.4 DATA QC & ASSERTIONS (non-destructive; optional filtered view)\n",
        "# Produces: qc_summary.json, qc_constant_cols.csv, qc_missing_by_feature.csv (again),\n",
        "#           qc_skew_kurtosis.csv, qc_outlier_rate.csv, qc_drift.csv,\n",
        "#           features_filtered.parquet (optional, if you turn on APPLY_FILTERS)\n",
        "# ============================================================\n",
        "\n",
        "import json, pandas as pd, numpy as np\n",
        "from scipy.stats import skew, kurtosis\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", message=\"Precision loss occurred in moment calculation\")\n",
        "warnings.filterwarnings(\"ignore\", message=\"Degrees of freedom <= 0 for slice\")\n",
        "\n",
        "FEATURES_PATH = \"features.parquet\"\n",
        "\n",
        "APPLY_FILTERS = True          # set False if you only want reports\n",
        "COVERAGE_MIN = 300            # min tickers per date\n",
        "Z_OUTLIER = 5.0               # |z| threshold post-standardization\n",
        "EARLY_YEARS = 5               # windows for drift check\n",
        "RECENT_YEARS = 5\n",
        "\n",
        "df = pd.read_parquet(FEATURES_PATH)\n",
        "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
        "feature_cols = [c for c in df.columns if c not in {\"date\",\"ticker\",\"open\",\"high\",\"low\",\"close\",\"adj_close\",\"volume\"}]\n",
        "\n",
        "# Basic shape / duplicates\n",
        "dup_count = df.duplicated([\"date\",\"ticker\"]).sum()\n",
        "idx_dupes = int(dup_count)\n",
        "\n",
        "# Per-ticker monotonic date check\n",
        "monotonic_bad = []\n",
        "for t, g in df.groupby(\"ticker\"):\n",
        "    if not g[\"date\"].sort_values().is_monotonic_increasing:\n",
        "        monotonic_bad.append(t)\n",
        "\n",
        "# Constant/empty columns\n",
        "MIN_N = 200  # only compute moments if we’ve got enough points\n",
        "sk_stats = []\n",
        "\n",
        "const_cols, empty_cols = [], []\n",
        "for c in feature_cols:\n",
        "    nn = df[c].notna().sum()\n",
        "    if nn == 0:\n",
        "        empty_cols.append(c)\n",
        "        continue\n",
        "    # treat “constant” as very low variance or single unique value\n",
        "    if df[c].nunique(dropna=True) == 1 or np.nanstd(df[c].to_numpy(dtype=float)) < 1e-12:\n",
        "        const_cols.append(c)\n",
        "\n",
        "pd.Series(const_cols, name=\"constant_cols\").to_csv(\"qc_constant_cols.csv\", index=False)\n",
        "pd.Series(empty_cols,  name=\"empty_cols\").to_csv(\"qc_empty_cols.csv\", index=False)\n",
        "\n",
        "# Missingness\n",
        "missing_pct = (1.0 - df[feature_cols].notna().mean()).sort_values(ascending=False)\n",
        "missing_pct.to_csv(\"qc_missing_by_feature.csv\", header=[\"missing_pct\"])\n",
        "\n",
        "# Coverage by date and warmup/low-coverage mask (reuse warmup logic from 1.3)\n",
        "spy_dates = df.loc[df[\"ticker\"]==\"SPY\", \"date\"].sort_values().unique()\n",
        "warmup_cutoff = pd.to_datetime(spy_dates[min(273, len(spy_dates)-1)]) if len(spy_dates) > 300 else df[\"date\"].min()\n",
        "coverage = df.groupby(\"date\")[\"ticker\"].nunique()\n",
        "low_cov_dates = coverage[coverage < COVERAGE_MIN].index\n",
        "keep_mask = (df[\"date\"] >= warmup_cutoff) & (~df[\"date\"].isin(low_cov_dates))\n",
        "keep_rate = float(keep_mask.mean())\n",
        "\n",
        "# Outlier rate (features are z-scored per date already)\n",
        "outlier_rate = {}\n",
        "for c in feature_cols:\n",
        "    s = df[c]\n",
        "    outlier_rate[c] = float((s.abs() > Z_OUTLIER).mean())\n",
        "pd.Series(outlier_rate, name=\"outlier_rate\").sort_values(ascending=False).to_csv(\"qc_outlier_rate.csv\")\n",
        "\n",
        "# Skew/Kurtosis (global, ignoring NaNs)\n",
        "sk_rows = []\n",
        "for c in feature_cols:\n",
        "    x = df[c].to_numpy(dtype=float)\n",
        "    x = x[np.isfinite(x)]\n",
        "    if len(x) < MIN_N or np.nanstd(x) < 1e-8:\n",
        "        # optional: quantile-based skew as fallback\n",
        "        try:\n",
        "            q1,q2,q3 = np.nanpercentile(x, [25,50,75])\n",
        "            bowley = ((q3 + q1) - 2*q2) / ((q3 - q1) + 1e-9)\n",
        "        except Exception:\n",
        "            bowley = np.nan\n",
        "        sk_rows.append([c, np.nan, np.nan, bowley, np.nan, np.nan])\n",
        "        continue\n",
        "    sk = float(skew(x, bias=False))\n",
        "    ku = float(kurtosis(x, fisher=True, bias=False))\n",
        "    p99 = float(np.nanpercentile(x, 99))\n",
        "    med = float(np.nanmedian(x))\n",
        "    dom = abs(p99) / (abs(med) + 1e-9)\n",
        "    sk_rows.append([c, sk, ku, np.nan, dom, p99])\n",
        "\n",
        "pd.DataFrame(sk_rows, columns=[\"feature\",\"skew\",\"kurtosis_fisher\",\"bowley_skew\",\"p99_to_median_abs\",\"p99\"])\\\n",
        "  .sort_values(\"p99_to_median_abs\", ascending=False)\\\n",
        "  .to_csv(\"qc_skew_kurtosis.csv\", index=False)\n",
        "\n",
        "# Drift: early vs recent windows\n",
        "dstart, dend = df[\"date\"].min(), df[\"date\"].max()\n",
        "span_years = (dend - dstart).days / 365.25\n",
        "if span_years < (EARLY_YEARS + RECENT_YEARS):\n",
        "    # fallback: split the dataset in half\n",
        "    mid = dstart + (dend - dstart) / 2\n",
        "    early = df[(df[\"date\"] >= dstart) & (df[\"date\"] <= mid)]\n",
        "    late  = df[(df[\"date\"] >  mid) & (df[\"date\"] <= dend)]\n",
        "else:\n",
        "    early_end    = pd.Timestamp(dstart) + pd.DateOffset(years=EARLY_YEARS)\n",
        "    recent_start = pd.Timestamp(dend)   - pd.DateOffset(years=RECENT_YEARS)\n",
        "    early = df[(df[\"date\"] >= dstart) & (df[\"date\"] <= early_end)]\n",
        "    late  = df[(df[\"date\"] >= recent_start) & (df[\"date\"] <= dend)]\n",
        "\n",
        "drift_rows = []\n",
        "for c in feature_cols:\n",
        "    e = early[c].astype(\"float64\"); l = late[c].astype(\"float64\")\n",
        "    e = e[np.isfinite(e)]; l = l[np.isfinite(l)]\n",
        "    if len(e) < MIN_N or len(l) < MIN_N:\n",
        "        drift_rows.append([c, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan])\n",
        "        continue\n",
        "    e_mean, e_std = float(np.nanmean(e)), float(np.nanstd(e))\n",
        "    l_mean, l_std = float(np.nanmean(l)), float(np.nanstd(l))\n",
        "    drift_rows.append([c, e_mean, l_mean, l_mean - e_mean, e_std, l_std, (l_std+1e-9)/(e_std+1e-9)])\n",
        "\n",
        "pd.DataFrame(\n",
        "    drift_rows,\n",
        "    columns=[\"feature\",\"early_mean\",\"late_mean\",\"mean_diff\",\"early_std\",\"late_std\",\"std_ratio_late_over_early\"]\n",
        ").to_csv(\"qc_drift.csv\", index=False)\n",
        "\n",
        "def bowley_skew(x):\n",
        "    q1, q2, q3 = np.nanpercentile(x, [25,50,75])\n",
        "    denom = (q3 - q1) + 1e-9\n",
        "    return float(((q3 + q1) - 2*q2) / denom)\n",
        "# you can compute this alongside or instead of moment skew for each feature\n",
        "\n",
        "# Optionally write filtered view for modeling\n",
        "if APPLY_FILTERS:\n",
        "    # Also drop truly empty/constant cols from the filtered file only\n",
        "    drop_cols = list(set(empty_cols) | set(const_cols))\n",
        "    cols_keep = [c for c in df.columns if c not in drop_cols]\n",
        "    df_filt = df.loc[keep_mask, cols_keep].copy()\n",
        "    df_filt.to_parquet(\"features_filtered.parquet\", index=False)\n",
        "\n",
        "# Summary JSON (for quick eyeball)\n",
        "summary = {\n",
        "    \"rows\": int(len(df)),\n",
        "    \"tickers\": int(df[\"ticker\"].nunique()),\n",
        "    \"dates\": int(df[\"date\"].nunique()),\n",
        "    \"date_min\": str(df[\"date\"].min().date()),\n",
        "    \"date_max\": str(df[\"date\"].max().date()),\n",
        "    \"duplicates_idx\": idx_dupes,\n",
        "    \"monotonic_date_issues\": len(monotonic_bad),\n",
        "    \"constant_cols\": len(const_cols),\n",
        "    \"empty_cols\": len(empty_cols),\n",
        "    \"warmup_cutoff\": str(warmup_cutoff.date()),\n",
        "    \"coverage_min\": COVERAGE_MIN,\n",
        "    \"keep_rate_after_filters\": keep_rate,\n",
        "    \"median_missing_pct\": float(missing_pct.median()),\n",
        "    \"max_missing_pct\": float(missing_pct.max()),\n",
        "    \"mean_outlier_rate_|z|>5\": float(pd.Series(outlier_rate).mean()),\n",
        "    \"filtered_file_written\": APPLY_FILTERS\n",
        "}\n",
        "\n",
        "# 🚦 Hard QC checks — stop if these fail\n",
        "assert summary[\"duplicates_idx\"] == 0, \"Duplicate (date,ticker) rows found.\"\n",
        "assert summary[\"keep_rate_after_filters\"] >= 0.85, \"Too many rows dropped by filters.\"\n",
        "assert summary[\"constant_cols\"] <= 10, \"Suspicious number of constant columns.\"\n",
        "\n",
        "with open(\"qc_summary.json\",\"w\") as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "print(\"QC done → qc_summary.json, qc_* CSVs\",\n",
        "      \"and features_filtered.parquet\" if APPLY_FILTERS else \"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8eWz-gvOXhR",
        "outputId": "a50b2062-bfb4-4912-a6f0-39d932e8a3a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/numpy/lib/_nanfunctions_impl.py:1633: RuntimeWarning: Mean of empty slice\n",
            "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QC done → qc_summary.json, qc_* CSVs and features_filtered.parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json, pandas as pd\n",
        "\n",
        "with open(\"qc_summary.json\") as f:\n",
        "    s = json.load(f)\n",
        "\n",
        "print(\"=== QC SUMMARY ===\")\n",
        "for k in [\n",
        "    \"rows\",\"tickers\",\"dates\",\"date_min\",\"date_max\",\n",
        "    \"duplicates_idx\",\"monotonic_date_issues\",\n",
        "    \"constant_cols\",\"empty_cols\",\n",
        "    \"warmup_cutoff\",\"coverage_min\",\"keep_rate_after_filters\",\n",
        "    \"median_missing_pct\",\"max_missing_pct\",\"mean_outlier_rate_|z|>5\",\n",
        "    \"filtered_file_written\"\n",
        "]:\n",
        "    print(f\"{k}: {s.get(k)}\")\n",
        "\n",
        "print(\"\\n=== Top 10 most-missing features ===\")\n",
        "print(pd.read_csv(\"qc_missing_by_feature.csv\").head(10))\n",
        "\n",
        "print(\"\\n=== Top 10 highest outlier rates (|z|>5) ===\")\n",
        "print(pd.read_csv(\"qc_outlier_rate.csv\").head(10))\n",
        "\n",
        "print(\"\\n=== Constant / Empty columns ===\")\n",
        "try: print(pd.read_csv(\"qc_constant_cols.csv\").head())\n",
        "except: print(\"none\")\n",
        "try: print(pd.read_csv(\"qc_empty_cols.csv\").head())\n",
        "except: print(\"none\")\n",
        "\n",
        "print(\"\\n=== Drift (largest mean change early→late) ===\")\n",
        "drift = pd.read_csv(\"qc_drift.csv\")\n",
        "drift[\"abs_mean_diff\"] = drift[\"mean_diff\"].abs()\n",
        "print(drift.sort_values(\"abs_mean_diff\", ascending=False).head(10))\n",
        "\n",
        "print(\"\\n=== Filtered file shape ===\")\n",
        "ff = pd.read_parquet(\"features_filtered.parquet\")\n",
        "print(ff.shape, \"rows x cols; dates:\", ff['date'].min(), \"→\", ff['date'].max(), \"; tickers:\", ff['ticker'].nunique())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OApdlkYwWSsC",
        "outputId": "c971b140-b221-4e46-dc7e-a24d5e7fb18e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== QC SUMMARY ===\n",
            "rows: 2337209\n",
            "tickers: 514\n",
            "dates: 4932\n",
            "date_min: 2006-01-03\n",
            "date_max: 2025-08-11\n",
            "duplicates_idx: 0\n",
            "monotonic_date_issues: 0\n",
            "constant_cols: 3\n",
            "empty_cols: 3\n",
            "warmup_cutoff: 2007-02-05\n",
            "coverage_min: 300\n",
            "keep_rate_after_filters: 0.951552043484344\n",
            "median_missing_pct: 0.005388050448205506\n",
            "max_missing_pct: 1.0\n",
            "mean_outlier_rate_|z|>5: 0.032075781133748024\n",
            "filtered_file_written: True\n",
            "\n",
            "=== Top 10 most-missing features ===\n",
            "       Unnamed: 0  missing_pct\n",
            "0  earnings_yield     1.000000\n",
            "1             roe     1.000000\n",
            "2        accruals     1.000000\n",
            "3        mom_12_1     0.055640\n",
            "4         mom_12m     0.055640\n",
            "5         mom_6_1     0.027930\n",
            "6          mom_6m     0.027930\n",
            "7      ret_lag_60     0.013635\n",
            "8      ret_lag_59     0.013415\n",
            "9      ret_lag_58     0.013195\n",
            "\n",
            "=== Top 10 highest outlier rates (|z|>5) ===\n",
            "     Unnamed: 0  outlier_rate\n",
            "0     vix_close      0.999780\n",
            "1        sma_20      0.973318\n",
            "2        sma_50      0.967107\n",
            "3        atr_14      0.005798\n",
            "4      slope_20      0.002142\n",
            "5  sma_20_gt_50      0.000210\n",
            "6         rv_20      0.000152\n",
            "7     ret_lag_8      0.000041\n",
            "8     ret_lag_1      0.000041\n",
            "9     ret_lag_2      0.000041\n",
            "\n",
            "=== Constant / Empty columns ===\n",
            "               constant_cols\n",
            "0  earnings_yield_is_missing\n",
            "1             roe_is_missing\n",
            "2        accruals_is_missing\n",
            "       empty_cols\n",
            "0  earnings_yield\n",
            "1             roe\n",
            "2        accruals\n",
            "\n",
            "=== Drift (largest mean change early→late) ===\n",
            "                feature  early_mean   late_mean   mean_diff  early_std  \\\n",
            "68               sma_20   27.225558  170.082277  142.856719  44.295847   \n",
            "69               sma_50   27.190669  168.805483  141.614814  44.094553   \n",
            "74            vix_close   23.546991   20.022549   -3.524442  11.924481   \n",
            "76        book_to_price   -0.288617   -0.409328   -0.120711   0.201807   \n",
            "79    shareholder_yield    0.058068   -0.058688   -0.116756   0.306947   \n",
            "75              breadth    0.429111    0.526045    0.096935   0.224801   \n",
            "73            spy_rv_20    0.203213    0.158985   -0.044228   0.150975   \n",
            "83             leverage   -0.265574   -0.227629    0.037946   0.225628   \n",
            "78             cf_yield   -0.332773   -0.363656   -0.030882   0.232864   \n",
            "80  gross_profitability   -0.127282   -0.150549   -0.023267   0.216091   \n",
            "\n",
            "      late_std  std_ratio_late_over_early  abs_mean_diff  \n",
            "68  348.359773                   7.864389     142.856719  \n",
            "69  344.876817                   7.821302     141.614814  \n",
            "74    5.564515                   0.466646       3.524442  \n",
            "76    0.204887                   1.015263       0.120711  \n",
            "79    0.249586                   0.813125       0.116756  \n",
            "75    0.242064                   1.076796       0.096935  \n",
            "73    0.075441                   0.499690       0.044228  \n",
            "83    0.207334                   0.918919       0.037946  \n",
            "78    0.220200                   0.945617       0.030882  \n",
            "80    0.213662                   0.988760       0.023267  \n",
            "\n",
            "=== Filtered file shape ===\n",
            "(2223976, 94) rows x cols; dates: 2007-02-05 00:00:00 → 2025-08-11 00:00:00 ; tickers: 514\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>📦 Summary — Section 1 (Data & Universe)</summary>\n",
        "\n",
        "In this section, we **built the full, modeling-ready dataset** by merging historical prices, technical indicators, market context, and fundamentals into a single leakage-controlled feature matrix.  \n",
        "Key steps included:\n",
        "\n",
        "- **Data acquisition** — pulled long-term daily OHLCV for the equity universe, hedges, and context symbols, plus quarterly fundamentals from FMP.\n",
        "- **Feature engineering** — created lagged returns/volatility, momentum metrics, trend filters, ATR, volatility-adjusted momentum, and value/quality factor composites. Fundamentals were forward-filled to daily frequency.\n",
        "- **Leakage control & scaling** — shifted predictive features by one day, winsorized extreme values, and cross-sectionally z-scored each feature per date.\n",
        "- **Missing data handling** — conservative imputation for fundamentals and binary masks to record missingness.\n",
        "- **Quality control** — removed low-coverage dates, early warmup period, constant/empty columns, and duplicate rows; generated QC reports and metadata.\n",
        "\n",
        "**Outcome:** A clean, consistent, and statistically robust `features_filtered.parquet` file — ready for direct use in **Section 2 (Regime Modeling)** without recomputing or re-fetching any raw data.\n",
        "\n",
        "</details>\n"
      ],
      "metadata": {
        "id": "vOLQATza111V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary> Variables to reuse — Section 1 (Data & Universe) </summary>\n",
        "**Status:** Done. Artifacts are written; QC checks passed; ready to start **Section 2 (Regime Modeling)** using the saved files and globals below.\n",
        "\n",
        "---\n",
        "\n",
        "## Canonical Artifacts (reuse, don’t recompute)\n",
        "- `universe.csv` – S&P 500 tickers (Yahoo-style), excludes hedges/context.\n",
        "- `raw_prices.parquet` – OHLCV + `adj_close` for equities + hedges + `^VIX` (long format).\n",
        "- `features.parquet` – lagged, winsorized, cross-sectionally z-scored features (+ *_is_missing masks).\n",
        "- `features_filtered.parquet` – modeling-ready view (warmup & low-coverage dates removed; empty/constant cols dropped).\n",
        "- `funda_quarterly.parquet`, `funda_daily.parquet` – fundamentals at quarterly/daily granularity.\n",
        "- `meta.yaml` – machine-readable metadata (sources, lookback, QC guidance).\n",
        "- QC reports: `qc_summary.json`, `qc_missing_by_feature.csv`, `qc_coverage_by_date.csv`, `qc_constant_cols.csv`, `qc_empty_cols.csv`, `qc_outlier_rate.csv`, `qc_skew_kurtosis.csv`, `qc_drift.csv`, `qc_recommendations.csv`.\n",
        "\n",
        "---\n",
        "\n",
        "## Reusable Globals (organized)\n",
        "> These exist (or are trivially reloadable) after Section 1. Prefer these over re-deriving.\n",
        "\n",
        "### Dates / Ranges\n",
        "- `START_DATE = \"2006-01-01\"`  \n",
        "- `END_DATE = datetime.today().strftime(\"%Y-%m-%d\")`\n",
        "\n",
        "### Universe & Symbols\n",
        "- `sp500_url` – Wikipedia source for constituents.\n",
        "- `tickers_raw` → raw symbols from Wikipedia.\n",
        "- `tickers` → Yahoo-normalized tickers (periods → dashes).\n",
        "- `hedges` → `[\"SPY\",\"XLY\",\"XLF\",\"XLV\",\"XLK\",\"XLI\",\"XLE\",\"XLP\",\"XLB\",\"XLU\",\"XLRE\"]`\n",
        "- `context_symbols` → `[\"^VIX\"]`  *(later used as `{\"^VIX\"}` set in 1.2)*\n",
        "- `universe` → sorted unique S&P tickers.\n",
        "- `universe_all` → `universe + hedges + context_symbols`\n",
        "- `universe_full` → list from `universe.csv` (canonical equities universe for downstream code).\n",
        "\n",
        "### DataFrames (load-once, reuse)\n",
        "- `prices` → long OHLCV for `universe_all` (saved as `raw_prices.parquet`).\n",
        "- `features` → merged technical + context + fundamentals (post-shift, winsorize, z-score) (saved).\n",
        "- `vix` → `^VIX` close series; `spy` → SPY prices with `spy_ret`, `spy_rv_20`.\n",
        "- `ctx` → market context by date: `[\"spy_rv_20\",\"vix_close\",\"breadth\"]`.\n",
        "- `px_daily_all` → `[\"date\",\"ticker\",\"adj_close\"]` for equities universe.\n",
        "- `dates_all` → unique trading dates.\n",
        "- `funda_q` → quarterly fundamentals by ticker (saved).\n",
        "- `funda_daily` → daily forward-filled fundamentals (saved).\n",
        "\n",
        "### Feature Engineering Toggles / Windows\n",
        "- `COMPUTE_SLOPE = True`\n",
        "- `SLOPE_WINDOW = 20`\n",
        "- `RV_WIN = 20`\n",
        "- `ATR_WIN = 14`\n",
        "\n",
        "### Provider / API / Caching\n",
        "- `PROVIDER = \"fmp\"`\n",
        "- `FMP_API_KEY` – from env or prompt (in-memory only).\n",
        "- `CACHE_DIR = \"cache\"`\n",
        "- `CHUNK_TICKERS = 100`, `START_AT = 0`, `SKIP_IF_CACHED = True`\n",
        "- `MAX_WORKERS = 4`, `RETRY_ATTEMPTS = 5`, `BATCH_SLEEP = (0.2, 0.6)`\n",
        "\n",
        "### Useful Function Handles\n",
        "- `to_fmp_symbol(sym)` – Yahoo “-” ↔ FMP “.” class ticker mapping.\n",
        "- `is_index_like(sym)` – identifies index symbols (e.g., `^VIX`).\n",
        "- `compute_atr(df, window=ATR_WIN)`\n",
        "- `vectorized_rolling_slope(y, window=SLOPE_WINDOW)`\n",
        "- `mom_over_n(adj_close, n)`\n",
        "- `_tidy_quarterly_df(df)`, `_coalesce_cols(df, cols, default)`\n",
        "- `_fetch_quarterly_funda_fmp(ticker)` – pulls BS/IS/CF, coalesces variants.\n",
        "- `fetch_or_load_cached_quarterly(ticker)` – cached loader for fundamentals.\n",
        "- `cs_standardize_fast(df, cols, lo=0.01, hi=0.99)` – per-date winsorize+z-score.\n",
        "\n",
        "### Column Sets / Masks (downstream-friendly)\n",
        "- `non_feature_cols = {\"date\",\"ticker\",\"open\",\"high\",\"low\",\"close\",\"adj_close\",\"volume\"}`\n",
        "- `cols_to_shift` – all predictive feature columns actually shifted by 1 bar.\n",
        "- `cs_cols` – features standardized cross-sectionally (lags, vol, mom, fundamentals, etc.).\n",
        "- `context_keep_raw = [\"spy_rv_20\",\"vix_close\",\"breadth\"]`\n",
        "- *(QC section)*\n",
        "  - `FEATURES_PATH = \"features.parquet\"`, `UNIVERSE_PATH = \"universe.csv\"`\n",
        "  - `COVERAGE_MIN = 300`\n",
        "  - `APPLY_FILTERS = True`\n",
        "  - `Z_OUTLIER = 5.0`, `EARLY_YEARS = 5`, `RECENT_YEARS = 5`\n",
        "  - `warmup_cutoff` – computed from SPY date series (≈273 trading-day warmup).\n",
        "  - `keep_mask` – dates ≥ `warmup_cutoff` and with coverage ≥ `COVERAGE_MIN`.\n",
        "  - *(Note: `features_filtered.parquet` is written using `keep_mask` and pruned columns.)*\n",
        "\n",
        "---\n",
        "\n",
        "## What this means for Section 2 (Regimes)\n",
        "- **Use** `features_filtered.parquet` (or reload `features` and apply `keep_mask`) to build HMM inputs.\n",
        "- Inputs available out of the box: `spy_rv_20`, `vix_close`, `breadth`, and per-asset returns (`ret_1d`), plus everything in `cs_cols`.\n",
        "- **No duplicate `(date, ticker)` rows**, **no monotonic issues**; early sparse periods removed by `warmup_cutoff`/`keep_mask`.\n",
        "\n",
        "---\n",
        "\n",
        "## Sanity Questions (short answers)\n",
        "- **“Are we good to go?”** Yes — Section 1 is complete and validated; proceed to regime modeling.\n",
        "- **“Empty rows?”** Raw OHLCV rows with all NaNs were dropped; the modeling file (`features_filtered.parquet`) is filtered to warmup/coverage and prunes empty/constant columns. Row-level all-NaN feature cases should not remain after these filters.\n",
        "- **“Add the assertions?”** Already present and passing in QC (`qc_summary.json`). No need to add them again unless you change the pipeline.\n",
        "</details>"
      ],
      "metadata": {
        "id": "JGcOoZHEdNC4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Regime Modeling"
      ],
      "metadata": {
        "id": "u5YS_Mluwz_v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details> <summary>\n",
        "Outline (HMM → Regime Labels & Probabilities)</summary>\n",
        "\n",
        "# 2) Regime Modeling — Updated Outline (HMM → Regime Labels & Probabilities)\n",
        "\n",
        "## 2.0 Scope & Interfaces\n",
        "- **Goal:** Assign a daily market regime (Risk-On, Risk-Off, Transition) with posterior probabilities to drive regime-aware weighting, turnover caps, and risk targets in Sections 3–5.\n",
        "- **Inputs (from Section 1):**\n",
        "  - `features_filtered.parquet` with **raw** `spy_rv_20`, `vix_close`, `breadth`, and SPY `adj_close` for return computation.\n",
        "  - Trading calendar (aligned daily business days).\n",
        "- **Outputs (artifacts):**\n",
        "  - `regime_labels.parquet`: `date, state_id, p0..pK, regime_label`\n",
        "  - `regime_labels.csv` (plot-friendly)\n",
        "  - `regime_plot.png` (timeline with shading), `state_profiles.csv` (state stats)\n",
        "  - `regime_hmm.pkl` (bundle: scaler + HMM per walk-forward window)\n",
        "  - `regime_meta.json` (config, state→label map, scaler params, transition matrix, diagnostics)\n",
        "  - `regime_sensitivity.json` (K/feature/era stability tests)\n",
        "- **Pass/Fail gates:**\n",
        "  - Interpretable state profiles (return/vol ordering aligns with labels)\n",
        "  - Reasonable persistence (median run length > 5–10 days; no chattering)\n",
        "  - Stable mapping across walk-forward windows (low semantic flip rate)\n",
        "  - No leakage (all inputs at t known at t)\n",
        "\n",
        "---\n",
        "\n",
        "## 2.1 Data Assembly (Market Panel)\n",
        "- **Series:**\n",
        "  - SPY **log return** at t (computed from `adj_close`, shifted to avoid leakage if needed).\n",
        "  - SPY realized volatility (20-day) — from raw `spy_rv_20`.\n",
        "  - VIX **level** (`vix_close`) and optionally **daily Δ** (t − t-1).\n",
        "  - Market breadth (% advancers in S&P, known at t).\n",
        "- **IMPORTANT:** Use **raw** context series from Section 1 (`spy_rv_20`, `vix_close`, `breadth`), **not** cross-sectional z-scored features.\n",
        "- **Breadth timing:** Confirm that `breadth` reflects t-1 data available at t; if not, shift by 1.\n",
        "- **Alignment:** Daily business days; merge by `date`; forward-fill only for indicators known at t; drop rows with missing core inputs.\n",
        "- **Standardization:** Fit `StandardScaler` **per train window** on the raw context features; persist scaler per window (stored in `regime_hmm.pkl`).\n",
        "- **Sanity checks:**\n",
        "  - Stationarity proxy (mean/var drift over eras).\n",
        "  - Outlier handling: no winsorization needed for HMM since we scale raw series per window.\n",
        "  - Coverage check: ensure no missing dates in test stitching.\n",
        "\n",
        "---\n",
        "\n",
        "## 2.2 Model Choice & Configuration\n",
        "- **Primary:** Gaussian HMM with `covariance_type=\"full\"`; components K ∈ {2,3} (default 3).\n",
        "- **Alternative (optional):** Student-t HMM, GMM-HMM, Markov-Switching VAR, or Bayesian HMM with sticky priors.\n",
        "- **Hyperparameters:**\n",
        "  - `n_components`, `covariance_type`, `n_iter`, `random_state`.\n",
        "  - Dirichlet priors / sticky transitions to enforce regime persistence.\n",
        "- **Training protocol:**\n",
        "  - Train on standardized features in the train window.\n",
        "  - Multiple random restarts; choose model with highest log-likelihood.\n",
        "  - If applying **finance recency weighting rule**: optionally weight log-likelihood so recent data has more influence (can be implemented here if desired).\n",
        "\n",
        "---\n",
        "\n",
        "## 2.3 State Labeling & Semantics\n",
        "- **Profile each state:**\n",
        "  - Mean and vol of SPY returns.\n",
        "  - Mean VIX level, mean ΔVIX.\n",
        "  - Mean breadth, tail metrics (5% quantile returns).\n",
        "- **Label rules:**\n",
        "  - Highest mean return & lowest vol → **Risk-On**\n",
        "  - Highest vol & lowest return → **Risk-Off**\n",
        "  - Remaining state → **Transition**\n",
        "- **Tie-breakers:** breadth, VIX changes, downside tails.\n",
        "- **Persist mapping:** Save `state_id → regime_label` per window in `regime_meta.json` so semantics don’t silently drift across walk-forward windows.\n",
        "\n",
        "---\n",
        "\n",
        "## 2.4 Smoothing, Persistence & Debounce\n",
        "- **Posterior smoothing:** Option to use Viterbi most-likely path vs. raw posterior argmax.\n",
        "- **Debounce parameters:** `MIN_DWELL_DAYS` and `POSTERIOR_THRESH` from `config.yaml`.\n",
        "- **Gap handling:** Holidays/missing days inherit last known regime; no forward-looking fill.\n",
        "\n",
        "---\n",
        "\n",
        "## 2.5 Robustness & Sensitivity\n",
        "- **K sensitivity:** Run K=2 and K=3; prefer K with clearest separation (return/vol) and healthy dwell-time.\n",
        "- **Feature sensitivity:** Drop-one/add-one tests (remove VIX, remove breadth, etc.) to check label stability.\n",
        "- **Era stability:** Compare state profiles and transition matrices pre/post-2015 and during crisis years (e.g., 2020).\n",
        "- **Bootstrap:** Block bootstrap re-fit; produce confusion matrix for label stability across samples.\n",
        "\n",
        "---\n",
        "\n",
        "## 2.6 Diagnostics & QA\n",
        "- **Plots:**\n",
        "  - Timeline with regime shading over SPY price & drawdown.\n",
        "  - Posterior probabilities (stacked area).\n",
        "  - State return histograms, QQ plots.\n",
        "  - Transition matrix heatmap, dwell-time distribution.\n",
        "- **Tables:**\n",
        "  - State profiles (returns, vol, VIX, breadth, tails).\n",
        "  - Transition matrix & steady-state distribution.\n",
        "  - Switch frequency and chattering metrics.\n",
        "- **Alerts:**\n",
        "  - Flag if any state has inconsistent semantics (positive mean but top-2 vol, dwell-time < 3 days, mapping flips).\n",
        "\n",
        "---\n",
        "\n",
        "## 2.7 Regime-Aware Policy Hooks (Interfaces to Sections 3–5)\n",
        "- **Weights & turnover caps:** JSON map per regime (e.g., throttle momentum in Risk-Off, upweight quality).\n",
        "- **Risk targets:** Per-regime vol targets (e.g., 10%/8%/6% for On/Trans/Off).\n",
        "- **Hedge intensity:** Baseline hedge ratios per regime; pass to RL policy as defaults.\n",
        "- **Confidence proxy:** Use max posterior or entropy to scale aggressiveness.\n",
        "\n",
        "---\n",
        "\n",
        "## 2.8 Walk-Forward Integration\n",
        "- **Windows:** Match Section 6 (rolling/expanding).\n",
        "- **Per window:**\n",
        "  - Fit scaler + HMM on train subset.\n",
        "  - Apply to test subset only.\n",
        "  - Save artifacts: `regime_labels_<winid>.parquet`, `regime_hmm.pkl`, `regime_meta.json`.\n",
        "- **Stitching:** Concatenate per-window outputs into one continuous timeline for backtests.\n",
        "- **Label stability:** Use saved state→label mapping to avoid regime meaning drift.\n",
        "\n",
        "---\n",
        "\n",
        "## 2.9 Forward (Shadow) Mode\n",
        "- **Daily update:** Apply persisted scaler + HMM to latest t; append to `regime_labels.parquet`.\n",
        "- **Retrain cadence:** Weekly/bi-weekly.\n",
        "- **Logging:** Save model hash, posterior, chosen label, features vector.\n",
        "- **Alerts:** If mapping flips or dwell-time anomaly detected.\n",
        "\n",
        "---\n",
        "\n",
        "## 2.10 Configuration & Reproducibility\n",
        "- **Config keys (`config.yaml`):**\n",
        "  - Features list for HMM.\n",
        "  - `n_components`, `MIN_DWELL_DAYS`, `POSTERIOR_THRESH`.\n",
        "  - Finance recency weighting toggle & decay parameter (if implemented here).\n",
        "  - Random seed, plot toggles.\n",
        "- **Serialization:**\n",
        "  - joblib for model + scaler.\n",
        "  - JSON for meta (labels, thresholds, diagnostics).\n",
        "- **Tests:**\n",
        "  - Deterministic output with fixed seed.\n",
        "  - No leakage (t-only features).\n",
        "  - Posterior rows sum to 1; dates strictly increasing.\n",
        "  - No gaps after stitching.\n",
        "  - Label semantics test per window.\n",
        "\n",
        "---\n",
        "\n",
        "## 2.11 Deliverables Checklist\n",
        "- `regime_labels.parquet` (+ CSV).\n",
        "- `regime_hmm.pkl` (model + scaler per window).\n",
        "- `regime_meta.json` (state→label, scaler params, diagnostics).\n",
        "- `regime_timeline.png`, `regime_posteriors.png`, `state_profiles.csv`, `transition_matrix.csv`.\n",
        "- `regime_sensitivity.json` (K/feature/era stability).\n",
        "- `regime_policy_map.json` (interfaces to Sections 3–5).\n",
        "\n",
        "\n",
        "---\n",
        "</details>"
      ],
      "metadata": {
        "id": "mo-4MmP4fdjw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Section 2.0 — Scope & Interfaces (Regime Modeling bootstrap)\n",
        "# Builds on Section 1 artifacts; defines config, I/O, sanity checks,\n",
        "# and prepares the market-level panel stub used by 2.1+ (no HMM yet).\n",
        "# ============================================================\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import os\n",
        "import json\n",
        "import yaml\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import Dict, Any, List\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "# 0) Paths & directories (reuse Section 1 outputs)\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "FEATURES_PATH_DEFAULT = (\n",
        "    \"features_filtered.parquet\"\n",
        "    if os.path.exists(\"features_filtered.parquet\")\n",
        "    else \"features.parquet\"\n",
        ")\n",
        "UNIVERSE_PATH = \"universe.csv\"\n",
        "ARTIFACT_DIR = \"artifacts\"\n",
        "REGIME_DIR = os.path.join(ARTIFACT_DIR, \"regimes\")\n",
        "PLOTS_DIR = os.path.join(REGIME_DIR, \"plots\")\n",
        "\n",
        "os.makedirs(REGIME_DIR, exist_ok=True)\n",
        "os.makedirs(PLOTS_DIR, exist_ok=True)\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "# 1) Config — defaults + optional override via config.yaml\n",
        "# Keys are intentionally minimal here; 2.1–2.10 will read them.\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "@dataclass\n",
        "class RegimeConfig:\n",
        "    # Raw context features for HMM (NOT cross-sectional z-scores)\n",
        "    hmm_features: List[str]\n",
        "    include_dvix: bool                 # add ΔVIX feature to panel\n",
        "    n_components_grid: List[int]       # HMM K sensitivity (e.g., [2,3])\n",
        "    covariance_type: str               # \"full\" by default\n",
        "    random_seed: int\n",
        "    # Debounce (used later in 2.4)\n",
        "    min_dwell_days: int\n",
        "    posterior_thresh: float\n",
        "    # Optional finance rule: give more weight to recent samples during HMM fit\n",
        "    recency_weighting: bool\n",
        "    recency_half_life_days: int\n",
        "    # I/O\n",
        "    plots_enabled: bool\n",
        "    save_csv_alongside_parquet: bool\n",
        "    features_path: str = FEATURES_PATH_DEFAULT\n",
        "    universe_path: str = UNIVERSE_PATH\n",
        "    regime_dir: str = REGIME_DIR\n",
        "    plots_dir: str = PLOTS_DIR\n",
        "\n",
        "DEFAULT_CFG = RegimeConfig(\n",
        "    hmm_features=[\"spy_rv_20\", \"vix_close\", \"breadth\"],  # from Section 1 (raw context)\n",
        "    include_dvix=True,\n",
        "    n_components_grid=[2, 3],\n",
        "    covariance_type=\"full\",\n",
        "    random_seed=42,\n",
        "    min_dwell_days=3,\n",
        "    posterior_thresh=0.55,\n",
        "    recency_weighting=False,           # flip to True if enabling in 2.2\n",
        "    recency_half_life_days=90,\n",
        "    plots_enabled=True,\n",
        "    save_csv_alongside_parquet=True,\n",
        ")\n",
        "\n",
        "CONFIG_FILE = \"config.yaml\"\n",
        "user_cfg = {}\n",
        "if os.path.exists(CONFIG_FILE):\n",
        "    try:\n",
        "        with open(CONFIG_FILE, \"r\") as f:\n",
        "            raw_cfg = yaml.safe_load(f) or {}\n",
        "            if isinstance(raw_cfg, dict):\n",
        "                user_cfg = raw_cfg.get(\"regimes\", {}) or {}\n",
        "    except Exception:\n",
        "        user_cfg = {}\n",
        "\n",
        "def merge_cfg(default: RegimeConfig, override: Dict[str, Any]) -> RegimeConfig:\n",
        "    d = asdict(default)\n",
        "    for k, v in override.items():\n",
        "        if k in d and v is not None:\n",
        "            d[k] = v\n",
        "    return RegimeConfig(**d)\n",
        "\n",
        "CFG = merge_cfg(DEFAULT_CFG, user_cfg)\n",
        "\n",
        "# Persist effective config for traceability\n",
        "with open(os.path.join(REGIME_DIR, \"regime_config_effective.json\"), \"w\") as f:\n",
        "    json.dump(asdict(CFG), f, indent=2)\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "# 2) Load Section 1 artifacts and build the market panel stub\n",
        "# NOTE: use RAW context features from Section 1 (no CS-z).\n",
        "# This version auto-detects whether ^VIX exists as a ticker,\n",
        "# or vix_close/breadth/spy_rv_20 are already on every row.\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "assert os.path.exists(CFG.features_path), f\"Missing features file: {CFG.features_path}\"\n",
        "fe = pd.read_parquet(CFG.features_path)\n",
        "fe[\"date\"] = pd.to_datetime(fe[\"date\"], utc=False, errors=\"coerce\")\n",
        "fe = fe.dropna(subset=[\"date\"]).sort_values([\"date\", \"ticker\"])\n",
        "\n",
        "# Required columns present?\n",
        "required_cols = {\"date\", \"ticker\", \"adj_close\", \"spy_rv_20\", \"vix_close\", \"breadth\"}\n",
        "missing = list(required_cols - set(fe.columns))\n",
        "if missing:\n",
        "    raise ValueError(f\"Required columns missing in features file: {sorted(missing)}\")\n",
        "\n",
        "# SPY must exist for returns\n",
        "if not (fe[\"ticker\"] == \"SPY\").any():\n",
        "    raise ValueError(\"SPY rows not found in features file; cannot compute spy_ret.\")\n",
        "\n",
        "# Build SPY returns\n",
        "spy = fe.loc[fe[\"ticker\"] == \"SPY\", [\"date\", \"adj_close\", \"spy_rv_20\"]].copy()\n",
        "spy[\"spy_ret\"] = np.log(spy[\"adj_close\"] / spy[\"adj_close\"].shift(1))\n",
        "\n",
        "# vix_close / breadth / rv_20 may be replicated on every row; prefer unique-by-date view\n",
        "# If ^VIX rows exist, we can still just take unique-by-date—works for both layouts.\n",
        "vix_by_date = fe[[\"date\", \"vix_close\"]].drop_duplicates(\"date\").copy()\n",
        "breadth_by_date = fe[[\"date\", \"breadth\"]].drop_duplicates(\"date\").copy()\n",
        "rv20_by_date = fe[[\"date\", \"spy_rv_20\"]].drop_duplicates(\"date\").copy()\n",
        "\n",
        "# Merge market panel (date-level)\n",
        "mkt = (\n",
        "    spy[[\"date\", \"spy_ret\"]]                     # SPY returns\n",
        "    .merge(rv20_by_date, on=\"date\", how=\"inner\") # realized vol\n",
        "    .merge(vix_by_date, on=\"date\", how=\"inner\")  # VIX level\n",
        "    .merge(breadth_by_date, on=\"date\", how=\"inner\")  # breadth\n",
        "    .sort_values(\"date\")\n",
        ")\n",
        "\n",
        "# Optional ΔVIX (level change)\n",
        "if CFG.include_dvix:\n",
        "    mkt[\"dvix\"] = mkt[\"vix_close\"].diff()\n",
        "\n",
        "# Breadth timing guard: uncomment if your breadth is same-day and should be known-at-t\n",
        "# mkt[\"breadth\"] = mkt[\"breadth\"].shift(1)\n",
        "\n",
        "# Complete-case rows only (HMM requires no NaNs)\n",
        "core_cols = [\"spy_ret\", \"spy_rv_20\", \"vix_close\", \"breadth\"] + ([\"dvix\"] if CFG.include_dvix else [])\n",
        "mkt = mkt.dropna(subset=core_cols).reset_index(drop=True)\n",
        "\n",
        "# Save panel (consumed by 2.1/2.2)\n",
        "panel_path = os.path.join(CFG.regime_dir, \"market_panel.parquet\")\n",
        "mkt.to_parquet(panel_path, index=False)\n",
        "if CFG.save_csv_alongside_parquet:\n",
        "    mkt.to_csv(os.path.join(CFG.regime_dir, \"market_panel.csv\"), index=False)\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "# 5) Finalize & console summary (with robust date handling)\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "def _fmt_date(ts):\n",
        "    return None if pd.isna(ts) else pd.Timestamp(ts).strftime(\"%Y-%m-%d\")\n",
        "\n",
        "if mkt.empty:\n",
        "    # Diagnostics to help you decide if breadth shift is needed, etc.\n",
        "    core_for_diag = [\"spy_ret\", \"spy_rv_20\", \"vix_close\", \"breadth\"] + ([\"dvix\"] if CFG.include_dvix else [])\n",
        "    non_null_counts = {c: int(fe[c].notna().sum()) if c in fe.columns else 0 for c in core_for_diag}\n",
        "    spy_src = fe.loc[fe[\"ticker\"] == \"SPY\", [\"date\", \"adj_close\", \"spy_rv_20\"]].assign(\n",
        "        spy_ret=lambda d: np.log(d[\"adj_close\"] / d[\"adj_close\"].shift(1))\n",
        "    )\n",
        "    coverage_diag = {\n",
        "        \"rows_with_spy_ret_and_rv20\": int(spy_src.dropna(subset=[\"spy_ret\", \"spy_rv_20\"]).shape[0]),\n",
        "        \"unique_dates_with_vix\": int(vix_by_date.dropna(subset=[\"vix_close\"]).shape[0]),\n",
        "        \"unique_dates_with_breadth\": int(breadth_by_date.dropna(subset=[\"breadth\"]).shape[0]),\n",
        "    }\n",
        "    raise ValueError(\n",
        "        \"Market panel is empty after merging/dropping NaNs. \"\n",
        "        f\"Non-null counts (in features file): {non_null_counts}. \"\n",
        "        f\"Coverage by component: {coverage_diag}. \"\n",
        "        \"Common fixes: ensure breadth timing (try shifting breadth by 1), \"\n",
        "        \"or check for gaps in SPY/VIX/breadth date alignment.\"\n",
        "    )\n",
        "\n",
        "summary = {\n",
        "    \"features_file\": CFG.features_path,\n",
        "    \"universe_file\": CFG.universe_path,\n",
        "    \"market_panel_rows\": int(mkt.shape[0]),\n",
        "    \"market_panel_cols\": list(mkt.columns),\n",
        "    \"date_min\": _fmt_date(mkt['date'].min()),\n",
        "    \"date_max\": _fmt_date(mkt['date'].max()),\n",
        "    \"config_effective\": os.path.abspath(os.path.join(CFG.regime_dir, \"regime_config_effective.json\")),\n",
        "    \"panel_path\": os.path.abspath(panel_path),\n",
        "    \"meta_path\": os.path.abspath(os.path.join(CFG.regime_dir, 'regime_meta.json')),\n",
        "}\n",
        "print(json.dumps(summary, indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z59_nRdbzRNn",
        "outputId": "bb4839c2-ed11-4f7f-e026-59c28c68f18d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"features_file\": \"features_filtered.parquet\",\n",
            "  \"universe_file\": \"universe.csv\",\n",
            "  \"market_panel_rows\": 4658,\n",
            "  \"market_panel_cols\": [\n",
            "    \"date\",\n",
            "    \"spy_ret\",\n",
            "    \"spy_rv_20\",\n",
            "    \"vix_close\",\n",
            "    \"breadth\",\n",
            "    \"dvix\"\n",
            "  ],\n",
            "  \"date_min\": \"2007-02-06\",\n",
            "  \"date_max\": \"2025-08-11\",\n",
            "  \"config_effective\": \"/content/artifacts/regimes/regime_config_effective.json\",\n",
            "  \"panel_path\": \"/content/artifacts/regimes/market_panel.parquet\",\n",
            "  \"meta_path\": \"/content/artifacts/regimes/regime_meta.json\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Section 2.1 — Data Assembly (windowed extraction + scaling)\n",
        "# Uses artifacts from 2.0; prepares X_train/X_test for HMM.\n",
        "# ============================================================\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import os\n",
        "import json\n",
        "from dataclasses import asdict\n",
        "from typing import Dict, Any, Tuple, List, Optional\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import joblib\n",
        "\n",
        "# Reuse CFG, paths from 2.0\n",
        "REGIME_DIR = CFG.regime_dir\n",
        "PLOTS_DIR = CFG.plots_dir\n",
        "PANEL_PATH = os.path.join(REGIME_DIR, \"market_panel.parquet\")\n",
        "META_PATH = os.path.join(REGIME_DIR, \"regime_meta.json\")\n",
        "\n",
        "assert os.path.exists(PANEL_PATH), f\"Missing market panel: {PANEL_PATH}\"\n",
        "mkt = pd.read_parquet(PANEL_PATH)\n",
        "mkt[\"date\"] = pd.to_datetime(mkt[\"date\"])\n",
        "mkt = mkt.sort_values(\"date\").reset_index(drop=True)\n",
        "\n",
        "# Choose feature list (raw context features only; dvix optional)\n",
        "hmm_feat_cols = list(CFG.hmm_features)\n",
        "if CFG.include_dvix and \"dvix\" not in hmm_feat_cols:\n",
        "    hmm_feat_cols.append(\"dvix\")\n",
        "\n",
        "# Safety: ensure columns exist\n",
        "missing_cols = [c for c in hmm_feat_cols + [\"spy_ret\"] if c not in mkt.columns]\n",
        "if missing_cols:\n",
        "    raise ValueError(f\"Missing required columns in market panel: {missing_cols}\")\n",
        "\n",
        "def make_window_masks(df: pd.DataFrame,\n",
        "                      train_start: str,\n",
        "                      train_end: str,\n",
        "                      test_start: str,\n",
        "                      test_end: str) -> Tuple[pd.Series, pd.Series]:\n",
        "    d = df[\"date\"]\n",
        "    train_mask = (d >= pd.to_datetime(train_start)) & (d <= pd.to_datetime(train_end))\n",
        "    test_mask  = (d >= pd.to_datetime(test_start))  & (d <= pd.to_datetime(test_end))\n",
        "    return train_mask, test_mask\n",
        "\n",
        "def build_hmm_matrices(df: pd.DataFrame,\n",
        "                       features: List[str],\n",
        "                       train_start: str,\n",
        "                       train_end: str,\n",
        "                       test_start: str,\n",
        "                       test_end: str,\n",
        "                       scaler_out_path: Optional[str] = None,\n",
        "                       breadth_shift_days: int = 0) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      {\n",
        "        'X_train': np.ndarray,\n",
        "        'X_test': np.ndarray,\n",
        "        'dates_train': pd.DatetimeIndex,\n",
        "        'dates_test': pd.DatetimeIndex,\n",
        "        'scaler_path': str,\n",
        "        'scaler_mean': list,\n",
        "        'scaler_scale': list,\n",
        "        'qc': dict\n",
        "      }\n",
        "    \"\"\"\n",
        "    dfw = df.copy()\n",
        "\n",
        "    # Optional breadth shift (if you decide breadth should be known-at-t from t-1)\n",
        "    if breadth_shift_days != 0 and \"breadth\" in features:\n",
        "        dfw[\"breadth\"] = dfw[\"breadth\"].shift(breadth_shift_days)\n",
        "\n",
        "    # Drop rows with missing features\n",
        "    dfw = dfw.dropna(subset=features).reset_index(drop=True)\n",
        "\n",
        "    # Window masks\n",
        "    tr_mask, te_mask = make_window_masks(dfw, train_start, train_end, test_start, test_end)\n",
        "\n",
        "    # Slice\n",
        "    train_df = dfw.loc[tr_mask, [\"date\"] + features].dropna()\n",
        "    test_df  = dfw.loc[te_mask, [\"date\"] + features].dropna()\n",
        "\n",
        "    if train_df.empty or test_df.empty:\n",
        "        raise ValueError(\n",
        "            f\"Empty train/test after slicing: \"\n",
        "            f\"train({train_start}→{train_end}) rows={train_df.shape[0]}, \"\n",
        "            f\"test({test_start}→{test_end}) rows={test_df.shape[0]}. \"\n",
        "            f\"Consider adjusting dates or breadth_shift_days.\"\n",
        "        )\n",
        "\n",
        "    # Standardize on TRAIN ONLY; transform TEST with same scaler\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(train_df[features].to_numpy(dtype=float))\n",
        "    X_test  = scaler.transform(test_df[features].to_numpy(dtype=float))\n",
        "\n",
        "    # Persist scaler per window\n",
        "    if scaler_out_path is None:\n",
        "        win_tag = f\"{train_start}_{train_end}__{test_start}_{test_end}\".replace(\"-\", \"\")\n",
        "        scaler_out_path = os.path.join(REGIME_DIR, f\"scaler_{win_tag}.joblib\")\n",
        "    joblib.dump(scaler, scaler_out_path)\n",
        "\n",
        "    # Quick QC: mean/var drift (train vs test) and feature coverage\n",
        "    qc = {\n",
        "        \"train_rows\": int(train_df.shape[0]),\n",
        "        \"test_rows\": int(test_df.shape[0]),\n",
        "        \"features\": features,\n",
        "        \"train_means\": dict(zip(features, np.mean(X_train, axis=0).round(6).tolist())),\n",
        "        \"train_stds\": dict(zip(features, np.std(X_train, axis=0, ddof=0).round(6).tolist())),\n",
        "        \"test_means\": dict(zip(features, np.mean(X_test, axis=0).round(6).tolist())),\n",
        "        \"test_stds\": dict(zip(features, np.std(X_test, axis=0, ddof=0).round(6).tolist())),\n",
        "    }\n",
        "\n",
        "    # Save a tiny per-window QC file\n",
        "    win_qc_path = scaler_out_path.replace(\".joblib\", \"_qc.json\")\n",
        "    with open(win_qc_path, \"w\") as f:\n",
        "        json.dump(qc, f, indent=2)\n",
        "\n",
        "    return {\n",
        "        \"X_train\": X_train,\n",
        "        \"X_test\": X_test,\n",
        "        \"dates_train\": train_df[\"date\"].to_list(),\n",
        "        \"dates_test\": test_df[\"date\"].to_list(),\n",
        "        \"scaler_path\": scaler_out_path,\n",
        "        \"scaler_mean\": scaler.mean_.round(12).tolist(),\n",
        "        \"scaler_scale\": scaler.scale_.round(12).tolist(),\n",
        "        \"qc\": qc,\n",
        "    }\n",
        "\n",
        "# Example: pick a first walk-forward split anchored to your warmup_cutoff\n",
        "# You can replace these with your Section 6 generator later.\n",
        "train_start = \"2007-02-06\"  # day after warmup_cutoff in your QC\n",
        "train_end   = \"2016-12-30\"\n",
        "test_start  = \"2017-01-03\"\n",
        "test_end    = mkt[\"date\"].max().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "window = build_hmm_matrices(\n",
        "    df=mkt,\n",
        "    features=hmm_feat_cols,\n",
        "    train_start=train_start,\n",
        "    train_end=train_end,\n",
        "    test_start=test_start,\n",
        "    test_end=test_end,\n",
        "    scaler_out_path=None,\n",
        "    breadth_shift_days=0,  # set to 1 if you confirm breadth must be known-at-t from t-1\n",
        ")\n",
        "\n",
        "# Persist a small window manifest so later steps (2.2+) can load it\n",
        "manifest = {\n",
        "    \"window\": {\n",
        "        \"train_start\": train_start,\n",
        "        \"train_end\": train_end,\n",
        "        \"test_start\": test_start,\n",
        "        \"test_end\": test_end,\n",
        "    },\n",
        "    \"features\": hmm_feat_cols,\n",
        "    \"scaler_path\": window[\"scaler_path\"],\n",
        "    \"n_train\": len(window[\"dates_train\"]),\n",
        "    \"n_test\": len(window[\"dates_test\"]),\n",
        "}\n",
        "with open(os.path.join(REGIME_DIR, \"window_manifest.json\"), \"w\") as f:\n",
        "    json.dump(manifest, f, indent=2)\n",
        "\n",
        "print(json.dumps({\n",
        "    \"status\": \"2.1 ready\",\n",
        "    \"features_used\": hmm_feat_cols,\n",
        "    \"scaler_saved\": window[\"scaler_path\"],\n",
        "    \"train_rows\": manifest[\"n_train\"],\n",
        "    \"test_rows\": manifest[\"n_test\"],\n",
        "}, indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfBUiQIZvMAS",
        "outputId": "8cc6cc2f-e753-4760-de83-2600e643558c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"status\": \"2.1 ready\",\n",
            "  \"features_used\": [\n",
            "    \"spy_rv_20\",\n",
            "    \"vix_close\",\n",
            "    \"breadth\",\n",
            "    \"dvix\"\n",
            "  ],\n",
            "  \"scaler_saved\": \"artifacts/regimes/scaler_20070206_20161230__20170103_20250811.joblib\",\n",
            "  \"train_rows\": 2495,\n",
            "  \"test_rows\": 2163\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hmmlearn --quiet"
      ],
      "metadata": {
        "id": "GQf0An0h0TWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Section 2.2 — Model Choice & Configuration (Gaussian HMM)\n",
        "# Primary: GaussianHMM (full covariance), K in {2,3}\n",
        "# - Multiple restarts; pick best train log-likelihood\n",
        "# - Sticky transitions (Dirichlet-like persistence) via diagonal bias\n",
        "# - Finance recency weighting: time-decayed sub-sequences (ENABLED)\n",
        "# Reuses:\n",
        "#   - artifacts/regimes/market_panel.parquet (from 2.0)\n",
        "#   - artifacts/regimes/window_manifest.json (from 2.1)\n",
        "#   - scaler_*.joblib (from 2.1)\n",
        "# Outputs:\n",
        "#   - artifacts/regimes/regime_hmm.pkl (joblib bundle: model + meta)\n",
        "#   - artifacts/regimes/hmm_kgrid.json (scores by K)\n",
        "# ============================================================\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import os\n",
        "import json\n",
        "from typing import Dict, Any, List, Tuple\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "from hmmlearn.hmm import GaussianHMM\n",
        "\n",
        "# Reuse config and paths from 2.0 / 2.1\n",
        "REGIME_DIR = CFG.regime_dir\n",
        "PANEL_PATH = os.path.join(REGIME_DIR, \"market_panel.parquet\")\n",
        "MANIFEST_PATH = os.path.join(REGIME_DIR, \"window_manifest.json\")\n",
        "META_PATH = os.path.join(REGIME_DIR, \"regime_meta.json\")\n",
        "\n",
        "assert os.path.exists(PANEL_PATH), f\"Missing market panel: {PANEL_PATH}\"\n",
        "assert os.path.exists(MANIFEST_PATH), f\"Missing window manifest: {MANIFEST_PATH}\"\n",
        "\n",
        "with open(MANIFEST_PATH, \"r\") as f:\n",
        "    MAN = json.load(f)\n",
        "\n",
        "mkt = pd.read_parquet(PANEL_PATH).sort_values(\"date\").reset_index(drop=True)\n",
        "mkt[\"date\"] = pd.to_datetime(mkt[\"date\"])\n",
        "\n",
        "scaler = joblib.load(MAN[\"scaler_path\"])\n",
        "features = MAN[\"features\"]\n",
        "assert all(c in mkt.columns for c in features), f\"Panel missing features: {set(features) - set(mkt.columns)}\"\n",
        "\n",
        "# Train/test windows\n",
        "train_start = pd.to_datetime(MAN[\"window\"][\"train_start\"])\n",
        "train_end   = pd.to_datetime(MAN[\"window\"][\"train_end\"])\n",
        "test_start  = pd.to_datetime(MAN[\"window\"][\"test_start\"])\n",
        "test_end    = pd.to_datetime(MAN[\"window\"][\"test_end\"])\n",
        "\n",
        "train_df = mkt[(mkt[\"date\"] >= train_start) & (mkt[\"date\"] <= train_end)][[\"date\"] + features].dropna().reset_index(drop=True)\n",
        "test_df  = mkt[(mkt[\"date\"] >= test_start)  & (mkt[\"date\"] <= test_end)][[\"date\"] + features].dropna().reset_index(drop=True)\n",
        "\n",
        "X_train = scaler.transform(train_df[features].to_numpy(dtype=float))\n",
        "X_test  = scaler.transform(test_df[features].to_numpy(dtype=float))\n",
        "dates_train = train_df[\"date\"].to_numpy()\n",
        "dates_test  = test_df[\"date\"].to_numpy()\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "# Hyperparameters — Test run now, bump for real run (marked TOCHANGE)\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "N_COMPONENTS_GRID = [2]   # TOCHANGE: [3] or [2,3] for real run\n",
        "N_ITER = 200              # TOCHANGE: 1000 for real run\n",
        "N_INIT = 2                # TOCHANGE: 10 for real run\n",
        "RANDOM_STATE = 42\n",
        "COVARIANCE_TYPE = \"full\"\n",
        "TOL = 1e-3                # TOCHANGE: 1e-4 for real run\n",
        "\n",
        "# Sticky transitions strength (Dirichlet-like, diagonal blend post-fit)\n",
        "# Larger -> stickier regimes (longer dwell times)\n",
        "LAMBDA_STICK = 0.15       # TOCHANGE: 0.30–0.50 for real run\n",
        "\n",
        "# Finance recency weighting — ENABLED\n",
        "APPLY_RECENCY = True\n",
        "HALF_LIFE_DAYS = 756      # ~3 years; keeps 2008 meaningful\n",
        "# TOCHANGE: try 504 (~2y, more recency), 756 (~3y, balanced), 1260 (~5y, less recency)\n",
        "\n",
        "EPSILON_FLOOR = 0.10      # ensures old episodes never get <10% of peak weight\n",
        "# TOCHANGE: 0.05–0.15 depending on how protective you want to be\n",
        "\n",
        "# For recency sampler (still lightweight in test; scale for real run)\n",
        "SEG_LEN = 60              # TOCHANGE: 90–120 for real run\n",
        "N_SEGMENTS = 80           # TOCHANGE: 200–400 for real run\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "# Utilities\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "def _diag_sticky_blend(transmat: np.ndarray, lam: float) -> np.ndarray:\n",
        "    k = transmat.shape[0]\n",
        "    T = (1.0 - lam) * transmat + lam * np.eye(k)\n",
        "    T = T / T.sum(axis=1, keepdims=True)\n",
        "    return T\n",
        "\n",
        "def _build_time_decay_weights(dates: np.ndarray, half_life_days: int) -> np.ndarray:\n",
        "    t = np.array([pd.Timestamp(d).toordinal() for d in dates], dtype=float)\n",
        "    age = (t.max() - t)  # newer dates -> smaller age\n",
        "    decay = np.log(2) / max(1, half_life_days)\n",
        "    w = np.exp(-decay * age)\n",
        "    return w / (w.sum() + 1e-12)\n",
        "\n",
        "def _sample_time_weighted_subsequences(\n",
        "    X: np.ndarray,\n",
        "    dates: np.ndarray,\n",
        "    seg_len: int,\n",
        "    n_segments: int,\n",
        "    half_life_days: int,\n",
        "    random_state: int,\n",
        ") -> Tuple[np.ndarray, List[int]]:\n",
        "    rng = np.random.RandomState(random_state)\n",
        "    n = X.shape[0]\n",
        "    if n < seg_len:\n",
        "        return X.copy(), [n]\n",
        "    ends = np.arange(seg_len - 1, n)\n",
        "    p = _build_time_decay_weights(dates[ends], half_life_days)\n",
        "    p = np.maximum(p, EPSILON_FLOOR * p.max())\n",
        "    p = p / p.sum()\n",
        "\n",
        "    chosen = rng.choice(ends, size=min(n_segments, len(ends)), replace=True, p=p)\n",
        "    lengths, chunks = [], []\n",
        "    for e in chosen:\n",
        "        s = e - (seg_len - 1)\n",
        "        chunk = X[s:e+1]\n",
        "        chunks.append(chunk)\n",
        "        lengths.append(len(chunk))\n",
        "    X_concat = np.vstack(chunks)\n",
        "    return X_concat, lengths\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "# Train HMMs; pick best by train log-likelihood\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "best = {\"score\": -np.inf, \"model\": None, \"k\": None, \"seed\": None, \"train_lengths\": None, \"fit_mode\": None}\n",
        "results = []\n",
        "\n",
        "for k in N_COMPONENTS_GRID:\n",
        "    for r in range(N_INIT):\n",
        "        seed = RANDOM_STATE + r\n",
        "\n",
        "        if APPLY_RECENCY:\n",
        "            X_fit, lengths = _sample_time_weighted_subsequences(\n",
        "                X_train, dates_train,\n",
        "                seg_len=SEG_LEN,\n",
        "                n_segments=N_SEGMENTS,\n",
        "                half_life_days=HALF_LIFE_DAYS,\n",
        "                random_state=seed,\n",
        "            )\n",
        "            fit_mode = \"recency\"\n",
        "        else:\n",
        "            X_fit, lengths = X_train, [len(X_train)]\n",
        "            fit_mode = \"plain\"\n",
        "\n",
        "        # Init model\n",
        "        model = GaussianHMM(\n",
        "            n_components=k,\n",
        "            covariance_type=COVARIANCE_TYPE,\n",
        "            n_iter=N_ITER,\n",
        "            tol=TOL,\n",
        "            random_state=seed,\n",
        "            verbose=False,\n",
        "            # IMPORTANT: do not include 's' or 't' here, otherwise your custom\n",
        "            # startprob_/transmat_ get overwritten on init.\n",
        "            init_params=\"mc\",      # means, covars only\n",
        "            params=\"stmc\",         # learn startprob, transmat, means, covars\n",
        "        )\n",
        "\n",
        "        # Sticky-biased initialization (near-diagonal)\n",
        "        trans0 = np.full((k, k), (1.0 - 0.90) / max(1, k - 1))\n",
        "        np.fill_diagonal(trans0, 0.90)\n",
        "        model.transmat_ = trans0\n",
        "\n",
        "        # Uniform start probabilities\n",
        "        model.startprob_ = np.full(k, 1.0 / k)\n",
        "\n",
        "        # Fit\n",
        "        model.fit(X_fit, lengths=lengths)\n",
        "\n",
        "        # Post-fit sticky blend (Dirichlet-like)\n",
        "        model.transmat_ = _diag_sticky_blend(model.transmat_, LAMBDA_STICK)\n",
        "\n",
        "        score = model.score(X_train)  # comparable scoring on original train sequence\n",
        "\n",
        "        results.append({\n",
        "            \"k\": k,\n",
        "            \"seed\": seed,\n",
        "            \"score\": float(score),\n",
        "            \"fit_mode\": fit_mode,\n",
        "            \"transmat\": model.transmat_.tolist(),\n",
        "        })\n",
        "\n",
        "        if score > best[\"score\"]:\n",
        "            best.update({\"score\": score, \"model\": model, \"k\": k, \"seed\": seed, \"train_lengths\": lengths, \"fit_mode\": fit_mode})\n",
        "\n",
        "# Save grid scores\n",
        "with open(os.path.join(REGIME_DIR, \"hmm_kgrid.json\"), \"w\") as f:\n",
        "    json.dump({\"results\": results, \"chosen\": {\"k\": best[\"k\"], \"seed\": best[\"seed\"], \"score\": float(best[\"score\"]), \"fit_mode\": best[\"fit_mode\"]}}, f, indent=2)\n",
        "\n",
        "# Persist best model bundle\n",
        "bundle = {\n",
        "    \"model\": best[\"model\"],\n",
        "    \"k\": best[\"k\"],\n",
        "    \"random_state\": best[\"seed\"],\n",
        "    \"features\": features,\n",
        "    \"scaler_path\": MAN[\"scaler_path\"],\n",
        "    \"train_dates\": [str(d) for d in dates_train],\n",
        "    \"test_dates\": [str(d) for d in dates_test],\n",
        "    \"fit_mode\": best[\"fit_mode\"],\n",
        "    \"sticky_lambda\": LAMBDA_STICK,\n",
        "    \"n_iter\": N_ITER,\n",
        "    \"n_init\": N_INIT,\n",
        "    \"tol\": TOL,\n",
        "    \"covariance_type\": COVARIANCE_TYPE,\n",
        "    \"recency_weighting\": True,  # enabled\n",
        "    \"recency_half_life_days\": HALF_LIFE_DAYS,\n",
        "    \"recency_seg_len\": SEG_LEN,         # TOCHANGE: 90–120\n",
        "    \"recency_n_segments\": N_SEGMENTS,   # TOCHANGE: 200–400\n",
        "    \"created_at\": datetime.utcnow().isoformat() + \"Z\",\n",
        "    \"recency_epsilon_floor\": EPSILON_FLOOR\n",
        "}\n",
        "joblib.dump(bundle, os.path.join(REGIME_DIR, \"regime_hmm.pkl\"))\n",
        "\n",
        "print(json.dumps({\n",
        "    \"status\": \"2.2 trained\",\n",
        "    \"chosen_k\": best[\"k\"],\n",
        "    \"fit_mode\": best[\"fit_mode\"],\n",
        "    \"train_score\": float(best[\"score\"]),\n",
        "    \"n_iter\": N_ITER,\n",
        "    \"n_init\": N_INIT,\n",
        "    \"sticky_lambda\": LAMBDA_STICK,\n",
        "    \"recency_weighting\": True,\n",
        "    \"half_life_days\": HALF_LIFE_DAYS,\n",
        "    \"seg_len\": SEG_LEN,\n",
        "    \"n_segments\": N_SEGMENTS,\n",
        "}, indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3ekvERhym7N",
        "outputId": "bb62ce80-692b-4126-f62a-f7c38f1a7080"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"status\": \"2.2 trained\",\n",
            "  \"chosen_k\": 2,\n",
            "  \"fit_mode\": \"recency\",\n",
            "  \"train_score\": -8178.604987870423,\n",
            "  \"n_iter\": 200,\n",
            "  \"n_init\": 2,\n",
            "  \"sticky_lambda\": 0.15,\n",
            "  \"recency_weighting\": true,\n",
            "  \"half_life_days\": 756,\n",
            "  \"seg_len\": 60,\n",
            "  \"n_segments\": 80\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick peek at effective sampling weights (fixed timedelta math)\n",
        "ends = np.arange(SEG_LEN - 1, len(dates_train))\n",
        "end_dates = pd.to_datetime(dates_train[ends])\n",
        "\n",
        "pp = _build_time_decay_weights(end_dates, HALF_LIFE_DAYS)\n",
        "pp = np.maximum(pp, EPSILON_FLOOR * pp.max())\n",
        "pp = pp / pp.sum()\n",
        "\n",
        "# Age in *days* relative to the most recent end_date\n",
        "max_date = end_dates.max()\n",
        "ages_days = (max_date - end_dates) / np.timedelta64(1, \"D\")  # float days\n",
        "\n",
        "# Weighted mean age (how \"old\" the typical sampled segment end is)\n",
        "w_mean_age_days = float(np.sum(ages_days * pp))\n",
        "\n",
        "# 95% weight age: age threshold below which 95% of weight lies\n",
        "order = np.argsort(ages_days)                  # youngest → oldest\n",
        "cumw = np.cumsum(pp[order])\n",
        "w95_idx = np.searchsorted(cumw, 0.95)\n",
        "w95_age_days = float(ages_days[order][min(w95_idx, len(ages_days)-1)])\n",
        "\n",
        "print({\n",
        "    \"weights_min\": float(pp.min()),\n",
        "    \"weights_max\": float(pp.max()),\n",
        "    \"weighted_mean_age_days\": w_mean_age_days,\n",
        "    \"weighted_p95_age_days\": w95_age_days,\n",
        "})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKJAsmdi85Ek",
        "outputId": "0fba3dd5-b0f8-440e-f723-72edb1ed26b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'weights_min': 0.0001335955016895934, 'weights_max': 0.001335955016895934, 'weighted_mean_age_days': 1017.4276078929489, 'weighted_p95_age_days': 2990.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Section 2.3 — State Labeling & Semantics\n",
        "# - Score posteriors for all dates\n",
        "# - Profile states on TRAIN window only (no peeking)\n",
        "# - Label states: Risk-On (↑ret, ↓vol), Risk-Off (↓ret, ↑vol), Transition (rest)\n",
        "# - Persist labels, posteriors, and profiles\n",
        "# Outputs:\n",
        "#   artifacts/regimes/regime_labels.parquet (date, state_id, p0..pK, regime_label)\n",
        "#   artifacts/regimes/state_profiles.csv\n",
        "#   artifacts/regimes/regime_meta.json (updated label map)\n",
        "# ============================================================\n",
        "\n",
        "from __future__ import annotations\n",
        "import os, json\n",
        "from typing import Dict, Any\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from datetime import datetime\n",
        "\n",
        "REGIME_DIR = CFG.regime_dir\n",
        "PANEL_PATH = os.path.join(REGIME_DIR, \"market_panel.parquet\")\n",
        "MANIFEST_PATH = os.path.join(REGIME_DIR, \"window_manifest.json\")\n",
        "BUNDLE_PATH = os.path.join(REGIME_DIR, \"regime_hmm.pkl\")\n",
        "META_PATH = os.path.join(REGIME_DIR, \"regime_meta.json\")\n",
        "\n",
        "# Load artifacts\n",
        "mkt = pd.read_parquet(PANEL_PATH).sort_values(\"date\").reset_index(drop=True)\n",
        "mkt[\"date\"] = pd.to_datetime(mkt[\"date\"])\n",
        "with open(MANIFEST_PATH, \"r\") as f:\n",
        "    MAN = json.load(f)\n",
        "bundle = joblib.load(BUNDLE_PATH)\n",
        "model = bundle[\"model\"]\n",
        "features = bundle[\"features\"]\n",
        "scaler = joblib.load(bundle[\"scaler_path\"])\n",
        "\n",
        "# Prepare matrices for ALL dates (but label semantics computed on TRAIN ONLY)\n",
        "X_all = scaler.transform(mkt[features].to_numpy(dtype=float))\n",
        "dates_all = mkt[\"date\"].to_numpy()\n",
        "\n",
        "# Score posteriors\n",
        "post = model.predict_proba(X_all)  # shape: (T, K)\n",
        "states_argmax = post.argmax(axis=1)\n",
        "K = post.shape[1]\n",
        "\n",
        "# Train/test masks\n",
        "train_start = pd.to_datetime(MAN[\"window\"][\"train_start\"])\n",
        "train_end   = pd.to_datetime(MAN[\"window\"][\"train_end\"])\n",
        "test_start  = pd.to_datetime(MAN[\"window\"][\"test_start\"])\n",
        "test_end    = pd.to_datetime(MAN[\"window\"][\"test_end\"])\n",
        "train_mask = (mkt[\"date\"] >= train_start) & (mkt[\"date\"] <= train_end)\n",
        "test_mask  = (mkt[\"date\"] >= test_start)  & (mkt[\"date\"] <= test_end)\n",
        "\n",
        "# Helper: posterior-weighted stats on TRAIN window\n",
        "def weighted_mean(x, w):\n",
        "    w = np.asarray(w, dtype=float)\n",
        "    x = np.asarray(x, dtype=float)\n",
        "    s = w.sum()\n",
        "    return float((x * w).sum() / s) if s > 0 else np.nan\n",
        "\n",
        "def weighted_std(x, w):\n",
        "    mu = weighted_mean(x, w)\n",
        "    w = np.asarray(w, dtype=float)\n",
        "    x = np.asarray(x, dtype=float)\n",
        "    s = w.sum()\n",
        "    if s <= 1: return np.nan\n",
        "    var = ((w * (x - mu)**2).sum()) / s\n",
        "    return float(np.sqrt(max(var, 0.0)))\n",
        "\n",
        "def weighted_quantile(x, w, q=0.05):\n",
        "    x = np.asarray(x, dtype=float)\n",
        "    w = np.asarray(w, dtype=float)\n",
        "    order = np.argsort(x)\n",
        "    x_sorted, w_sorted = x[order], w[order]\n",
        "    cw = np.cumsum(w_sorted)\n",
        "    if cw[-1] == 0: return np.nan\n",
        "    return float(x_sorted[np.searchsorted(cw, q * cw[-1])])\n",
        "\n",
        "# Compute per-state profiles on TRAIN\n",
        "train_ix = np.where(train_mask.values)[0]\n",
        "has_dvix = \"dvix\" in mkt.columns\n",
        "profiles = []\n",
        "for s in range(K):\n",
        "    w = post[train_ix, s]\n",
        "    if w.sum() == 0:\n",
        "        mu_ret = mu_vol = mu_vix = mu_brd = q05 = np.nan\n",
        "        sd_ret = np.nan\n",
        "    else:\n",
        "        mu_ret = weighted_mean(mkt.loc[train_mask, \"spy_ret\"].values, w)\n",
        "        sd_ret = weighted_std(mkt.loc[train_mask, \"spy_ret\"].values, w)\n",
        "        mu_vol = weighted_mean(mkt.loc[train_mask, \"spy_rv_20\"].values, w)\n",
        "        mu_vix = weighted_mean(mkt.loc[train_mask, \"vix_close\"].values, w)\n",
        "        mu_brd = weighted_mean(mkt.loc[train_mask, \"breadth\"].values, w)\n",
        "        mu_dvix = weighted_mean(mkt.loc[train_mask, \"dvix\"].values, w) if has_dvix else np.nan\n",
        "        q05    = weighted_quantile(mkt.loc[train_mask, \"spy_ret\"].values, w, q=0.05)\n",
        "\n",
        "    profiles.append({\n",
        "        \"state_id\": s,\n",
        "        \"ret_mean\": mu_ret,\n",
        "        \"ret_std\": sd_ret,\n",
        "        \"rv20_mean\": mu_vol,\n",
        "        \"vix_mean\": mu_vix,\n",
        "        \"dvix_mean\": mu_dvix if has_dvix else np.nan,\n",
        "        \"breadth_mean\": mu_brd,\n",
        "        \"ret_q05\": q05,\n",
        "    })\n",
        "\n",
        "prof_df = pd.DataFrame(profiles)\n",
        "\n",
        "# Labeling rules (train window only, no peeking):\n",
        "#  - Risk-On: highest mean return, lowest vol (tie-breakers help if ambiguous)\n",
        "#  - Risk-Off: highest vol, lowest mean return (tie-breakers help if ambiguous)\n",
        "#  - Transition: whichever state is not assigned above\n",
        "# Primary ranks\n",
        "ret_rank = prof_df[\"ret_mean\"].rank(method=\"dense\")                        # higher = better\n",
        "# For clarity: choose Risk-Off by *highest* rv20 (vol spike)\n",
        "risk_off_id = int(prof_df[\"rv20_mean\"].idxmax())                           # highest vol\n",
        "risk_on_id  = int(ret_rank.idxmax())                                       # highest return\n",
        "\n",
        "# Tie-breaker refinement (only if they collide or look ambiguous)\n",
        "# Risk-On tie-breakers: breadth↑, VIX↓, tail q05↑\n",
        "# Risk-Off tie-breakers: vol↑, ret↓, ΔVIX↑, breadth↓\n",
        "def _best_risk_on_row(df: pd.DataFrame) -> int:\n",
        "    score = (\n",
        "        df[\"breadth_mean\"].fillna(-1.0).rank(method=\"dense\", ascending=False) +\n",
        "        df[\"vix_mean\"].fillna(np.inf).rank(method=\"dense\", ascending=True) +\n",
        "        df[\"ret_q05\"].fillna(-np.inf).rank(method=\"dense\", ascending=False)\n",
        "    )\n",
        "    return int(score.idxmax())\n",
        "\n",
        "def _best_risk_off_row(df: pd.DataFrame) -> int:\n",
        "    score = (\n",
        "        df[\"rv20_mean\"].fillna(-np.inf).rank(method=\"dense\", ascending=False) +\n",
        "        df[\"ret_mean\"].fillna(np.inf).rank(method=\"dense\", ascending=True) +\n",
        "        (df[\"dvix_mean\"] if \"dvix_mean\" in df.columns else pd.Series(0.0, index=df.index)).fillna(0.0).rank(method=\"dense\", ascending=False) +\n",
        "        df[\"breadth_mean\"].fillna(np.inf).rank(method=\"dense\", ascending=True)\n",
        "    )\n",
        "    return int(score.idxmax())\n",
        "\n",
        "if risk_on_id == risk_off_id:\n",
        "    risk_on_id  = _best_risk_on_row(prof_df)\n",
        "    risk_off_id = _best_risk_off_row(prof_df)\n",
        "    # Safety: if still colliding (extremely rare), force Risk-Off = highest vol, Risk-On = highest return\n",
        "    if risk_on_id == risk_off_id:\n",
        "        risk_off_id = int(prof_df[\"rv20_mean\"].idxmax())\n",
        "        risk_on_id  = int(prof_df[\"ret_mean\"].idxmax())\n",
        "\n",
        "# Final label map\n",
        "label_map = {risk_on_id: \"Risk-On\", risk_off_id: \"Risk-Off\"}\n",
        "for s in range(K):\n",
        "    if s not in label_map:\n",
        "        label_map[s] = \"Transition\"\n",
        "\n",
        "# ---------- Build outputs USING the FINAL label_map ----------\n",
        "out = mkt[[\"date\"]].copy()\n",
        "out[\"state_id\"] = post.argmax(axis=1)\n",
        "for s in range(K):\n",
        "    out[f\"p{s}\"] = post[:, s]\n",
        "out[\"regime_label\"] = out[\"state_id\"].map(label_map)\n",
        "\n",
        "out_path = os.path.join(REGIME_DIR, \"regime_labels.parquet\")\n",
        "out.to_parquet(out_path, index=False)\n",
        "out.to_csv(os.path.join(REGIME_DIR, \"regime_labels.csv\"), index=False)\n",
        "\n",
        "prof_df.to_csv(os.path.join(REGIME_DIR, \"state_profiles.csv\"), index=False)\n",
        "\n",
        "# ---------- Update meta WITH the FINAL label_map ----------\n",
        "if os.path.exists(META_PATH):\n",
        "    with open(META_PATH, \"r\") as f:\n",
        "        meta = json.load(f)\n",
        "else:\n",
        "    meta = {}\n",
        "\n",
        "meta.setdefault(\"created_at\", datetime.utcnow().isoformat() + \"Z\")\n",
        "meta.setdefault(\"config\", {})\n",
        "meta.setdefault(\"diagnostics\", {})\n",
        "meta[\"diagnostics\"][\"state_profiles_train\"] = prof_df.to_dict(orient=\"records\")\n",
        "meta[\"state_label_map\"] = {int(k): v for k, v in label_map.items()}\n",
        "meta.setdefault(\"features_used\", features)\n",
        "meta[\"notes\"] = meta.get(\"notes\", []) + [\n",
        "    \"State labeling computed on train window only (no peeking).\",\n",
        "    \"Risk-On: highest mean ret & lowest vol; Risk-Off: highest vol & lowest ret; else Transition.\",\n",
        "    \"Tie-breakers: breadth↑, VIX↓, tail q05↑ (Risk-On); vol↑, ret↓, ΔVIX↑, breadth↓ (Risk-Off).\",\n",
        "]\n",
        "\n",
        "with open(META_PATH, \"w\") as f:\n",
        "    json.dump(meta, f, indent=2)\n",
        "\n",
        "print(json.dumps({\n",
        "    \"status\": \"2.3 labeled\",\n",
        "    \"k\": K,\n",
        "    \"label_map\": label_map,\n",
        "    \"profiles_path\": os.path.join(REGIME_DIR, \"state_profiles.csv\"),\n",
        "    \"labels_path\": out_path,\n",
        "}, indent=2))\n",
        "\n",
        "# Posteriors sanity\n",
        "assert np.allclose(post.sum(axis=1), 1.0, atol=1e-6), \"Posterior rows must sum to 1.\"\n",
        "assert not pd.isna(out[\"regime_label\"]).any(), \"All states must map to a regime label.\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1sBNFT58pPr",
        "outputId": "654d7cff-e1e1-4ed8-efa2-75638a988e55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"status\": \"2.3 labeled\",\n",
            "  \"k\": 2,\n",
            "  \"label_map\": {\n",
            "    \"0\": \"Risk-On\",\n",
            "    \"1\": \"Risk-Off\"\n",
            "  },\n",
            "  \"profiles_path\": \"artifacts/regimes/state_profiles.csv\",\n",
            "  \"labels_path\": \"artifacts/regimes/regime_labels.parquet\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Section 2.4 — Smoothing, Persistence & Debounce\n",
        "# - Option: Viterbi most-likely path vs. posterior argmax\n",
        "# - Debounce: POSTERIOR_THRESH and MIN_DWELL_DAYS from config\n",
        "# - Gap handling: inherit last known regime (dates already market-days)\n",
        "# Reuses:\n",
        "#   - artifacts/regimes/market_panel.parquet (2.0)\n",
        "#   - artifacts/regimes/window_manifest.json (2.1)\n",
        "#   - artifacts/regimes/regime_hmm.pkl (2.2)\n",
        "#   - artifacts/regimes/regime_labels.parquet (2.3)\n",
        "# Outputs:\n",
        "#   - artifacts/regimes/regime_labels.parquet (updated with *_smoothed cols)\n",
        "#   - artifacts/regimes/regime_meta.json (updated diagnostics)\n",
        "#   - console summary of dwell-time stats\n",
        "# ============================================================\n",
        "\n",
        "from __future__ import annotations\n",
        "import os, json\n",
        "from typing import List, Dict, Any, Tuple\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "REGIME_DIR     = CFG.regime_dir\n",
        "PANEL_PATH     = os.path.join(REGIME_DIR, \"market_panel.parquet\")\n",
        "LABELS_PATH    = os.path.join(REGIME_DIR, \"regime_labels.parquet\")\n",
        "META_PATH      = os.path.join(REGIME_DIR, \"regime_meta.json\")\n",
        "BUNDLE_PATH    = os.path.join(REGIME_DIR, \"regime_hmm.pkl\")\n",
        "\n",
        "assert os.path.exists(PANEL_PATH),  f\"Missing market panel: {PANEL_PATH}\"\n",
        "assert os.path.exists(LABELS_PATH), f\"Missing labels from 2.3: {LABELS_PATH}\"\n",
        "assert os.path.exists(BUNDLE_PATH), f\"Missing HMM bundle: {BUNDLE_PATH}\"\n",
        "\n",
        "# --- Config knobs (extend 2.0 config if not present) ---\n",
        "P_THRESH   = getattr(CFG, \"posterior_thresh\", 0.55) # TOCHANGE: consider 0.60–0.65 for a stricter switch confirmation.\n",
        "MIN_DWELL  = getattr(CFG, \"min_dwell_days\", 3) #  # TOCHANGE: consider 5–10 to further reduce chattering.\n",
        "SMOOTH_MTH = getattr(CFG, \"smoothing_method\", \"posterior\")  # \"posterior\" | \"viterbi\" # TOCHANGE: try \"viterbi\" for the real run and compare dwell-time stats and chattering.\n",
        "\n",
        "# --- Load artifacts ---\n",
        "labels = pd.read_parquet(LABELS_PATH).sort_values(\"date\").reset_index(drop=True)\n",
        "bundle = joblib.load(BUNDLE_PATH)\n",
        "model  = bundle[\"model\"]\n",
        "features = bundle[\"features\"]\n",
        "scaler  = joblib.load(bundle[\"scaler_path\"])\n",
        "\n",
        "mkt = pd.read_parquet(PANEL_PATH).sort_values(\"date\").reset_index(drop=True)\n",
        "mkt[\"date\"] = pd.to_datetime(mkt[\"date\"])\n",
        "\n",
        "# sanity\n",
        "assert np.array_equal(labels[\"date\"].values, mkt[\"date\"].values), \"Date alignment mismatch between labels and market panel.\"\n",
        "\n",
        "# --- Choose base path: Viterbi vs posterior argmax ---\n",
        "# We need posteriors for thresholding either way; for Viterbi we re-score X_all.\n",
        "X_all = scaler.transform(mkt[features].to_numpy(dtype=float))\n",
        "post  = model.predict_proba(X_all)  # (T, K)\n",
        "K     = post.shape[1]\n",
        "\n",
        "if SMOOTH_MTH.lower() == \"viterbi\":\n",
        "    base_states = model.predict(X_all)     # most-likely state path\n",
        "else:\n",
        "    base_states = post.argmax(axis=1)      # raw posterior argmax (already in 2.3)\n",
        "\n",
        "# --- Debounce step 1: posterior threshold gating (no switch if low confidence) ---\n",
        "maxp = post.max(axis=1)\n",
        "debounce_states = np.array(base_states, dtype=int)\n",
        "for i in range(1, len(debounce_states)):\n",
        "    if debounce_states[i] != debounce_states[i-1]:\n",
        "        # require sufficient posterior confidence on the *new* state\n",
        "        if maxp[i] < P_THRESH:\n",
        "            debounce_states[i] = debounce_states[i-1]\n",
        "\n",
        "# --- Debounce step 2: enforce minimum dwell time by collapsing short runs ---\n",
        "def _runs(state_series: np.ndarray) -> List[Tuple[int,int,int]]:\n",
        "    \"\"\"Return list of (start_idx, end_idx_inclusive, state) runs.\"\"\"\n",
        "    out = []\n",
        "    s = 0\n",
        "    cur = state_series[0]\n",
        "    for i in range(1, len(state_series)):\n",
        "        if state_series[i] != cur:\n",
        "            out.append((s, i-1, cur))\n",
        "            s = i\n",
        "            cur = state_series[i]\n",
        "    out.append((s, len(state_series)-1, cur))\n",
        "    return out\n",
        "\n",
        "def _collapse_short_runs(states: np.ndarray, min_len: int, post: np.ndarray) -> np.ndarray:\n",
        "    arr = states.copy()\n",
        "    changed = True\n",
        "    # iterate until stable (collapsing can merge adjacent runs)\n",
        "    while changed:\n",
        "        changed = False\n",
        "        runs = _runs(arr)\n",
        "        for (s, e, st) in runs:\n",
        "            run_len = e - s + 1\n",
        "            if run_len < min_len:\n",
        "                # Candidate neighbors: previous and next, choose higher avg posterior over this segment\n",
        "                prev_state = runs[runs.index((s, e, st))-1][2] if runs.index((s, e, st)) > 0 else None\n",
        "                next_state = runs[runs.index((s, e, st))+1][2] if runs.index((s, e, st)) < len(runs)-1 else None\n",
        "\n",
        "                # If no neighbors (degenerate), skip\n",
        "                if prev_state is None and next_state is None:\n",
        "                    continue\n",
        "\n",
        "                # Compute average posterior for neighbors over the short segment\n",
        "                best_neighbor = None\n",
        "                best_score = -np.inf\n",
        "                for cand in [prev_state, next_state]:\n",
        "                    if cand is None:\n",
        "                        continue\n",
        "                    score = float(post[s:e+1, cand].mean())\n",
        "                    if score > best_score:\n",
        "                        best_score = score\n",
        "                        best_neighbor = cand\n",
        "                # Relabel the short run to best neighbor\n",
        "                arr[s:e+1] = best_neighbor\n",
        "                changed = True\n",
        "                break  # restart since runs have changed\n",
        "    return arr\n",
        "\n",
        "smoothed_states = _collapse_short_runs(debounce_states, MIN_DWELL, post)\n",
        "\n",
        "# --- Map to labels using the semantics from 2.3 (state_label_map) ---\n",
        "# read label map from meta\n",
        "if os.path.exists(META_PATH):\n",
        "    with open(META_PATH, \"r\") as f:\n",
        "        meta = json.load(f)\n",
        "else:\n",
        "    meta = {}\n",
        "\n",
        "state_label_map = meta.get(\"state_label_map\", None)\n",
        "if state_label_map is None:\n",
        "    # fallback to identity names if meta missing (shouldn't happen)\n",
        "    state_label_map = {int(s): f\"State{s}\" for s in range(K)}\n",
        "\n",
        "# Update labels DataFrame with smoothed outputs\n",
        "labels[\"state_id_smoothed\"] = smoothed_states\n",
        "for s in range(K):\n",
        "    # keep original p0..pK as-is from 2.3; they reflect the raw model posteriors\n",
        "    if f\"p{s}\" not in labels.columns:\n",
        "        labels[f\"p{s}\"] = post[:, s]\n",
        "\n",
        "labels[\"regime_label_smoothed\"] = labels[\"state_id_smoothed\"].map({int(k): v for k, v in state_label_map.items()})\n",
        "\n",
        "# --- Dwell-time diagnostics ---\n",
        "def _dwell_stats(states: np.ndarray) -> pd.DataFrame:\n",
        "    rr = _runs(states)\n",
        "    return pd.DataFrame({\n",
        "        \"state_id\": [st for (_,_,st) in rr],\n",
        "        \"run_len\":  [e - s + 1 for (s,e,_) in rr],\n",
        "    }).groupby(\"state_id\").agg(\n",
        "        median_run_length=(\"run_len\", \"median\"),\n",
        "        mean_run_length  =(\"run_len\", \"mean\"),\n",
        "        n_runs           =(\"run_len\", \"count\"),\n",
        "        max_run_length   =(\"run_len\", \"max\"),\n",
        "    ).reset_index()\n",
        "\n",
        "dwell_df = _dwell_stats(labels[\"state_id_smoothed\"].to_numpy())\n",
        "\n",
        "# --- Save updated labels back to disk ---\n",
        "labels.to_parquet(LABELS_PATH, index=False)\n",
        "labels.to_csv(LABELS_PATH.replace(\".parquet\", \".csv\"), index=False)\n",
        "\n",
        "# --- Update regime_meta.json diagnostics & config snapshot ---\n",
        "meta.setdefault(\"diagnostics\", {})\n",
        "meta[\"diagnostics\"][\"smoothing\"] = {\n",
        "    \"method\": SMOOTH_MTH,\n",
        "    \"posterior_thresh\": P_THRESH,\n",
        "    \"min_dwell_days\": MIN_DWELL,\n",
        "    \"dwell_stats\": dwell_df.to_dict(orient=\"records\"),\n",
        "}\n",
        "meta.setdefault(\"notes\", [])\n",
        "meta[\"notes\"] += [\n",
        "    \"2.4 smoothing applied with debounce (posterior threshold + min dwell).\",\n",
        "    \"If method='viterbi', base path is Viterbi; else posterior argmax.\",\n",
        "]\n",
        "# de-dup notes\n",
        "meta[\"notes\"] = list(dict.fromkeys(meta[\"notes\"]))\n",
        "\n",
        "with open(META_PATH, \"w\") as f:\n",
        "    json.dump(meta, f, indent=2)\n",
        "\n",
        "print(json.dumps({\n",
        "    \"status\": \"2.4 smoothed\",\n",
        "    \"method\": SMOOTH_MTH,\n",
        "    \"posterior_thresh\": P_THRESH,\n",
        "    \"min_dwell_days\": MIN_DWELL,\n",
        "    \"k\": K,\n",
        "    \"median_dwell_by_state\": {\n",
        "        int(r[\"state_id\"]): float(r[\"median_run_length\"]) for r in dwell_df.to_dict(orient=\"records\")\n",
        "    },\n",
        "    \"labels_path\": LABELS_PATH\n",
        "}, indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eo9ZowDyKJ-f",
        "outputId": "76fae990-fc0a-46ed-e135-812f1a37b6d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"status\": \"2.4 smoothed\",\n",
            "  \"method\": \"posterior\",\n",
            "  \"posterior_thresh\": 0.55,\n",
            "  \"min_dwell_days\": 3,\n",
            "  \"k\": 2,\n",
            "  \"median_dwell_by_state\": {\n",
            "    \"0\": 33.5,\n",
            "    \"1\": 12.0\n",
            "  },\n",
            "  \"labels_path\": \"artifacts/regimes/regime_labels.parquet\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Section 2.5 — Robustness & Sensitivity\n",
        "# - K sensitivity: K ∈ {2,3}\n",
        "# - Feature sensitivity: drop-one/add-one variants\n",
        "# - Era stability: pre/post-2015 and 2020 crisis\n",
        "# - Bootstrap: block bootstrap label stability\n",
        "# Reuses:\n",
        "#   - artifacts/regimes/market_panel.parquet (2.0)\n",
        "#   - artifacts/regimes/window_manifest.json (2.1)\n",
        "#   - scaler_*.joblib (2.1)\n",
        "#   - artifacts/regimes/regime_hmm.pkl (2.2 baseline)\n",
        "#   - artifacts/regimes/regime_labels.parquet (2.3 baseline labels)\n",
        "# Outputs:\n",
        "#   - artifacts/regimes/regime_sensitivity.json\n",
        "# Notes:\n",
        "#   This is a light test pass; heavier settings are tagged with # TOCHANGE\n",
        "# ============================================================\n",
        "\n",
        "from __future__ import annotations\n",
        "import os, json\n",
        "from typing import Dict, Any, List, Tuple\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from hmmlearn.hmm import GaussianHMM\n",
        "\n",
        "REGIME_DIR  = CFG.regime_dir\n",
        "PANEL_PATH  = os.path.join(REGIME_DIR, \"market_panel.parquet\")\n",
        "MAN_PATH    = os.path.join(REGIME_DIR, \"window_manifest.json\")\n",
        "BUNDLE_PATH = os.path.join(REGIME_DIR, \"regime_hmm.pkl\")\n",
        "LABELS_PATH = os.path.join(REGIME_DIR, \"regime_labels.parquet\")\n",
        "OUT_PATH    = os.path.join(REGIME_DIR, \"regime_sensitivity.json\")\n",
        "\n",
        "assert os.path.exists(PANEL_PATH) and os.path.exists(MAN_PATH) and os.path.exists(BUNDLE_PATH)\n",
        "mkt = pd.read_parquet(PANEL_PATH).sort_values(\"date\").reset_index(drop=True)\n",
        "mkt[\"date\"] = pd.to_datetime(mkt[\"date\"])\n",
        "\n",
        "with open(MAN_PATH, \"r\") as f:\n",
        "    MAN = json.load(f)\n",
        "\n",
        "bundle   = joblib.load(BUNDLE_PATH)\n",
        "features_base = bundle[\"features\"]\n",
        "scaler   = joblib.load(bundle[\"scaler_path\"])\n",
        "k_base   = int(bundle[\"k\"])\n",
        "recency  = bool(bundle.get(\"recency_weighting\", True))\n",
        "hl_days  = int(bundle.get(\"recency_half_life_days\", 756))\n",
        "seg_len  = int(bundle.get(\"recency_seg_len\", 60))\n",
        "n_segs   = int(bundle.get(\"recency_n_segments\", 80))\n",
        "tol      = float(bundle.get(\"tol\", 1e-3))\n",
        "n_iter   = int(bundle.get(\"n_iter\", 200))         # TOCHANGE: 1000 for real run\n",
        "n_init   = int(bundle.get(\"n_init\", 2))           # TOCHANGE: 10 for real run\n",
        "covtype  = bundle.get(\"covariance_type\", \"full\")\n",
        "lam_stick= float(bundle.get(\"sticky_lambda\", 0.15))  # TOCHANGE: 0.30–0.50 for real run\n",
        "rand0    = int(bundle.get(\"random_state\", 42))\n",
        "\n",
        "train_start = pd.to_datetime(MAN[\"window\"][\"train_start\"])\n",
        "train_end   = pd.to_datetime(MAN[\"window\"][\"train_end\"])\n",
        "test_start  = pd.to_datetime(MAN[\"window\"][\"test_start\"])\n",
        "test_end    = pd.to_datetime(MAN[\"window\"][\"test_end\"])\n",
        "\n",
        "mask_train = (mkt[\"date\"] >= train_start) & (mkt[\"date\"] <= train_end)\n",
        "mask_test  = (mkt[\"date\"] >= test_start)  & (mkt[\"date\"] <= test_end)\n",
        "\n",
        "# ⬇️ ADD: tiny helper to fit a local scaler on the train (or era) subset\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def _fit_local_scaler(feats: List[str], subset_mask: pd.Series) -> StandardScaler:\n",
        "    df = mkt.loc[subset_mask, feats].dropna()\n",
        "    scaler_local = StandardScaler()\n",
        "    scaler_local.fit(df.to_numpy(dtype=float))\n",
        "    return scaler_local\n",
        "\n",
        "def _diag_sticky_blend(T: np.ndarray, lam: float) -> np.ndarray:\n",
        "    k = T.shape[0]\n",
        "    out = (1.0 - lam) * T + lam * np.eye(k)\n",
        "    return out / out.sum(axis=1, keepdims=True)\n",
        "\n",
        "def _build_time_decay_weights(dates: np.ndarray, half_life_days: int) -> np.ndarray:\n",
        "    t = np.array([pd.Timestamp(d).toordinal() for d in dates], dtype=float)\n",
        "    age = (t.max() - t)\n",
        "    decay = np.log(2) / max(1, half_life_days)\n",
        "    w = np.exp(-decay * age)\n",
        "    return w / (w.sum() + 1e-12)\n",
        "\n",
        "# Canonical recency-sampling params (align with 2.2 bundle keys)\n",
        "REC_SEG_LEN    = int(bundle.get(\"recency_seg_len\", 60))\n",
        "REC_N_SEGMENTS = int(bundle.get(\"recency_n_segments\", 80))\n",
        "REC_HALF_LIFE  = int(bundle.get(\"recency_half_life_days\", 756))\n",
        "REC_EPS        = float(bundle.get(\"recency_epsilon_floor\", 0.10))  # matches 2.2 key\n",
        "\n",
        "def _sample_time_weighted_subsequences(\n",
        "    X: np.ndarray,\n",
        "    dates: np.ndarray,\n",
        "    seg_len: int = REC_SEG_LEN,\n",
        "    n_segments: int = REC_N_SEGMENTS,\n",
        "    half_life_days: int = REC_HALF_LIFE,\n",
        "    seed: int = 42,\n",
        "):\n",
        "    rng = np.random.RandomState(seed)\n",
        "    n = X.shape[0]\n",
        "    if n < seg_len:\n",
        "        return X.copy(), [n]\n",
        "    ends = np.arange(seg_len - 1, n)\n",
        "    p = _build_time_decay_weights(dates[ends], half_life_days)\n",
        "    p = np.maximum(p, REC_EPS * p.max())\n",
        "    p = p / p.sum()\n",
        "    chosen = rng.choice(ends, size=min(n_segments, len(ends)), replace=True, p=p)\n",
        "    chunks, lengths = [], []\n",
        "    for e in chosen:\n",
        "        s = e - (seg_len - 1)\n",
        "        chunks.append(X[s:e+1])\n",
        "        lengths.append(seg_len)\n",
        "    return np.vstack(chunks), lengths\n",
        "\n",
        "# ⬇️ MODIFY: _fit_hmm_for_features now fits and returns a local scaler,\n",
        "# and uses it for both training and scoring.\n",
        "def _fit_hmm_for_features(feats: List[str], k: int, rs: int, subset_mask: pd.Series) -> Dict[str, Any]:\n",
        "    # Fit local scaler on the subset (train or era) to avoid feature-count mismatch\n",
        "    scaler_local = _fit_local_scaler(feats, subset_mask)\n",
        "\n",
        "    df = mkt.loc[subset_mask, [\"date\"] + feats].dropna().reset_index(drop=True)\n",
        "    dates = df[\"date\"].to_numpy()\n",
        "    X = scaler_local.transform(df[feats].to_numpy(dtype=float))\n",
        "\n",
        "    if recency:\n",
        "        X_fit, lengths = _sample_time_weighted_subsequences(\n",
        "            X, dates, seg_len=seg_len, n_segments=n_segs, half_life_days=hl_days, seed=rs\n",
        "        )\n",
        "    else:\n",
        "        X_fit, lengths = X, [len(X)]\n",
        "\n",
        "    model = GaussianHMM(\n",
        "        n_components=k,\n",
        "        covariance_type=covtype,\n",
        "        n_iter=n_iter,\n",
        "        tol=tol,\n",
        "        random_state=rs,\n",
        "        verbose=False,\n",
        "        init_params=\"mc\",   # means, covars\n",
        "        params=\"stmc\",      # learn startprob/transmat as well\n",
        "    )\n",
        "    # sticky-ish init\n",
        "    T0 = np.full((k, k), (1.0 - 0.90) / max(1, k - 1)); np.fill_diagonal(T0, 0.90)\n",
        "    model.transmat_ = T0\n",
        "    model.startprob_ = np.full(k, 1.0 / k)\n",
        "\n",
        "    model.fit(X_fit, lengths=lengths)\n",
        "    model.transmat_ = _diag_sticky_blend(model.transmat_, lam_stick)\n",
        "\n",
        "    # score on original (non-sampled) sequence\n",
        "    score = float(model.score(X))\n",
        "\n",
        "    return {\n",
        "        \"model\": model,\n",
        "        \"score\": score,\n",
        "        \"dates\": dates,\n",
        "        \"feats\": feats,\n",
        "        \"scaler\": scaler_local  # ⬅️ return it\n",
        "    }\n",
        "\n",
        "# ⬇️ MODIFY: _profile_and_label takes the local scaler we fit above\n",
        "def _profile_and_label(model, feats: List[str], scaler_local: StandardScaler) -> Dict[str, Any]:\n",
        "    X_all = scaler_local.transform(mkt[feats].to_numpy(dtype=float))\n",
        "    post  = model.predict_proba(X_all)\n",
        "    K = post.shape[1]\n",
        "\n",
        "    # compute profiles on TRAIN ONLY (no peeking)\n",
        "    train_ix = np.where(mask_train.values)[0]\n",
        "\n",
        "    def wmean(x, w):\n",
        "        w = np.asarray(w); x = np.asarray(x); s = w.sum()\n",
        "        return float((x*w).sum()/s) if s>0 else np.nan\n",
        "    def wstd(x, w):\n",
        "        mu = wmean(x, w); w = np.asarray(w); x = np.asarray(x); s=w.sum()\n",
        "        if s<=1: return np.nan\n",
        "        return float(np.sqrt(max(((w*(x-mu)**2).sum()/s), 0.0)))\n",
        "    def wq05(x, w):\n",
        "        x=np.asarray(x); w=np.asarray(w); o=np.argsort(x); xs,ws=x[o],w[o]; cw=np.cumsum(ws)\n",
        "        return float(xs[np.searchsorted(cw, 0.05*cw[-1])]) if cw[-1]>0 else np.nan\n",
        "\n",
        "    prof = []\n",
        "    for s in range(K):\n",
        "        w = post[train_ix, s]\n",
        "        prof.append({\n",
        "            \"state_id\": s,\n",
        "            \"ret_mean\": wmean(mkt.loc[mask_train,\"spy_ret\"].values, w),\n",
        "            \"ret_std\":  wstd (mkt.loc[mask_train,\"spy_ret\"].values, w),\n",
        "            \"rv20_mean\": wmean(mkt.loc[mask_train,\"spy_rv_20\"].values, w),\n",
        "            \"vix_mean\":  wmean(mkt.loc[mask_train,\"vix_close\"].values, w),\n",
        "            \"dvix_mean\": wmean(mkt.loc[mask_train,\"dvix\"].values, w) if \"dvix\" in mkt.columns else np.nan,\n",
        "            \"breadth_mean\": wmean(mkt.loc[mask_train,\"breadth\"].values, w),\n",
        "            \"ret_q05\":  wq05 (mkt.loc[mask_train,\"spy_ret\"].values, w),\n",
        "        })\n",
        "    prof_df = pd.DataFrame(prof)\n",
        "\n",
        "    risk_off_id = int(prof_df[\"rv20_mean\"].idxmax())\n",
        "    risk_on_id  = int(prof_df[\"ret_mean\"].idxmax())\n",
        "    label_map = {risk_on_id: \"Risk-On\", risk_off_id: \"Risk-Off\"}\n",
        "    for s in range(K):\n",
        "        if s not in label_map:\n",
        "            label_map[s] = \"Transition\"\n",
        "\n",
        "    return {\n",
        "        \"profiles\": prof_df.to_dict(orient=\"records\"),\n",
        "        \"label_map\": {int(k): v for k,v in label_map.items()},\n",
        "        \"posteriors_shape\": list(post.shape),\n",
        "        \"transmat\": model.transmat_.tolist(),\n",
        "    }\n",
        "\n",
        "def _agreement_vs_baseline(new_states: np.ndarray, baseline_states: np.ndarray) -> float:\n",
        "    # simple percent agreement\n",
        "    if len(new_states) != len(baseline_states):\n",
        "        return np.nan\n",
        "    return float((new_states == baseline_states).mean())\n",
        "\n",
        "# --- Load baseline label sequence (we'll compare to *smoothed* if present) ---\n",
        "base_labels = pd.read_parquet(LABELS_PATH).sort_values(\"date\")\n",
        "base_labels[\"date\"] = pd.to_datetime(base_labels[\"date\"])  # <- add this\n",
        "\n",
        "if \"state_id_smoothed\" in base_labels.columns:\n",
        "    base_states = base_labels[\"state_id_smoothed\"].to_numpy()\n",
        "else:\n",
        "    base_states = base_labels[\"state_id\"].to_numpy()\n",
        "\n",
        "results: Dict[str, Any] = {\"k_sensitivity\": [], \"feature_sensitivity\": [], \"era_stability\": [], \"bootstrap\": {}}\n",
        "\n",
        "def _score_states_on_valid_dates(model, feats, scaler_local):\n",
        "    full_df = mkt[[\"date\"] + feats].dropna().reset_index(drop=True)\n",
        "    Xa = scaler_local.transform(full_df[feats].to_numpy(dtype=float))\n",
        "    states = model.predict_proba(Xa).argmax(axis=1)\n",
        "    return full_df[\"date\"].to_numpy(), states\n",
        "\n",
        "def _agreement_on_intersection(dates_new, states_new, base_labels_df) -> float:\n",
        "    df_new = pd.DataFrame({\"date\": dates_new, \"state_new\": states_new})\n",
        "    df_join = df_new.merge(\n",
        "        base_labels_df[[\"date\", \"state_id_smoothed\" if \"state_id_smoothed\" in base_labels_df.columns else \"state_id\"]]\n",
        "        .rename(columns={\"state_id_smoothed\":\"state_base\",\"state_id\":\"state_base\"}),\n",
        "        on=\"date\", how=\"inner\"\n",
        "    )\n",
        "    if len(df_join) == 0:\n",
        "        return np.nan\n",
        "    return float((df_join[\"state_new\"].to_numpy() == df_join[\"state_base\"].to_numpy()).mean())\n",
        "\n",
        "# 1) K sensitivity ------------------------------------------------------------\n",
        "K_GRID = [2, 3]  # TOCHANGE: can expand to [2,3] in real run if currently narrowed\n",
        "for k in K_GRID:\n",
        "    best = {\"score\": -np.inf, \"meta\": None}\n",
        "    for r in range(n_init):\n",
        "        rs = rand0 + r\n",
        "        fit = _fit_hmm_for_features(features_base, k, rs, mask_train)\n",
        "        meta = _profile_and_label(fit[\"model\"], features_base, fit[\"scaler\"])\n",
        "        dates_scored, states_all = _score_states_on_valid_dates(fit[\"model\"], features_base, fit[\"scaler\"])\n",
        "        agree = _agreement_on_intersection(dates_scored, states_all, base_labels)\n",
        "\n",
        "        entry = {\n",
        "            \"k\": k, \"seed\": rs, \"score\": fit[\"score\"], \"agreement_vs_baseline\": agree,\n",
        "            \"profiles\": meta[\"profiles\"], \"label_map\": meta[\"label_map\"],\n",
        "            \"transmat\": meta[\"transmat\"]\n",
        "        }\n",
        "        if fit[\"score\"] > best[\"score\"]:\n",
        "            best = {\"score\": fit[\"score\"], \"meta\": entry}\n",
        "    results[\"k_sensitivity\"].append(best[\"meta\"])\n",
        "\n",
        "# 2) Feature sensitivity -------------------------------------------------------\n",
        "# Define variants relative to baseline features\n",
        "fsets = []\n",
        "fsets.append((\"baseline\", features_base))\n",
        "if \"vix_close\" in features_base: fsets.append((\"no_vix\", [f for f in features_base if f!=\"vix_close\"]))\n",
        "if \"breadth\"   in features_base: fsets.append((\"no_breadth\", [f for f in features_base if f!=\"breadth\"]))\n",
        "if \"dvix\"      in features_base: fsets.append((\"no_dvix\", [f for f in features_base if f!=\"dvix\"]))\n",
        "# minimal core set\n",
        "core = [f for f in [\"spy_rv_20\",\"vix_close\"] if f in mkt.columns]\n",
        "if core: fsets.append((\"core_rv_vix\", core))\n",
        "\n",
        "for name, feats in fsets:\n",
        "    k = k_base\n",
        "    best = {\"score\": -np.inf, \"meta\": None}\n",
        "    for r in range(n_init):\n",
        "        rs = rand0 + 100 + r\n",
        "        fit = _fit_hmm_for_features(feats, k, rs, mask_train)\n",
        "        meta = _profile_and_label(fit[\"model\"], feats, fit[\"scaler\"])\n",
        "        dates_scored, states_all = _score_states_on_valid_dates(fit[\"model\"], feats, fit[\"scaler\"])\n",
        "        agree = _agreement_on_intersection(dates_scored, states_all, base_labels)\n",
        "        entry = {\n",
        "            \"feature_set\": name, \"k\": k, \"seed\": rs, \"feats\": feats,\n",
        "            \"score\": fit[\"score\"], \"agreement_vs_baseline\": agree,\n",
        "            \"profiles\": meta[\"profiles\"], \"label_map\": meta[\"label_map\"],\n",
        "            \"transmat\": meta[\"transmat\"]\n",
        "        }\n",
        "        if fit[\"score\"] > best[\"score\"]:\n",
        "            best = {\"score\": fit[\"score\"], \"meta\": entry}\n",
        "    results[\"feature_sensitivity\"].append(best[\"meta\"])\n",
        "\n",
        "\n",
        "# 3) Era stability -------------------------------------------------------------\n",
        "def _fit_on_era(start: str, end: str, k: int, seed: int, feats: List[str], name: str) -> Dict[str, Any]:\n",
        "    mask = (mkt[\"date\"] >= pd.to_datetime(start)) & (mkt[\"date\"] <= pd.to_datetime(end))\n",
        "    fit  = _fit_hmm_for_features(feats, k, seed, mask)\n",
        "    meta = _profile_and_label(fit[\"model\"], feats, fit[\"scaler\"])\n",
        "    return {\n",
        "        \"era\": name, \"k\": k, \"seed\": seed, \"score\": fit[\"score\"],\n",
        "        \"profiles\": meta[\"profiles\"], \"label_map\": meta[\"label_map\"],\n",
        "        \"transmat\": meta[\"transmat\"], \"start\": str(start), \"end\": str(end)\n",
        "    }\n",
        "\n",
        "\n",
        "# ⬇️ OPTIONAL TIDY-UP — Bootstrap: use a local scaler for baseline features too\n",
        "df_tr = mkt.loc[mask_train, [\"date\"] + features_base].dropna().reset_index(drop=True)\n",
        "scaler_base_local = StandardScaler().fit(df_tr[features_base].to_numpy(dtype=float))  # local\n",
        "X_tr  = scaler_base_local.transform(df_tr[features_base].to_numpy(dtype=float))\n",
        "dt_tr = df_tr[\"date\"].to_numpy()\n",
        "\n",
        "eras = [\n",
        "    (\"pre_2015\",  \"2007-02-06\", \"2014-12-31\"),\n",
        "    (\"post_2015\", \"2015-01-01\", str(train_end.date())),\n",
        "    (\"crisis_2020\",\"2020-02-15\",\"2020-12-31\"),\n",
        "]\n",
        "for name, s, e in eras:\n",
        "    rs = rand0 + hash(name) % 1000\n",
        "    results[\"era_stability\"].append(_fit_on_era(s, e, k_base, rs, features_base, name))\n",
        "\n",
        "\n",
        "# 4) Bootstrap (block) ---------------------------------------------------------\n",
        "def _block_bootstrap_indices(n: int, block: int, n_blocks: int, rng: np.random.RandomState):\n",
        "    starts = rng.randint(0, n, size=n_blocks)\n",
        "    idx = []\n",
        "    for st in starts:\n",
        "        idx.extend([(st + j) % n for j in range(block)])\n",
        "    return np.array(idx[:n])\n",
        "\n",
        "BOOT_REPS   = 5    # TOCHANGE: 100–300 for real run\n",
        "BLOCK_DAYS  = 20   # TOCHANGE: 20–60 depending on serial corr\n",
        "rng = np.random.RandomState(rand0+999)\n",
        "\n",
        "# Build train matrix for bootstrap using local scaler\n",
        "df_tr = mkt.loc[mask_train, [\"date\"] + features_base].dropna().reset_index(drop=True)\n",
        "scaler_base_local = StandardScaler().fit(df_tr[features_base].to_numpy(dtype=float))\n",
        "X_tr  = scaler_base_local.transform(df_tr[features_base].to_numpy(dtype=float))\n",
        "dt_tr = df_tr[\"date\"].to_numpy()\n",
        "\n",
        "boot_summ = {\"k\": k_base, \"reps\": BOOT_REPS, \"block_days\": BLOCK_DAYS, \"agreement_vs_baseline\": []}\n",
        "for b in range(BOOT_REPS):\n",
        "    idx = _block_bootstrap_indices(len(dt_tr), BLOCK_DAYS, max(1, len(dt_tr)//BLOCK_DAYS), rng)\n",
        "    Xb  = X_tr[idx]\n",
        "\n",
        "    model = GaussianHMM(\n",
        "        n_components=k_base, covariance_type=covtype, n_iter=n_iter, tol=tol,\n",
        "        random_state=rand0 + 500 + b, verbose=False, init_params=\"mc\", params=\"stmc\"\n",
        "    )\n",
        "    T0 = np.full((k_base, k_base), (1.0 - 0.90) / max(1, k_base - 1)); np.fill_diagonal(T0, 0.90)\n",
        "    model.transmat_ = T0; model.startprob_ = np.full(k_base, 1.0 / k_base)\n",
        "    model.fit(Xb, lengths=[len(Xb)])\n",
        "    model.transmat_ = _diag_sticky_blend(model.transmat_, lam_stick)\n",
        "\n",
        "    # Score on valid dates & align to baseline\n",
        "    dates_scored, states_all = _score_states_on_valid_dates(model, features_base, scaler_base_local)\n",
        "    agree = _agreement_on_intersection(dates_scored, states_all, base_labels)\n",
        "    boot_summ[\"agreement_vs_baseline\"].append(agree)\n",
        "\n",
        "boot_summ[\"agreement_mean\"] = float(np.mean(boot_summ[\"agreement_vs_baseline\"]))\n",
        "boot_summ[\"agreement_std\"]  = float(np.std (boot_summ[\"agreement_vs_baseline\"]))\n",
        "results[\"bootstrap\"] = boot_summ\n",
        "\n",
        "# Save results\n",
        "with open(OUT_PATH, \"w\") as f:\n",
        "    json.dump({\n",
        "        \"created_at\": datetime.utcnow().isoformat() + \"Z\",\n",
        "        \"inputs\": {\n",
        "            \"features_base\": features_base,\n",
        "            \"k_base\": k_base,\n",
        "            \"recency_weighting\": recency,\n",
        "            \"half_life_days\": hl_days,\n",
        "            \"lam_stick\": lam_stick,\n",
        "            \"n_iter\": n_iter, \"n_init\": n_init, \"tol\": tol, \"covariance_type\": covtype\n",
        "        },\n",
        "        \"results\": results\n",
        "    }, f, indent=2)\n",
        "\n",
        "print(json.dumps({\n",
        "    \"status\": \"2.5 done\",\n",
        "    \"out\": OUT_PATH,\n",
        "    \"k_choices\": [2,3],\n",
        "    \"feature_sets_tested\": [fs[0] for fs in fsets],\n",
        "    \"bootstrap\": {\"reps\": BOOT_REPS, \"agreement_mean\": boot_summ[\"agreement_mean\"], \"agreement_std\": boot_summ[\"agreement_std\"]},\n",
        "}, indent=2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGWBeVKSMGWt",
        "outputId": "cdfa7d01-140f-4024-a6d4-9945b4d4ddd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"status\": \"2.5 done\",\n",
            "  \"out\": \"artifacts/regimes/regime_sensitivity.json\",\n",
            "  \"k_choices\": [\n",
            "    2,\n",
            "    3\n",
            "  ],\n",
            "  \"feature_sets_tested\": [\n",
            "    \"baseline\",\n",
            "    \"no_vix\",\n",
            "    \"no_breadth\",\n",
            "    \"no_dvix\",\n",
            "    \"core_rv_vix\"\n",
            "  ],\n",
            "  \"bootstrap\": {\n",
            "    \"reps\": 5,\n",
            "    \"agreement_mean\": 0.531172176899957,\n",
            "    \"agreement_std\": 0.247859990522504\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Section 2.6 — Diagnostics & QA\n",
        "# Plots:\n",
        "#   • Timeline with regime shading over SPY price (rebased) & drawdown\n",
        "#   • Posterior probabilities (stacked area)\n",
        "#   • State return histograms, QQ plots\n",
        "#   • Transition matrix heatmap, dwell-time distribution\n",
        "# Tables:\n",
        "#   • State profiles (load from 2.3), transition matrix & steady-state\n",
        "#   • Switch frequency & chattering metrics\n",
        "# Alerts:\n",
        "#   • Inconsistent semantics (e.g., positive mean but highest vol)\n",
        "#   • Very short dwell (median < 3d)\n",
        "#   • Mapping flips / excessive chattering\n",
        "# Reuses (do not recompute):\n",
        "#   - artifacts/regimes/market_panel.parquet (2.0)\n",
        "#   - artifacts/regimes/window_manifest.json (2.1)\n",
        "#   - artifacts/regimes/regime_hmm.pkl (2.2)\n",
        "#   - artifacts/regimes/regime_labels.parquet (2.3/2.4)\n",
        "#   - artifacts/regimes/state_profiles.csv (2.3)\n",
        "#   - artifacts/regimes/regime_meta.json (2.3/2.4)\n",
        "# Outputs:\n",
        "#   - artifacts/regimes/diagnostics/*.png\n",
        "#   - artifacts/regimes/diagnostics/*.csv / *.json\n",
        "#   - console summary + alerts\n",
        "# Notes:\n",
        "#   - #TOCHANGE marks spots to bump for real run (heavier plots/points)\n",
        "# ============================================================\n",
        "\n",
        "from __future__ import annotations\n",
        "import os, json\n",
        "from typing import Dict, Any, List, Tuple\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "REGIME_DIR  = CFG.regime_dir\n",
        "DIAG_DIR    = os.path.join(REGIME_DIR, \"diagnostics\")\n",
        "os.makedirs(DIAG_DIR, exist_ok=True)\n",
        "\n",
        "PANEL_PATH  = os.path.join(REGIME_DIR, \"market_panel.parquet\")\n",
        "MAN_PATH    = os.path.join(REGIME_DIR, \"window_manifest.json\")\n",
        "BUNDLE_PATH = os.path.join(REGIME_DIR, \"regime_hmm.pkl\")\n",
        "LABELS_PATH = os.path.join(REGIME_DIR, \"regime_labels.parquet\")\n",
        "META_PATH   = os.path.join(REGIME_DIR, \"regime_meta.json\")\n",
        "PROF_PATH   = os.path.join(REGIME_DIR, \"state_profiles.csv\")\n",
        "\n",
        "# --- Load artifacts\n",
        "assert os.path.exists(PANEL_PATH) and os.path.exists(MAN_PATH) and os.path.exists(BUNDLE_PATH) and os.path.exists(LABELS_PATH)\n",
        "\n",
        "mkt = pd.read_parquet(PANEL_PATH).sort_values(\"date\").reset_index(drop=True)\n",
        "mkt[\"date\"] = pd.to_datetime(mkt[\"date\"])\n",
        "labels = pd.read_parquet(LABELS_PATH).sort_values(\"date\").reset_index(drop=True)\n",
        "labels[\"date\"] = pd.to_datetime(labels[\"date\"])\n",
        "\n",
        "with open(MAN_PATH, \"r\") as f:\n",
        "    MAN = json.load(f)\n",
        "\n",
        "bundle = joblib.load(BUNDLE_PATH)\n",
        "features = bundle[\"features\"]\n",
        "k = int(bundle[\"k\"])\n",
        "\n",
        "meta = {}\n",
        "if os.path.exists(META_PATH):\n",
        "    with open(META_PATH, \"r\") as f:\n",
        "        meta = json.load(f)\n",
        "\n",
        "# --- Derived / convenience\n",
        "date_equal = np.array_equal(mkt[\"date\"].values, labels[\"date\"].values)\n",
        "if not date_equal:\n",
        "    # Align by inner-join on date (some rows may be dropped if any side had NA)\n",
        "    labels = labels.merge(mkt[[\"date\"]], on=\"date\", how=\"inner\").sort_values(\"date\").reset_index(drop=True)\n",
        "    mkt    = mkt.merge(labels[[\"date\"]], on=\"date\", how=\"inner\").sort_values(\"date\").reset_index(drop=True)\n",
        "\n",
        "# Prefer smoothed series if present\n",
        "state_col  = \"state_id_smoothed\" if \"state_id_smoothed\" in labels.columns else \"state_id\"\n",
        "label_col  = \"regime_label_smoothed\" if \"regime_label_smoothed\" in labels.columns else \"regime_label\"\n",
        "\n",
        "# K from posterior columns p0..pK-1 (robust to re-fits)\n",
        "p_cols = [c for c in labels.columns if c.startswith(\"p\")]\n",
        "K = len(p_cols) if len(p_cols) > 0 else k\n",
        "\n",
        "# --- Helpers\n",
        "def _runs(series: np.ndarray) -> List[Tuple[int,int,int]]:\n",
        "    \"\"\"Return (start_idx, end_idx, value) runs for integer state series.\"\"\"\n",
        "    out = []\n",
        "    s = 0; cur = series[0]\n",
        "    for i in range(1, len(series)):\n",
        "        if series[i] != cur:\n",
        "            out.append((s, i-1, int(cur)))\n",
        "            s = i; cur = series[i]\n",
        "    out.append((s, len(series)-1, int(cur)))\n",
        "    return out\n",
        "\n",
        "def _transition_matrix(states: np.ndarray, K: int) -> np.ndarray:\n",
        "    T = np.zeros((K, K), dtype=float)\n",
        "    for i in range(len(states)-1):\n",
        "        T[states[i], states[i+1]] += 1.0\n",
        "    row_sums = T.sum(axis=1, keepdims=True)\n",
        "    row_sums[row_sums==0] = 1.0\n",
        "    return T / row_sums\n",
        "\n",
        "def _steady_state(T: np.ndarray) -> np.ndarray:\n",
        "    # Empirical steady-state as left eigenvector (or fallback to state freq)\n",
        "    try:\n",
        "        vals, vecs = np.linalg.eig(T.T)\n",
        "        i = np.argmin(np.abs(vals - 1.0))\n",
        "        v = np.real(vecs[:, i]); v = np.maximum(v, 0)\n",
        "        if v.sum() == 0: raise ValueError\n",
        "        return v / v.sum()\n",
        "    except Exception:\n",
        "        return np.ones(T.shape[0]) / T.shape[0]\n",
        "\n",
        "def _rebase_price_from_returns(rets: np.ndarray, start=100.0) -> np.ndarray:\n",
        "    # Assumes rets are simple daily returns (e.g., spy_ret). If logrets, replace with exp(cumsum).\n",
        "    out = np.empty_like(rets, dtype=float); out[0] = start * (1.0 + np.nan_to_num(rets[0], nan=0.0))\n",
        "    for i in range(1, len(rets)):\n",
        "        out[i] = out[i-1] * (1.0 + np.nan_to_num(rets[i], nan=0.0))\n",
        "    return out\n",
        "\n",
        "def _drawdown(price: np.ndarray) -> np.ndarray:\n",
        "    cummax = np.maximum.accumulate(price)\n",
        "    dd = price / np.where(cummax==0, 1.0, cummax) - 1.0\n",
        "    return dd\n",
        "\n",
        "# --- Compute core diagnostics\n",
        "states = labels[state_col].to_numpy(dtype=int)\n",
        "Tmat   = _transition_matrix(states, K)\n",
        "ss_emp = _steady_state(Tmat)\n",
        "runs   = _runs(states)\n",
        "dwell  = pd.DataFrame({\n",
        "    \"state_id\": [st for (s,e,st) in runs],\n",
        "    \"run_len\":  [e-s+1 for (s,e,st) in runs],\n",
        "})\n",
        "\n",
        "# --- Switch/chattering metrics\n",
        "switches = (states[1:] != states[:-1]).sum()\n",
        "switch_rate = switches / max(1, len(states)-1)\n",
        "one_day_runs = (dwell[\"run_len\"] == 1).mean()  # fraction single-day\n",
        "lt3_runs = (dwell[\"run_len\"] < 3).mean()\n",
        "\n",
        "# --- Load state profiles (2.3) if present, else compute from TRAIN weights\n",
        "profiles_df = None\n",
        "if os.path.exists(PROF_PATH):\n",
        "    profiles_df = pd.read_csv(PROF_PATH)\n",
        "else:\n",
        "    # Fallback: rough unweighted per-state profiles on all data (not ideal, but avoids recompute)\n",
        "    tmp = []\n",
        "    for s in range(K):\n",
        "        mask = (states == s)\n",
        "        tmp.append({\n",
        "            \"state_id\": s,\n",
        "            \"ret_mean\": float(np.nanmean(mkt.loc[mask,\"spy_ret\"])),\n",
        "            \"ret_std\":  float(np.nanstd (mkt.loc[mask,\"spy_ret\"])),\n",
        "            \"rv20_mean\": float(np.nanmean(mkt.loc[mask,\"spy_rv_20\"])),\n",
        "            \"vix_mean\":  float(np.nanmean(mkt.loc[mask,\"vix_close\"])),\n",
        "            \"dvix_mean\": float(np.nanmean(mkt.loc[mask,\"dvix\"])) if \"dvix\" in mkt.columns else np.nan,\n",
        "            \"breadth_mean\": float(np.nanmean(mkt.loc[mask,\"breadth\"])),\n",
        "            \"ret_q05\":  float(np.nanquantile(mkt.loc[mask,\"spy_ret\"], 0.05)),\n",
        "        })\n",
        "    profiles_df = pd.DataFrame(tmp)\n",
        "profiles_df.to_csv(os.path.join(DIAG_DIR, \"state_profiles_table.csv\"), index=False)\n",
        "\n",
        "# --- Transition matrix & steady-state tables\n",
        "pd.DataFrame(Tmat, columns=[f\"to_{i}\" for i in range(K)], index=[f\"from_{i}\" for i in range(K)]) \\\n",
        "  .to_csv(os.path.join(DIAG_DIR, \"transition_matrix.csv\"))\n",
        "pd.DataFrame({\"state_id\": list(range(K)), \"steady_state_prob\": ss_emp}) \\\n",
        "  .to_csv(os.path.join(DIAG_DIR, \"steady_state.csv\"), index=False)\n",
        "\n",
        "# --- Switch frequency table (yearly)\n",
        "lab = labels[[\"date\", state_col]].copy()\n",
        "lab[\"year\"] = lab[\"date\"].dt.year\n",
        "lab[\"sw\"] = (lab[state_col].shift(-1) != lab[state_col]).astype(int)\n",
        "switch_by_year = lab.groupby(\"year\")[\"sw\"].sum().reset_index().rename(columns={\"sw\":\"n_switches\"})\n",
        "switch_by_year.to_csv(os.path.join(DIAG_DIR, \"switches_by_year.csv\"), index=False)\n",
        "\n",
        "# ============================================================\n",
        "# PLOTS\n",
        "# ============================================================\n",
        "\n",
        "# 1) Price timeline with regime shading & drawdown\n",
        "spy_price = _rebase_price_from_returns(mkt[\"spy_ret\"].to_numpy(dtype=float), start=100.0)\n",
        "spy_dd    = _drawdown(spy_price)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 5))\n",
        "ax.plot(mkt[\"date\"], spy_price, lw=1.25)\n",
        "# Shade by regime\n",
        "for (s,e,st) in runs:\n",
        "    ax.axvspan(mkt[\"date\"].iloc[s], mkt[\"date\"].iloc[e], alpha=0.15, label=f\"State {st}\" if s==runs[0][0] else None)\n",
        "ax.set_title(\"SPY (rebased) with Regime Shading\")\n",
        "ax.set_xlabel(\"Date\"); ax.set_ylabel(\"Rebased Price\")\n",
        "fig.tight_layout()\n",
        "fig.savefig(os.path.join(DIAG_DIR, \"regime_timeline.png\"), dpi=150)\n",
        "plt.close(fig)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 3))\n",
        "ax.plot(mkt[\"date\"], spy_dd, lw=1.0)\n",
        "ax.set_title(\"SPY Drawdown\")\n",
        "ax.set_xlabel(\"Date\"); ax.set_ylabel(\"Drawdown\")\n",
        "fig.tight_layout()\n",
        "fig.savefig(os.path.join(DIAG_DIR, \"timeline_drawdown.png\"), dpi=150)\n",
        "plt.close(fig)\n",
        "\n",
        "# 2) Posterior probabilities (stacked area)\n",
        "if len(p_cols) == K:\n",
        "    fig, ax = plt.subplots(figsize=(12, 4))\n",
        "    ax.stackplot(labels[\"date\"], *(labels[c].to_numpy() for c in p_cols))\n",
        "    ax.set_ylim(0,1); ax.set_title(\"Posterior Probabilities (Stacked)\")\n",
        "    ax.set_xlabel(\"Date\"); ax.set_ylabel(\"Probability\")\n",
        "    fig.tight_layout()\n",
        "    fig.savefig(os.path.join(DIAG_DIR, \"regime_posteriors.png\"), dpi=150)\n",
        "    plt.close(fig)\n",
        "\n",
        "\n",
        "# (erfinv helper without SciPy)\n",
        "def erfinv(y):\n",
        "    # Approximation (Winitzki) good enough for QQ visual; replace with SciPy in real run\n",
        "    a = 0.147\n",
        "    sign = np.sign(y)\n",
        "    x = np.clip(y, -0.999999, 0.999999)\n",
        "    ln = np.log(1 - x**2)\n",
        "    first = 2/(np.pi*a) + ln/2\n",
        "    return sign * np.sqrt( np.sqrt(first**2 - ln/a) - first )\n",
        "\n",
        "# 3) State return histograms + QQ plots\n",
        "# TOCHANGE: bump N_QQ_POINTS to 1000 for real run\n",
        "N_QQ_POINTS = 200\n",
        "qs = np.linspace(0.01, 0.99, N_QQ_POINTS)\n",
        "for s in range(K):\n",
        "    mask = (states == s)\n",
        "    r = mkt.loc[mask, \"spy_ret\"].dropna().to_numpy()\n",
        "    if len(r) == 0:\n",
        "        continue\n",
        "\n",
        "    # Histogram\n",
        "    fig, ax = plt.subplots(figsize=(5,3))\n",
        "    ax.hist(r, bins=40, alpha=0.8)  # #TOCHANGE: 80 bins for real run\n",
        "    ax.set_title(f\"State {s} return histogram\")\n",
        "    fig.tight_layout()\n",
        "    fig.savefig(os.path.join(DIAG_DIR, f\"state_{s}_ret_hist.png\"), dpi=150)\n",
        "    plt.close(fig)\n",
        "\n",
        "    # QQ vs normal\n",
        "    mu, sd = float(np.mean(r)), float(np.std(r, ddof=0))\n",
        "    if sd <= 0:\n",
        "        sd = 1e-9\n",
        "    emp_q = np.quantile(r, qs)\n",
        "    nor_q = mu + sd * np.sqrt(2) * erfinv(2*qs - 1)  # inverse CDF via erfinv\n",
        "    fig, ax = plt.subplots(figsize=(5,3))\n",
        "    ax.scatter(nor_q, emp_q, s=6, alpha=0.7)\n",
        "    lims = [min(nor_q.min(), emp_q.min()), max(nor_q.max(), emp_q.max())]\n",
        "    ax.plot(lims, lims, lw=1.0)\n",
        "    ax.set_title(f\"State {s} QQ vs Normal\")\n",
        "    ax.set_xlabel(\"Theoretical quantiles\"); ax.set_ylabel(\"Empirical quantiles\")\n",
        "    fig.tight_layout()\n",
        "    fig.savefig(os.path.join(DIAG_DIR, f\"state_{s}_qq.png\"), dpi=150)\n",
        "    plt.close(fig)\n",
        "\n",
        "\n",
        "# 4) Transition heatmap\n",
        "fig, ax = plt.subplots(figsize=(5,4))\n",
        "im = ax.imshow(Tmat, aspect=\"auto\", vmin=0, vmax=np.max(Tmat))\n",
        "ax.set_title(\"Transition Matrix\")\n",
        "ax.set_xlabel(\"to\"); ax.set_ylabel(\"from\")\n",
        "ax.set_xticks(range(K)); ax.set_yticks(range(K))\n",
        "fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
        "fig.tight_layout()\n",
        "fig.savefig(os.path.join(DIAG_DIR, \"transition_matrix_heatmap.png\"), dpi=150)\n",
        "plt.close(fig)\n",
        "\n",
        "# 5) Dwell-time distribution per state\n",
        "fig, ax = plt.subplots(figsize=(6,4))\n",
        "for s in range(K):\n",
        "    ax.hist(dwell.loc[dwell[\"state_id\"]==s, \"run_len\"], bins=range(1,51), alpha=0.6, label=f\"State {s}\")\n",
        "ax.legend()\n",
        "ax.set_title(\"Dwell-time distribution (days)\")\n",
        "ax.set_xlabel(\"Run length (days)\"); ax.set_ylabel(\"Count\")\n",
        "fig.tight_layout()\n",
        "fig.savefig(os.path.join(DIAG_DIR, \"dwell_time_distribution.png\"), dpi=150)\n",
        "plt.close(fig)\n",
        "\n",
        "# ============================================================\n",
        "# ALERTS\n",
        "# ============================================================\n",
        "alerts = []\n",
        "\n",
        "# Semantics: positive mean but highest vol -> suspicious \"Risk-On\"\n",
        "if profiles_df is not None and len(profiles_df) >= K:\n",
        "    vol_rank = profiles_df[\"rv20_mean\"].rank(ascending=True)  # 1 = lowest vol\n",
        "    hi_vol_state = int(profiles_df.loc[profiles_df[\"rv20_mean\"].idxmax(), \"state_id\"])\n",
        "    pos_mean_states = profiles_df.loc[profiles_df[\"ret_mean\"] > 0, \"state_id\"].astype(int).tolist()\n",
        "    if hi_vol_state in pos_mean_states and K >= 2:\n",
        "        alerts.append(f\"State {hi_vol_state}: positive mean return but highest realized vol (check semantics).\")\n",
        "\n",
        "# Dwell-time < 3 days median\n",
        "dwell_median = dwell.groupby(\"state_id\")[\"run_len\"].median()\n",
        "for s, med in dwell_median.items():\n",
        "    if med < 3:\n",
        "        alerts.append(f\"State {s}: median dwell {med}d < 3 (too chatty).\")\n",
        "\n",
        "# Chattering: high switch rate or many single-day runs\n",
        "if switch_rate > 0.15:  # #TOCHANGE: tighten to 0.10 for real run\n",
        "    alerts.append(f\"High switch rate: {switch_rate:.2%}\")\n",
        "if one_day_runs > 0.10:  # #TOCHANGE: tighten to 0.05 for real run\n",
        "    alerts.append(f\"Single-day run fraction elevated: {one_day_runs:.2%}\")\n",
        "\n",
        "# Mapping flips heuristic: compare label continuity around major drawdowns\n",
        "# (simple heuristic: if label changes >3 times within any 20-day window)\n",
        "# #TOCHANGE: widen window to 60 days for real run\n",
        "WINDOW = 20\n",
        "roll_switch = pd.Series((states[1:] != states[:-1]).astype(int)).rolling(WINDOW).sum().fillna(0)\n",
        "if (roll_switch > 3).any():\n",
        "    alerts.append(\"Frequent label flips in short windows (potential mapping instability).\")\n",
        "\n",
        "# Save alerts\n",
        "with open(os.path.join(DIAG_DIR, \"alerts.json\"), \"w\") as f:\n",
        "    json.dump({\"alerts\": alerts}, f, indent=2)\n",
        "\n",
        "# Save a compact summary CSV\n",
        "pd.DataFrame({\n",
        "    \"metric\": [\"K\", \"switches\", \"switch_rate\", \"one_day_runs_frac\", \"lt3_runs_frac\"],\n",
        "    \"value\": [K, switches, switch_rate, one_day_runs, lt3_runs]\n",
        "}).to_csv(os.path.join(DIAG_DIR, \"summary_metrics.csv\"), index=False)\n",
        "\n",
        "print(json.dumps({\n",
        "    \"status\": \"2.6 diagnostics complete\",\n",
        "    \"plots_dir\": DIAG_DIR,\n",
        "    \"alerts_count\": len(alerts),\n",
        "    \"notes\": [\n",
        "        \"State profiles loaded from 2.3 if available; else quick fallback was used.\",\n",
        "        \"Semantics checks are heuristics; confirm with 2.3 profiles and 2.5 sensitivity.\"\n",
        "    ]\n",
        "}, indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWuaoM4eZ57C",
        "outputId": "bd012e23-7c94-4430-d14c-58143e017742"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"status\": \"2.6 diagnostics complete\",\n",
            "  \"plots_dir\": \"artifacts/regimes/diagnostics\",\n",
            "  \"alerts_count\": 1,\n",
            "  \"notes\": [\n",
            "    \"State profiles loaded from 2.3 if available; else quick fallback was used.\",\n",
            "    \"Semantics checks are heuristics; confirm with 2.3 profiles and 2.5 sensitivity.\"\n",
            "  ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Section 2.7 — Regime-Aware Policy Hooks (Interfaces to Sec 3–5)\n",
        "# Reuses:\n",
        "#   - artifacts/regimes/regime_labels.parquet (2.3/2.4)\n",
        "#   - artifacts/regimes/regime_meta.json (2.3/2.4)\n",
        "#   - artifacts/regimes/regime_hmm.pkl (2.2)  [fallback if p-cols missing]\n",
        "#   - artifacts/regimes/window_manifest.json (2.1)\n",
        "#   - artifacts/regimes/market_panel.parquet (2.0)  [fallback scoring]\n",
        "# Outputs:\n",
        "#   - artifacts/regimes/regime_policy_map.json\n",
        "# Notes:\n",
        "#   - This file is the single interface consumed by Sections 3–5.\n",
        "#   - #TOCHANGE marks values to tune for the real run.\n",
        "# ============================================================\n",
        "\n",
        "from __future__ import annotations\n",
        "import os, json, hashlib\n",
        "from typing import Dict, Any\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "REGIME_DIR  = CFG.regime_dir\n",
        "PANEL_PATH  = os.path.join(REGIME_DIR, \"market_panel.parquet\")\n",
        "LABELS_PATH = os.path.join(REGIME_DIR, \"regime_labels.parquet\")\n",
        "META_PATH   = os.path.join(REGIME_DIR, \"regime_meta.json\")\n",
        "MAN_PATH    = os.path.join(REGIME_DIR, \"window_manifest.json\")\n",
        "BUNDLE_PATH = os.path.join(REGIME_DIR, \"regime_hmm.pkl\")\n",
        "OUT_PATH    = os.path.join(REGIME_DIR, \"regime_policy_map.json\")\n",
        "\n",
        "# --- Load essentials\n",
        "labels = pd.read_parquet(LABELS_PATH).sort_values(\"date\").reset_index(drop=True)\n",
        "labels[\"date\"] = pd.to_datetime(labels[\"date\"])\n",
        "with open(MAN_PATH, \"r\") as f: MAN = json.load(f)\n",
        "bundle = joblib.load(BUNDLE_PATH)\n",
        "\n",
        "# state→label semantics\n",
        "state_label_map = None\n",
        "if os.path.exists(META_PATH):\n",
        "    with open(META_PATH, \"r\") as f:\n",
        "        meta = json.load(f)\n",
        "    state_label_map = meta.get(\"state_label_map\", None)\n",
        "else:\n",
        "    meta = {}\n",
        "\n",
        "# infer K and get posteriors\n",
        "p_cols = [c for c in labels.columns if c.startswith(\"p\")]\n",
        "K = len(p_cols) if p_cols else int(bundle[\"k\"])\n",
        "\n",
        "# fallback: if no p-cols, score from model on all dates\n",
        "if not p_cols:\n",
        "    feats = bundle[\"features\"]\n",
        "    scaler = joblib.load(bundle[\"scaler_path\"])\n",
        "    mkt = pd.read_parquet(PANEL_PATH).sort_values(\"date\").reset_index(drop=True)\n",
        "    X_all = scaler.transform(mkt[feats].to_numpy(dtype=float))\n",
        "    post = bundle[\"model\"].predict_proba(X_all)\n",
        "    for s in range(post.shape[1]):\n",
        "        labels[f\"p{s}\"] = post[:, s]\n",
        "    p_cols = [f\"p{s}\" for s in range(K)]\n",
        "\n",
        "# choose smoothed ids/labels if available\n",
        "state_col = \"state_id_smoothed\" if \"state_id_smoothed\" in labels.columns else \"state_id\"\n",
        "label_col = \"regime_label_smoothed\" if \"regime_label_smoothed\" in labels.columns else \"regime_label\"\n",
        "\n",
        "# if meta has mapping but label_col missing, map on the fly\n",
        "if label_col not in labels.columns and state_label_map is not None:\n",
        "    labels[label_col] = labels[state_col].map({int(k): v for k, v in state_label_map.items()})\n",
        "\n",
        "# --- Confidence proxies\n",
        "def entropy(p: np.ndarray) -> float:\n",
        "    p = np.clip(p, 1e-12, 1.0)\n",
        "    return float(-(p*np.log(p)).sum() / np.log(len(p)))  # normalized to [0,1]\n",
        "\n",
        "def aggressiveness_from_confidence(p: np.ndarray) -> Dict[str, float]:\n",
        "    # Proxy 1: max posterior\n",
        "    c_max = float(p.max())\n",
        "    # Proxy 2: 1 - normalized entropy (higher -> more certain)\n",
        "    c_ent = 1.0 - entropy(p)\n",
        "    # Combine (simple mean)  #TOCHANGE: use weighted combo or monotone spline\n",
        "    c = 0.5 * (c_max + c_ent)\n",
        "    # Map to aggressiveness scalar g ∈ [g_min, g_max]\n",
        "    g_min, g_max = 0.35, 1.00     #TOCHANGE: (0.25,1.00) if you want deeper throttling\n",
        "    g = g_min + (g_max - g_min) * c\n",
        "    return {\"c_max\": c_max, \"c_entropy\": c_ent, \"c\": c, \"g\": g}\n",
        "\n",
        "# --- Latest regime & confidence (optionally smooth over last N days)\n",
        "#TOCHANGE: set N_SMOOTH=5–10 for prod; 1 for fast test\n",
        "N_SMOOTH = 3\n",
        "tail = labels.tail(N_SMOOTH)\n",
        "p_tail = tail[p_cols].to_numpy(dtype=float)\n",
        "p_mean = p_tail.mean(axis=0)\n",
        "latest_row = labels.iloc[-1]\n",
        "latest_label = str(latest_row[label_col]) if label_col in labels.columns else f\"State{int(latest_row[state_col])}\"\n",
        "conf = aggressiveness_from_confidence(p_mean)\n",
        "\n",
        "# --- Per-regime policy defaults (edit for your stack)\n",
        "# Use intuitive names; downstream can match by these labels\n",
        "# Turnover caps are relative (e.g., fraction of portfolio eligible to trade)\n",
        "policy_by_regime: Dict[str, Dict[str, Any]] = {\n",
        "    \"Risk-On\": {\n",
        "        \"weights_multipliers\": {          #TOCHANGE: tailor to your factors\n",
        "            \"momentum\": 1.20,\n",
        "            \"quality\":  1.00,\n",
        "            \"value\":    1.00,\n",
        "            \"low_vol\":  0.85,\n",
        "        },\n",
        "        \"turnover_cap\": 0.20,             #TOCHANGE: 0.25\n",
        "        \"risk_target_vol_annual\": 0.10,   # 10%\n",
        "        \"hedge_intensity\": 0.0,           # baseline hedge ratio\n",
        "    },\n",
        "    \"Transition\": {\n",
        "        \"weights_multipliers\": {\n",
        "            \"momentum\": 0.95,\n",
        "            \"quality\":  1.05,\n",
        "            \"value\":    1.05,\n",
        "            \"low_vol\":  1.05,\n",
        "        },\n",
        "        \"turnover_cap\": 0.15,\n",
        "        \"risk_target_vol_annual\": 0.08,   # 8%\n",
        "        \"hedge_intensity\": 0.15,\n",
        "    },\n",
        "    \"Risk-Off\": {\n",
        "        \"weights_multipliers\": {\n",
        "            \"momentum\": 0.70,             # throttle momo\n",
        "            \"quality\":  1.15,             # upweight quality/defensive\n",
        "            \"value\":    1.05,\n",
        "            \"low_vol\":  1.25,\n",
        "        },\n",
        "        \"turnover_cap\": 0.10,\n",
        "        \"risk_target_vol_annual\": 0.06,   # 6%\n",
        "        \"hedge_intensity\": 0.35,\n",
        "    },\n",
        "}\n",
        "\n",
        "# --- If our label universe differs (e.g., only 2 states), coerce keys\n",
        "present_labels = set(labels[label_col].dropna().astype(str).unique()) if label_col in labels.columns else set()\n",
        "for lbl in list(policy_by_regime.keys()):\n",
        "    if lbl not in present_labels and present_labels:\n",
        "        # map missing labels to a reasonable fallback  #TOCHANGE: make explicit mapping per run\n",
        "        del policy_by_regime[lbl]\n",
        "# If states are only numeric (no semantic labels), synthesize keys\n",
        "if not policy_by_regime and state_label_map is None:\n",
        "    unique_states = sorted(labels[state_col].unique())\n",
        "    for s in unique_states:\n",
        "        policy_by_regime[f\"State{s}\"] = {\n",
        "            \"weights_multipliers\": {\"momentum\":1.0,\"quality\":1.0,\"value\":1.0,\"low_vol\":1.0},\n",
        "            \"turnover_cap\": 0.15, \"risk_target_vol_annual\": 0.08, \"hedge_intensity\": 0.15,\n",
        "        }\n",
        "\n",
        "# --- Global scaling by confidence g (downstream can apply this linearly)\n",
        "# We expose both the raw confidence and recommend common scalings.\n",
        "scaling = {\n",
        "    \"aggressiveness_scalar_g\": conf[\"g\"],\n",
        "    \"confidence\": conf,                           # contains c_max, c_entropy, c (combined)\n",
        "    \"recommendations\": {\n",
        "        # Downstream usage suggestions\n",
        "        \"scale_position_sizes_by_g\": True,\n",
        "        \"scale_turnover_cap_by_g\": True,\n",
        "        \"scale_hedge_intensity_by_(1-g)\": True\n",
        "    }\n",
        "}\n",
        "\n",
        "# --- Package the full map\n",
        "out = {\n",
        "    \"created_at\": pd.Timestamp.utcnow().isoformat() + \"Z\",\n",
        "    \"latest_date\": str(latest_row[\"date\"].date()),\n",
        "    \"k\": int(K),\n",
        "    \"latest_regime_label\": latest_label,\n",
        "    \"latest_state_id\": int(latest_row[state_col]),\n",
        "    \"latest_posteriors\": {f\"p{s}\": float(latest_row.get(f\"p{s}\", np.nan)) for s in range(K)},\n",
        "    \"confidence\": scaling,\n",
        "    \"policy_by_regime\": policy_by_regime,\n",
        "    \"inputs\": {\n",
        "        \"labels_path\": LABELS_PATH,\n",
        "        \"meta_path\": META_PATH,\n",
        "        \"bundle_path\": BUNDLE_PATH,\n",
        "        \"scaler_path\": MAN[\"scaler_path\"],\n",
        "        \"features\": bundle[\"features\"],\n",
        "        \"window\": MAN.get(\"window\", {}),\n",
        "        \"smoothing_window_days\": N_SMOOTH,  #TOCHANGE\n",
        "    },\n",
        "}\n",
        "\n",
        "# include sensitivity & diagnostics pointers if present\n",
        "sens_path = os.path.join(REGIME_DIR, \"regime_sensitivity.json\")\n",
        "diag_dir  = os.path.join(REGIME_DIR, \"diagnostics\")\n",
        "if os.path.exists(sens_path):\n",
        "    out[\"inputs\"][\"sensitivity_path\"] = sens_path\n",
        "if os.path.isdir(diag_dir):\n",
        "    out[\"inputs\"][\"diagnostics_dir\"] = diag_dir\n",
        "\n",
        "# hash a minimal signature (useful for caching / auditing)\n",
        "sig = hashlib.sha256(json.dumps({\n",
        "    \"features\": out[\"inputs\"][\"features\"],\n",
        "    \"window\": out[\"inputs\"][\"window\"],\n",
        "    \"k\": out[\"k\"]\n",
        "}, sort_keys=True).encode()).hexdigest()\n",
        "out[\"signature\"] = sig\n",
        "\n",
        "with open(OUT_PATH, \"w\") as f:\n",
        "    json.dump(out, f, indent=2)\n",
        "\n",
        "print(json.dumps({\n",
        "    \"status\": \"2.7 policy hooks exported\",\n",
        "    \"out\": OUT_PATH,\n",
        "    \"latest_label\": latest_label,\n",
        "    \"g_scalar\": round(out[\"confidence\"][\"aggressiveness_scalar_g\"], 4),\n",
        "}, indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLOwjNqprmtB",
        "outputId": "28f1a552-6bb3-4560-a66d-53b748766501"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"status\": \"2.7 policy hooks exported\",\n",
            "  \"out\": \"artifacts/regimes/regime_policy_map.json\",\n",
            "  \"latest_label\": \"Risk-On\",\n",
            "  \"g_scalar\": 0.9681\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Section 2.8 — Walk-Forward Integration\n",
        "# - Rolling/expanding windows (match Section 6 when available)\n",
        "# - Fit scaler+HMM on TRAIN; score/label TEST ONLY\n",
        "# - Save per-window artifacts; stitch into a continuous timeline\n",
        "# - Preserve state→label semantics per window (no drift)\n",
        "# Reuses:\n",
        "#   - CFG.regime_dir (from 2.0)\n",
        "#   - artifacts/regimes/market_panel.parquet (2.0)\n",
        "#   - artifacts/regimes/window_manifest.json (2.1; single-window fallback)\n",
        "#   - artifacts/regimes/windows_manifest.json (preferred multi-window; else autogen)\n",
        "#   - artifacts/regimes/regime_sensitivity.json (2.5, optional pointer)\n",
        "#   - artifacts/regimes/diagnostics/* (2.6, optional pointer)\n",
        "# Outputs:\n",
        "#   - artifacts/regimes/windowed/regime_labels_<winid>.parquet (+ .csv)\n",
        "#   - artifacts/regimes/windowed/regime_hmm_<winid>.pkl\n",
        "#   - artifacts/regimes/windowed/regime_meta_<winid>.json\n",
        "#   - artifacts/regimes/regime_labels.parquet (+ .csv)  [stitched TEST]\n",
        "# Notes:\n",
        "#   - #TOCHANGE marks heavier settings for real run.\n",
        "# ============================================================\n",
        "\n",
        "from __future__ import annotations\n",
        "import os, json\n",
        "from typing import Dict, Any, List, Tuple\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from hmmlearn.hmm import GaussianHMM\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "REGIME_DIR   = CFG.regime_dir\n",
        "PANEL_PATH   = os.path.join(REGIME_DIR, \"market_panel.parquet\")\n",
        "MAN_SINGLE   = os.path.join(REGIME_DIR, \"window_manifest.json\")        # from 2.1 (single window)\n",
        "MAN_WINDOWS  = os.path.join(REGIME_DIR, \"windows_manifest.json\")       # preferred (multi)\n",
        "OUT_DIR_WIN  = os.path.join(REGIME_DIR, \"windowed\")\n",
        "os.makedirs(OUT_DIR_WIN, exist_ok=True)\n",
        "\n",
        "assert os.path.exists(PANEL_PATH), f\"Missing market panel: {PANEL_PATH}\"\n",
        "mkt = pd.read_parquet(PANEL_PATH).sort_values(\"date\").reset_index(drop=True)\n",
        "mkt[\"date\"] = pd.to_datetime(mkt[\"date\"])\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "# 0) Windows manifest: prefer multi-window; else autogen a LIGHT test manifest (#TOCHANGE)\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "def _autogen_windows(dates: pd.Series) -> List[Dict[str, Any]]:\n",
        "    # LIGHT test plan: 2 small rolling windows (#TOCHANGE for real run)\n",
        "    # Real run suggestion (#TOCHANGE): expanding train of ~5–8y; test 6–12m; stride 6m\n",
        "    dmin, dmax = dates.min(), dates.max()\n",
        "    # crude splits for quick smoke test: last ~4y span\n",
        "    cuts = [\n",
        "        {\"win_id\":\"W1\", \"train_start\":str((dmin + pd.Timedelta(days=365*3)).date()),\n",
        "         \"train_end\":  str((dmin + pd.Timedelta(days=365*9)).date()),\n",
        "         \"test_start\": str((dmin + pd.Timedelta(days=365*9)+pd.Timedelta(days=1)).date()),\n",
        "         \"test_end\":   str((dmin + pd.Timedelta(days=365*10)).date())},\n",
        "        {\"win_id\":\"W2\", \"train_start\":str((dmin + pd.Timedelta(days=365*4)).date()),\n",
        "         \"train_end\":  str((dmin + pd.Timedelta(days=365*10)).date()),\n",
        "         \"test_start\": str((dmin + pd.Timedelta(days=365*10)+pd.Timedelta(days=1)).date()),\n",
        "         \"test_end\":   str(dmax.date())},\n",
        "    ]\n",
        "    return cuts\n",
        "\n",
        "if os.path.exists(MAN_WINDOWS):\n",
        "    with open(MAN_WINDOWS, \"r\") as f:\n",
        "        WINS = json.load(f)\n",
        "elif os.path.exists(MAN_SINGLE):\n",
        "    # Wrap the single window as a one-window WF run\n",
        "    with open(MAN_SINGLE, \"r\") as f:\n",
        "        man = json.load(f)\n",
        "    WINS = [{\n",
        "        \"win_id\":\"W0\",\n",
        "        \"train_start\": man[\"window\"][\"train_start\"],\n",
        "        \"train_end\":   man[\"window\"][\"train_end\"],\n",
        "        \"test_start\":  man[\"window\"][\"test_start\"],\n",
        "        \"test_end\":    man[\"window\"][\"test_end\"],\n",
        "    }]\n",
        "else:\n",
        "    # autogen LIGHT test manifest (#TOCHANGE)\n",
        "    WINS = _autogen_windows(mkt[\"date\"])\n",
        "    with open(MAN_WINDOWS, \"w\") as f:\n",
        "        json.dump(WINS, f, indent=2)\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "# 1) HMM hyperparams — reuse Section 2.2 defaults; mark heavier real-run values\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "N_ITER = 200            # TOCHANGE: 1000 for real run\n",
        "N_INIT = 2              # TOCHANGE: 10 for real run\n",
        "COVTYPE = \"full\"\n",
        "TOL = 1e-3              # TOCHANGE: 1e-4 for real run\n",
        "RANDOM_STATE = 42\n",
        "LAMBDA_STICK = 0.15     # TOCHANGE: 0.30–0.50 for real run\n",
        "K = 3                   # TOCHANGE: choose from sensitivity (2 or 3); default 3\n",
        "\n",
        "APPLY_RECENCY   = True  # reuse the finance recency rule from 2.2\n",
        "HALF_LIFE_DAYS  = 756   # TOCHANGE: try 504/756/1260\n",
        "SEG_LEN         = 60    # TOCHANGE: 90–120\n",
        "N_SEGMENTS      = 80    # TOCHANGE: 200–400\n",
        "EPSILON_FLOOR   = 0.10  # TOCHANGE: 0.05–0.15\n",
        "\n",
        "FEATURES = CFG.hmm_features.copy()\n",
        "if getattr(CFG, \"include_dvix\", False) and \"dvix\" in mkt.columns and \"dvix\" not in FEATURES:\n",
        "    FEATURES.append(\"dvix\")\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "# 2) Utilities reused from 2.2/2.3/2.4\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "def _diag_sticky_blend(T: np.ndarray, lam: float) -> np.ndarray:\n",
        "    k = T.shape[0]\n",
        "    out = (1.0 - lam) * T + lam * np.eye(k)\n",
        "    return out / out.sum(axis=1, keepdims=True)\n",
        "\n",
        "def _build_time_decay_weights(dates: np.ndarray, half_life_days: int) -> np.ndarray:\n",
        "    t = np.array([pd.Timestamp(d).toordinal() for d in dates], dtype=float)\n",
        "    age = (t.max() - t)\n",
        "    decay = np.log(2) / max(1, half_life_days)\n",
        "    w = np.exp(-decay * age)\n",
        "    return w / (w.sum() + 1e-12)\n",
        "\n",
        "def _sample_time_weighted_subsequences(\n",
        "    X: np.ndarray, dates: np.ndarray,\n",
        "    seg_len: int=SEG_LEN, n_segments: int=N_SEGMENTS, half_life_days: int=HALF_LIFE_DAYS, seed: int=RANDOM_STATE\n",
        ") -> Tuple[np.ndarray, List[int]]:\n",
        "    rng = np.random.RandomState(seed)\n",
        "    n = X.shape[0]\n",
        "    if n < seg_len:\n",
        "        return X.copy(), [n]\n",
        "    ends = np.arange(seg_len - 1, n)\n",
        "    p = _build_time_decay_weights(dates[ends], half_life_days)\n",
        "    p = np.maximum(p, EPSILON_FLOOR * p.max()); p = p / p.sum()\n",
        "    chosen = rng.choice(ends, size=min(n_segments, len(ends)), replace=True, p=p)\n",
        "    chunks, lengths = [], []\n",
        "    for e in chosen:\n",
        "        s = e - (seg_len - 1)\n",
        "        chunks.append(X[s:e+1]); lengths.append(seg_len)\n",
        "    return np.vstack(chunks), lengths\n",
        "\n",
        "def _fit_hmm(X_train: np.ndarray, dates_train: np.ndarray, k: int, seed: int) -> GaussianHMM:\n",
        "    if APPLY_RECENCY:\n",
        "        X_fit, lengths = _sample_time_weighted_subsequences(X_train, dates_train, seg_len=SEG_LEN, n_segments=N_SEGMENTS, half_life_days=HALF_LIFE_DAYS, seed=seed)\n",
        "    else:\n",
        "        X_fit, lengths = X_train, [len(X_train)]\n",
        "    model = GaussianHMM(\n",
        "        n_components=k, covariance_type=COVTYPE, n_iter=N_ITER, tol=TOL,\n",
        "        random_state=seed, verbose=False, init_params=\"mc\", params=\"stmc\"\n",
        "    )\n",
        "    T0 = np.full((k, k), (1.0 - 0.90) / max(1, k - 1)); np.fill_diagonal(T0, 0.90)\n",
        "    model.transmat_ = T0; model.startprob_ = np.full(k, 1.0 / k)\n",
        "    model.fit(X_fit, lengths=lengths)\n",
        "    model.transmat_ = _diag_sticky_blend(model.transmat_, LAMBDA_STICK)\n",
        "    return model\n",
        "\n",
        "def _label_states(df_tr: pd.DataFrame, st_train: np.ndarray) -> Dict[int, str]:\n",
        "    # df_tr is TRAIN-only and aligned to st_train\n",
        "    assert len(df_tr) == len(st_train), \"Train data and train states length mismatch\"\n",
        "    tmp = []\n",
        "    states_unique = sorted(np.unique(st_train))\n",
        "    for s in states_unique:\n",
        "        mask = (st_train == s)\n",
        "        def wmean(x, w):\n",
        "            w = np.asarray(w, float); x = np.asarray(x, float)\n",
        "            z = w.sum()\n",
        "            return float((x*w).sum()/z) if z > 0 else np.nan\n",
        "        tmp.append({\n",
        "            \"state_id\":  s,\n",
        "            \"ret_mean\":  wmean(df_tr[\"spy_ret\"].values, mask),\n",
        "            \"rv20_mean\": wmean(df_tr[\"spy_rv_20\"].values, mask) if \"spy_rv_20\" in df_tr else np.nan,\n",
        "            \"vix_mean\":  wmean(df_tr[\"vix_close\"].values, mask) if \"vix_close\" in df_tr else np.nan,\n",
        "            \"breadth_mean\": wmean(df_tr[\"breadth\"].values, mask) if \"breadth\" in df_tr else np.nan,\n",
        "        })\n",
        "    prof = pd.DataFrame(tmp)\n",
        "    risk_off_id = int(prof[\"rv20_mean\"].idxmax())\n",
        "    risk_on_id  = int(prof[\"ret_mean\"].idxmax())\n",
        "    mapping = {risk_on_id: \"Risk-On\", risk_off_id: \"Risk-Off\"}\n",
        "    for s in states_unique:\n",
        "        if s not in mapping:\n",
        "            mapping[s] = \"Transition\"\n",
        "    return mapping\n",
        "\n",
        "def _debounce_series(state_ids: np.ndarray, min_dwell_days: int=CFG.min_dwell_days) -> np.ndarray:\n",
        "    # 2.4 minimal debounce: enforce min dwell by suppressing singleton flips\n",
        "    out = state_ids.copy()\n",
        "    i = 1\n",
        "    while i < len(out)-1:\n",
        "        if out[i] != out[i-1] and out[i] != out[i+1]:\n",
        "            out[i] = out[i-1]  # squash 1-day blip\n",
        "            i += 1\n",
        "        i += 1\n",
        "    # NOTE: #TOCHANGE implement full min-run-length >= min_dwell_days if needed\n",
        "    return out\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "# 3) Walk-forward loop\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "stitched = []  # list[DataFrame] of TEST chunks with posteriors + labels\n",
        "win_summ = []\n",
        "\n",
        "for w in WINS:\n",
        "    win_id = w.get(\"win_id\", f\"W{len(win_summ)}\")\n",
        "    ts, te = pd.to_datetime(w[\"train_start\"]), pd.to_datetime(w[\"train_end\"])\n",
        "    us, ue = pd.to_datetime(w[\"test_start\"]),  pd.to_datetime(w[\"test_end\"])\n",
        "    m_train = (mkt[\"date\"] >= ts) & (mkt[\"date\"] <= te)\n",
        "    m_test  = (mkt[\"date\"] >= us) & (mkt[\"date\"] <= ue)\n",
        "    df_tr = mkt.loc[m_train, [\"date\", \"spy_ret\"] + FEATURES].dropna().reset_index(drop=True)\n",
        "    df_te = mkt.loc[m_test,  [\"date\", \"spy_ret\"] + FEATURES].dropna().reset_index(drop=True)\n",
        "\n",
        "    if df_tr.empty or df_te.empty:\n",
        "        continue\n",
        "\n",
        "    scaler = StandardScaler().fit(df_tr[FEATURES].to_numpy(dtype=float))\n",
        "    X_tr = scaler.transform(df_tr[FEATURES].to_numpy(dtype=float))\n",
        "    X_te = scaler.transform(df_te[FEATURES].to_numpy(dtype=float))\n",
        "\n",
        "    # Fit HMM (single best run for speed; #TOCHANGE run N_INIT restarts, keep best)\n",
        "    seed = RANDOM_STATE\n",
        "    model = _fit_hmm(X_tr, df_tr[\"date\"].to_numpy(), k=K, seed=seed)\n",
        "\n",
        "    # Score TEST only\n",
        "    post = model.predict_proba(X_te)\n",
        "    hard = post.argmax(axis=1)\n",
        "\n",
        "    # Map semantics using TRAIN (no peeking)\n",
        "    # We need states on TRAIN for profiling; use predict_proba on train too (cheap)\n",
        "    st_train = model.predict_proba(X_tr).argmax(axis=1)\n",
        "    mapping  = _label_states(df_tr, st_train)\n",
        "    lbl_test = pd.Series([mapping.get(int(s), f\"State{int(s)}\") for s in hard], index=df_te.index)\n",
        "\n",
        "    # Debounce (light)\n",
        "    hard_db = _debounce_series(hard, min_dwell_days=CFG.min_dwell_days)\n",
        "\n",
        "    # Package per-window labels\n",
        "    out_cols = {\n",
        "        \"date\": df_te[\"date\"],\n",
        "        \"state_id\": hard,\n",
        "        \"state_id_smoothed\": hard_db,\n",
        "        \"regime_label\": lbl_test.values,\n",
        "        \"regime_label_smoothed\": pd.Series([mapping.get(int(s), f\"State{int(s)}\") for s in hard_db], index=df_te.index).values,\n",
        "    }\n",
        "    # Add posterior columns p0..pK-1\n",
        "    for s in range(post.shape[1]):\n",
        "        out_cols[f\"p{s}\"] = post[:, s]\n",
        "    lab_te = pd.DataFrame(out_cols).sort_values(\"date\").reset_index(drop=True)\n",
        "\n",
        "    # Save per-window artifacts\n",
        "    # Bundle: model + scaler + meta knobs for traceability\n",
        "    bundle = {\n",
        "        \"model\": model,\n",
        "        \"k\": K,\n",
        "        \"features\": FEATURES,\n",
        "        \"scaler\": scaler,  # keep in-bundle object; also persist path below if desired\n",
        "        \"random_state\": seed,\n",
        "        \"n_iter\": N_ITER, \"n_init\": N_INIT, \"tol\": TOL, \"covariance_type\": COVTYPE,\n",
        "        \"recency_weighting\": APPLY_RECENCY, \"recency_half_life_days\": HALF_LIFE_DAYS,\n",
        "        \"recency_seg_len\": SEG_LEN, \"recency_n_segments\": N_SEGMENTS, \"recency_epsilon_floor\": EPSILON_FLOOR,\n",
        "        \"sticky_lambda\": LAMBDA_STICK,\n",
        "        \"train_dates\": [str(d) for d in df_tr[\"date\"].to_numpy()],\n",
        "        \"test_dates\":  [str(d) for d in df_te[\"date\"].to_numpy()],\n",
        "        \"created_at\": datetime.utcnow().isoformat() + \"Z\",\n",
        "        \"fit_mode\": \"recency\" if APPLY_RECENCY else \"plain\",\n",
        "    }\n",
        "    bpath = os.path.join(OUT_DIR_WIN, f\"regime_hmm_{win_id}.pkl\")\n",
        "    joblib.dump(bundle, bpath)\n",
        "\n",
        "    meta = {\n",
        "        \"win_id\": win_id,\n",
        "        \"window\": {\"train_start\": str(ts.date()), \"train_end\": str(te.date()), \"test_start\": str(us.date()), \"test_end\": str(ue.date())},\n",
        "        \"features\": FEATURES,\n",
        "        \"k\": K,\n",
        "        \"state_label_map\": {int(k): v for k, v in mapping.items()},\n",
        "        \"sticky_lambda\": LAMBDA_STICK,\n",
        "        \"recency\": {\"enabled\": APPLY_RECENCY, \"half_life_days\": HALF_LIFE_DAYS, \"seg_len\": SEG_LEN, \"n_segments\": N_SEGMENTS, \"epsilon_floor\": EPSILON_FLOOR},\n",
        "        \"bundle_path\": bpath,\n",
        "        \"panel_path\": PANEL_PATH,\n",
        "        \"sensitivity_path\": os.path.join(REGIME_DIR, \"regime_sensitivity.json\") if os.path.exists(os.path.join(REGIME_DIR, \"regime_sensitivity.json\")) else None,\n",
        "        \"diagnostics_dir\": os.path.join(REGIME_DIR, \"diagnostics\") if os.path.isdir(os.path.join(REGIME_DIR, \"diagnostics\")) else None,\n",
        "        \"created_at\": datetime.utcnow().isoformat() + \"Z\",\n",
        "    }\n",
        "    mpath = os.path.join(OUT_DIR_WIN, f\"regime_meta_{win_id}.json\")\n",
        "    with open(mpath, \"w\") as f:\n",
        "        json.dump(meta, f, indent=2)\n",
        "\n",
        "    # Always write CSV alongside parquet  #TOCHANGE: keep in prod when many windows\n",
        "    lpath = os.path.join(OUT_DIR_WIN, f\"regime_labels_{win_id}.parquet\")\n",
        "    lab_te.to_parquet(lpath, index=False)\n",
        "    try:\n",
        "        lab_te.to_csv(lpath.replace(\".parquet\", \".csv\"), index=False)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    stitched.append(lab_te)\n",
        "    win_summ.append({\n",
        "        \"win_id\": win_id,\n",
        "        \"train_start\": str(ts.date()),\n",
        "        \"train_end\":   str(te.date()),\n",
        "        \"test_start\":  str(us.date()),\n",
        "        \"test_end\":    str(ue.date()),\n",
        "        \"n_train\": int(len(df_tr)),\n",
        "        \"n_test\": int(len(df_te)),\n",
        "        \"bundle_path\": bpath,\n",
        "        \"labels_path\": lpath,\n",
        "        \"meta_path\": mpath,\n",
        "    })\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "# 4) Save a windows index (QoL)\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "idx_json = os.path.join(OUT_DIR_WIN, \"windows_index.json\")\n",
        "idx_csv  = os.path.join(OUT_DIR_WIN, \"windows_index.csv\")\n",
        "with open(idx_json, \"w\") as f:\n",
        "    json.dump(win_summ, f, indent=2)\n",
        "try:\n",
        "    pd.DataFrame(win_summ).to_csv(idx_csv, index=False)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "# 5) Stitch all TEST chunks into one continuous timeline\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "if len(stitched) == 0:\n",
        "    raise RuntimeError(\"No window produced test labels; check window coverage.\")\n",
        "\n",
        "lab_all = pd.concat(stitched, axis=0, ignore_index=True).sort_values(\"date\").reset_index(drop=True)\n",
        "lab_all.to_parquet(os.path.join(REGIME_DIR, \"regime_labels.parquet\"), index=False)\n",
        "try:\n",
        "    lab_all.to_csv(os.path.join(REGIME_DIR, \"regime_labels.csv\"), index=False)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "print(json.dumps({\n",
        "    \"status\": \"2.8 walk-forward complete\",\n",
        "    \"n_windows\": len(win_summ),\n",
        "    \"windows\": win_summ,\n",
        "    \"stitched_out\": {\n",
        "        \"parquet\": os.path.join(REGIME_DIR, \"regime_labels.parquet\"),\n",
        "        \"csv\": os.path.join(REGIME_DIR, \"regime_labels.csv\"),\n",
        "    },\n",
        "    \"windows_index\": {\n",
        "    \"json\": idx_json,\n",
        "    \"csv\": idx_csv\n",
        "    },\n",
        "    \"notes\": [\n",
        "        \"Per-window scaler fitted on TRAIN only; TEST scored out-of-sample.\",\n",
        "        \"State→label semantics are saved per window and applied to the test chunk.\",\n",
        "        \"For real run: increase N_ITER/N_INIT and recency sampler size; align windows with Section 6.\"\n",
        "    ]\n",
        "}, indent=2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FcgDjXn_sub",
        "outputId": "1c2bbc26-8b99-4560-b08f-37c8b0372cb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"status\": \"2.8 walk-forward complete\",\n",
            "  \"n_windows\": 1,\n",
            "  \"windows\": [\n",
            "    {\n",
            "      \"win_id\": \"W0\",\n",
            "      \"train_start\": \"2007-02-06\",\n",
            "      \"train_end\": \"2016-12-30\",\n",
            "      \"test_start\": \"2017-01-03\",\n",
            "      \"test_end\": \"2025-08-11\",\n",
            "      \"n_train\": 2495,\n",
            "      \"n_test\": 2163,\n",
            "      \"bundle_path\": \"artifacts/regimes/windowed/regime_hmm_W0.pkl\",\n",
            "      \"labels_path\": \"artifacts/regimes/windowed/regime_labels_W0.parquet\",\n",
            "      \"meta_path\": \"artifacts/regimes/windowed/regime_meta_W0.json\"\n",
            "    }\n",
            "  ],\n",
            "  \"stitched_out\": {\n",
            "    \"parquet\": \"artifacts/regimes/regime_labels.parquet\",\n",
            "    \"csv\": \"artifacts/regimes/regime_labels.csv\"\n",
            "  },\n",
            "  \"windows_index\": {\n",
            "    \"json\": \"artifacts/regimes/windowed/windows_index.json\",\n",
            "    \"csv\": \"artifacts/regimes/windowed/windows_index.csv\"\n",
            "  },\n",
            "  \"notes\": [\n",
            "    \"Per-window scaler fitted on TRAIN only; TEST scored out-of-sample.\",\n",
            "    \"State\\u2192label semantics are saved per window and applied to the test chunk.\",\n",
            "    \"For real run: increase N_ITER/N_INIT and recency sampler size; align windows with Section 6.\"\n",
            "  ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Section 2.9 — Forward (Shadow) Mode\n",
        "# Daily update:\n",
        "#   • Load latest window bundle (model+scaler+features) and meta\n",
        "#   • Read newest feature rows (from Section 1 products via market_panel.parquet)\n",
        "#   • Transform → predict_proba → state_id → regime_label (via saved mapping)\n",
        "#   • Append to regime_labels.parquet (+ CSV), no backfilling\n",
        "# Retrain cadence: weekly/bi-weekly (#TOCHANGE)\n",
        "# Logging: JSONL with model hash/date/posteriors/label\n",
        "# Alerts: simple chattering/dwell anomaly (rolling window)\n",
        "# Reuses:\n",
        "#   - CFG.regime_dir (2.0)\n",
        "#   - artifacts/regimes/windowed/windows_index.json (2.8)\n",
        "#   - artifacts/regimes/windowed/regime_meta_<winid>.json (2.8)\n",
        "#   - artifacts/regimes/windowed/regime_hmm_<winid>.pkl (2.8)\n",
        "#   - artifacts/regimes/market_panel.parquet (2.0)\n",
        "#   - artifacts/regimes/regime_labels.parquet (2.8 stitched history)\n",
        "#   - artifacts/regimes/regime_policy_map.json (2.7; optional refresh)\n",
        "# Notes:\n",
        "#   - #TOCHANGE marks production choices (smoothing, policy refresh cadence, thresholds).\n",
        "# ============================================================\n",
        "\n",
        "from __future__ import annotations\n",
        "import os, json, hashlib\n",
        "from typing import Dict, Any, List\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "# Paths (reuse CFG from 2.0)\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "REGIME_DIR   = CFG.regime_dir\n",
        "PANEL_PATH   = os.path.join(REGIME_DIR, \"market_panel.parquet\")\n",
        "LAB_PATH_PQ  = os.path.join(REGIME_DIR, \"regime_labels.parquet\")\n",
        "LAB_PATH_CSV = os.path.join(REGIME_DIR, \"regime_labels.csv\")\n",
        "WIN_DIR      = os.path.join(REGIME_DIR, \"windowed\")\n",
        "WIN_INDEX    = os.path.join(WIN_DIR, \"windows_index.json\")\n",
        "\n",
        "# Optional: Section 1 global (won't fail if missing)\n",
        "START_DATE = globals().get(\"START_DATE\", None)\n",
        "\n",
        "# Optional policy refresh (2.7-lite)\n",
        "UPDATE_POLICY_MAP = True   # TOCHANGE: set True for prod daily refresh, False for fast tests\n",
        "POLICY_OUT        = os.path.join(REGIME_DIR, \"regime_policy_map.json\")\n",
        "\n",
        "# Forward log & alerts\n",
        "FWD_LOG   = os.path.join(REGIME_DIR, \"regime_forward_log.jsonl\")\n",
        "ALERTS_FP = os.path.join(REGIME_DIR, \"forward_alerts.json\")\n",
        "\n",
        "# Smoothing / chattering guardrails\n",
        "FWD_DEBOUNCE    = False    # TOCHANGE: consider True in prod (requires short context window)\n",
        "ROLL_WINDOW_D   = 20       # TOCHANGE: 60 for prod\n",
        "ROLL_MAX_SWITCH = 4        # TOCHANGE: 3 for prod\n",
        "\n",
        "# Confidence tail length for policy scaling\n",
        "N_CONF_TAIL = 3            # TOCHANGE: 5–10 for prod\n",
        "\n",
        "os.makedirs(WIN_DIR, exist_ok=True)\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "# Helpers\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "def _load_latest_window_meta() -> Dict[str, Any]:\n",
        "    \"\"\"Pick the window with the latest test_end from windows_index.json; fallback: scan meta files.\"\"\"\n",
        "    if os.path.exists(WIN_INDEX):\n",
        "        with open(WIN_INDEX, \"r\") as f:\n",
        "            idx = json.load(f)\n",
        "        if isinstance(idx, list) and len(idx) > 0:\n",
        "            # sort by test_end\n",
        "            idx_sorted = sorted(idx, key=lambda d: d.get(\"test_end\", \"\"), reverse=True)\n",
        "            return idx_sorted[0]\n",
        "    # Fallback: scan meta files\n",
        "    metas = [p for p in os.listdir(WIN_DIR) if p.startswith(\"regime_meta_\") and p.endswith(\".json\")]\n",
        "    if not metas:\n",
        "        raise FileNotFoundError(\"No window meta files found; run 2.8 first.\")\n",
        "    # choose the lexicographically latest as a fallback heuristic\n",
        "    metas.sort(reverse=True)\n",
        "    with open(os.path.join(WIN_DIR, metas[0]), \"r\") as f:\n",
        "        m = json.load(f)\n",
        "    return {\n",
        "        \"win_id\": m.get(\"win_id\", \"W?\"),\n",
        "        \"bundle_path\": m.get(\"bundle_path\"),\n",
        "        \"meta_path\": os.path.join(WIN_DIR, metas[0]),\n",
        "        \"labels_path\": None,\n",
        "        \"test_end\": m.get(\"window\", {}).get(\"test_end\", \"\"),\n",
        "    }\n",
        "\n",
        "def _model_signature(features: List[str], k: int, window: Dict[str, Any]) -> str:\n",
        "    return hashlib.sha256(json.dumps({\"features\": features, \"k\": k, \"window\": window}, sort_keys=True).encode()).hexdigest()\n",
        "\n",
        "def _entropy(p: np.ndarray) -> float:\n",
        "    p = np.clip(p, 1e-12, 1.0)\n",
        "    return float(-(p*np.log(p)).sum() / np.log(len(p)))\n",
        "\n",
        "def _aggressiveness_from_posterior(p_mean: np.ndarray) -> Dict[str, float]:\n",
        "    c_max = float(p_mean.max())\n",
        "    c_ent = 1.0 - _entropy(p_mean)\n",
        "    c = 0.5 * (c_max + c_ent)\n",
        "    g_min, g_max = 0.35, 1.00  # TOCHANGE\n",
        "    g = g_min + (g_max - g_min) * c\n",
        "    return {\"c_max\": c_max, \"c_entropy\": c_ent, \"c\": c, \"g\": g}\n",
        "\n",
        "def _append_jsonl(path: str, rec: Dict[str, Any]) -> None:\n",
        "    with open(path, \"a\") as f:\n",
        "        f.write(json.dumps(rec) + \"\\n\")\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "# Load artifacts\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "assert os.path.exists(PANEL_PATH), f\"Missing market panel: {PANEL_PATH}\"\n",
        "panel = pd.read_parquet(PANEL_PATH).sort_values(\"date\").reset_index(drop=True)\n",
        "panel[\"date\"] = pd.to_datetime(panel[\"date\"])\n",
        "\n",
        "meta_idx = _load_latest_window_meta()\n",
        "assert meta_idx.get(\"bundle_path\"), \"Latest window has no bundle_path; re-run 2.8.\"\n",
        "with open(meta_idx.get(\"meta_path\") or os.path.join(WIN_DIR, f\"regime_meta_{meta_idx['win_id']}.json\"), \"r\") as f:\n",
        "    META = json.load(f)\n",
        "\n",
        "BUNDLE = joblib.load(meta_idx[\"bundle_path\"])\n",
        "MODEL  = BUNDLE[\"model\"]\n",
        "SCALER = BUNDLE[\"scaler\"]\n",
        "FEATS  = BUNDLE[\"features\"]\n",
        "K      = int(BUNDLE[\"k\"])\n",
        "\n",
        "# State→label semantics\n",
        "state_label_map: Dict[int, str] = META.get(\"state_label_map\", {})\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "# Determine the forward slice (dates to append)\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "if os.path.exists(LAB_PATH_PQ):\n",
        "    labels_hist = pd.read_parquet(LAB_PATH_PQ).sort_values(\"date\").reset_index(drop=True)\n",
        "    labels_hist[\"date\"] = pd.to_datetime(labels_hist[\"date\"])\n",
        "    last_date_done = labels_hist[\"date\"].max()\n",
        "else:\n",
        "    labels_hist = None\n",
        "    # If no file yet, use START_DATE if provided, else take panel min-1\n",
        "    last_date_done = pd.to_datetime(START_DATE) - pd.Timedelta(days=1) if START_DATE else panel[\"date\"].min() - pd.Timedelta(days=1)\n",
        "\n",
        "new_mask = panel[\"date\"] > last_date_done\n",
        "to_score = panel.loc[new_mask, [\"date\"] + FEATS].dropna().reset_index(drop=True)\n",
        "\n",
        "if to_score.empty:\n",
        "    print(json.dumps({\n",
        "        \"status\": \"2.9 forward: nothing to do\",\n",
        "        \"last_processed\": str(last_date_done.date()) if pd.notnull(last_date_done) else None,\n",
        "        \"note\": \"No new rows in market_panel.parquet beyond current regime_labels.\"\n",
        "    }, indent=2))\n",
        "else:\n",
        "    # ─────────────────────────────────────────────────────────\n",
        "    # Transform → score → label\n",
        "    # ─────────────────────────────────────────────────────────\n",
        "    X = SCALER.transform(to_score[FEATS].to_numpy(dtype=float))\n",
        "    post = MODEL.predict_proba(X)\n",
        "    hard = post.argmax(axis=1)\n",
        "\n",
        "    # Map to labels (no re-profiling)\n",
        "    lbl = [state_label_map.get(int(s), f\"State{int(s)}\") for s in hard]\n",
        "\n",
        "    # Optional forward debounce (minimal). For single-day updates this does little.\n",
        "    if FWD_DEBOUNCE and labels_hist is not None and len(labels_hist) > 2:\n",
        "        prev_state = int(labels_hist.iloc[-1][\"state_id\"])\n",
        "        if len(hard) == 1:\n",
        "            # squash a 1-day blip if it differs from the last two states\n",
        "            prev_prev_state = int(labels_hist.iloc[-2][\"state_id\"])\n",
        "            if hard[0] != prev_state and prev_state == prev_prev_state:\n",
        "                hard[0] = prev_state\n",
        "                lbl[0]  = state_label_map.get(prev_state, f\"State{prev_state}\")\n",
        "\n",
        "    # Package new rows\n",
        "    out = pd.DataFrame({\n",
        "        \"date\": to_score[\"date\"],\n",
        "        \"state_id\": hard,\n",
        "        \"regime_label\": lbl,\n",
        "        **{f\"p{s}\": post[:, s] for s in range(K)}\n",
        "    }).sort_values(\"date\").reset_index(drop=True)\n",
        "\n",
        "    # Preserve smoothed columns if they exist by copying raw (forward mode = minimal smoothing)\n",
        "    if labels_hist is not None and \"state_id_smoothed\" in labels_hist.columns:\n",
        "        out[\"state_id_smoothed\"] = out[\"state_id\"].values\n",
        "    if labels_hist is not None and \"regime_label_smoothed\" in labels_hist.columns:\n",
        "        out[\"regime_label_smoothed\"] = out[\"regime_label\"].values\n",
        "\n",
        "    # Append to history (or create)\n",
        "    if labels_hist is not None:\n",
        "        labels_new = pd.concat([labels_hist, out], axis=0, ignore_index=True)\n",
        "    else:\n",
        "        labels_new = out\n",
        "\n",
        "    labels_new = labels_new.drop_duplicates(subset=[\"date\"]).sort_values(\"date\").reset_index(drop=True)\n",
        "    labels_new.to_parquet(LAB_PATH_PQ, index=False)\n",
        "    try:\n",
        "        labels_new.to_csv(LAB_PATH_CSV, index=False)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # ─────────────────────────────────────────────────────────\n",
        "    # Logging & alerts\n",
        "    # ─────────────────────────────────────────────────────────\n",
        "    sig = _model_signature(FEATS, K, META.get(\"window\", {}))\n",
        "    for i, r in out.iterrows():\n",
        "        rec = {\n",
        "            \"ts\": datetime.utcnow().isoformat() + \"Z\",\n",
        "            \"date\": str(pd.to_datetime(r[\"date\"]).date()),\n",
        "            \"model_sig\": sig,\n",
        "            \"state_id\": int(r[\"state_id\"]),\n",
        "            \"regime_label\": str(r[\"regime_label\"]),\n",
        "            \"posteriors\": {f\"p{s}\": float(r[f\"p{s}\"]) for s in range(K)}\n",
        "        }\n",
        "        _append_jsonl(FWD_LOG, rec)\n",
        "\n",
        "    # Simple chattering alert (rolling window over recent labels)\n",
        "    alerts = {\"generated_at\": datetime.utcnow().isoformat() + \"Z\", \"alerts\": []}\n",
        "    tail = labels_new.tail(ROLL_WINDOW_D).reset_index(drop=True)\n",
        "    if len(tail) >= 3:\n",
        "        sw = int((tail[\"state_id\"].diff().fillna(0) != 0).sum())\n",
        "        if sw >= ROLL_MAX_SWITCH:\n",
        "            alerts[\"alerts\"].append(\n",
        "                f\"High switch count in last {ROLL_WINDOW_D}d: {sw} (>= {ROLL_MAX_SWITCH})\"\n",
        "            )\n",
        "    if alerts[\"alerts\"]:\n",
        "        with open(ALERTS_FP, \"w\") as f:\n",
        "            json.dump(alerts, f, indent=2)\n",
        "\n",
        "    # ─────────────────────────────────────────────────────────\n",
        "    # Optional: refresh policy hooks (2.7-lite)\n",
        "    # ─────────────────────────────────────────────────────────\n",
        "    if UPDATE_POLICY_MAP:\n",
        "        p_cols = [c for c in labels_new.columns if c.startswith(\"p\")]\n",
        "        if len(p_cols) == K:\n",
        "            tail_p = labels_new[p_cols].tail(N_CONF_TAIL).to_numpy(dtype=float)\n",
        "            p_mean = tail_p.mean(axis=0)\n",
        "            conf   = _aggressiveness_from_posterior(p_mean)\n",
        "            latest = labels_new.iloc[-1]\n",
        "            policy = {\n",
        "                \"created_at\": datetime.utcnow().isoformat() + \"Z\",\n",
        "                \"latest_date\": str(pd.to_datetime(latest[\"date\"]).date()),\n",
        "                \"k\": K,\n",
        "                \"latest_regime_label\": str(latest.get(\"regime_label\")),\n",
        "                \"latest_state_id\": int(latest.get(\"state_id\")),\n",
        "                \"latest_posteriors\": {f\"p{s}\": float(latest.get(f\"p{s}\", np.nan)) for s in range(K)},\n",
        "                \"confidence\": {\n",
        "                    \"aggressiveness_scalar_g\": conf[\"g\"],\n",
        "                    \"confidence\": conf,\n",
        "                    \"recommendations\": {\n",
        "                        \"scale_position_sizes_by_g\": True,\n",
        "                        \"scale_turnover_cap_by_g\": True,\n",
        "                        \"scale_hedge_intensity_by_(1-g)\": True\n",
        "                    }\n",
        "                },\n",
        "                \"inputs\": {\n",
        "                    \"bundle_path\": meta_idx[\"bundle_path\"],\n",
        "                    \"meta_path\": meta_idx.get(\"meta_path\"),\n",
        "                    \"labels_path\": LAB_PATH_PQ,\n",
        "                    \"panel_path\": PANEL_PATH,\n",
        "                    \"smoothing_window_days\": N_CONF_TAIL  # TOCHANGE\n",
        "                },\n",
        "                \"signature\": sig\n",
        "            }\n",
        "            with open(POLICY_OUT, \"w\") as f:\n",
        "                json.dump(policy, f, indent=2)\n",
        "\n",
        "    print(json.dumps({\n",
        "        \"status\": \"2.9 forward appended\",\n",
        "        \"n_rows_appended\": int(out.shape[0]),\n",
        "        \"last_appended_date\": str(out[\"date\"].iloc[-1].date()),\n",
        "        \"bundle_used\": meta_idx[\"bundle_path\"],\n",
        "        \"policy_refreshed\": bool(UPDATE_POLICY_MAP),\n",
        "        \"log_path\": FWD_LOG,\n",
        "        \"alerts_written\": os.path.exists(ALERTS_FP),\n",
        "    }, indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZWhCdqFHfJ6",
        "outputId": "f151da30-65fe-490f-a502-c69e32e89dae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"status\": \"2.9 forward: nothing to do\",\n",
            "  \"last_processed\": \"2025-08-11\",\n",
            "  \"note\": \"No new rows in market_panel.parquet beyond current regime_labels.\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Section Forward Smoke Test (Prep + Run)\n",
        "# Goal: simulate a realistic forward append using the *actual*\n",
        "# latest tail of the panel, without touching production files.\n",
        "#\n",
        "# What it does:\n",
        "#   - Truncates current labels by last N days into a smoke copy\n",
        "#   - Loads latest windowed HMM bundle + meta (from 2.8 outputs)\n",
        "#   - Transforms last N feature rows with saved scaler\n",
        "#   - Predicts posteriors/labels using saved state map\n",
        "#   - Appends to smoke labels and writes a smoke policy map\n",
        "#\n",
        "# Reuses:\n",
        "#   - CFG (from 2.0)\n",
        "#   - artifacts/regimes/market_panel.parquet (1.→2.0)\n",
        "#   - artifacts/regimes/regime_labels.parquet (2.8 stitched)\n",
        "#   - artifacts/regimes/windowed/windows_index.json (2.8)\n",
        "#   - artifacts/regimes/windowed/regime_hmm_<win>.pkl (2.8)\n",
        "#   - artifacts/regimes/windowed/regime_meta_<win>.json (2.8)\n",
        "#\n",
        "# Outputs (smoke-only; production files untouched):\n",
        "#   - artifacts/regimes/forward_smoketest/regime_labels_smoke_base.parquet\n",
        "#   - artifacts/regimes/forward_smoketest/regime_labels_smoke.parquet (+csv)\n",
        "#   - artifacts/regimes/forward_smoketest/regime_policy_map_smoke.json\n",
        "#\n",
        "# Notes:\n",
        "#   - #TOCHANGE marks heavier/production settings.\n",
        "#   - This section is safe to run multiple times; it overwrites files in the smoke folder.\n",
        "# ============================================================\n",
        "\n",
        "from __future__ import annotations\n",
        "import os, json, hashlib\n",
        "from typing import Dict, Any\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "# ---- Tunables ------------------------------------------------\n",
        "SMOKE_N_DAYS = 3     # #TOCHANGE: 5–10 for a beefier smoke\n",
        "REGIME_DIR   = CFG.regime_dir\n",
        "SMOKE_DIR    = os.path.join(REGIME_DIR, \"forward_smoketest\")\n",
        "os.makedirs(SMOKE_DIR, exist_ok=True)\n",
        "\n",
        "# ---- Paths ---------------------------------------------------\n",
        "PANEL_PATH    = os.path.join(REGIME_DIR, \"market_panel.parquet\")\n",
        "LAB_PROD_PATH = os.path.join(REGIME_DIR, \"regime_labels.parquet\")\n",
        "WINDEX_PATH   = os.path.join(REGIME_DIR, \"windowed\", \"windows_index.json\")\n",
        "WIN_DIR       = os.path.join(REGIME_DIR, \"windowed\")\n",
        "\n",
        "# ---- Load panel & prod labels --------------------------------\n",
        "assert os.path.exists(PANEL_PATH), f\"Missing panel: {PANEL_PATH}\"\n",
        "panel = pd.read_parquet(PANEL_PATH).sort_values(\"date\").reset_index(drop=True)\n",
        "panel[\"date\"] = pd.to_datetime(panel[\"date\"])\n",
        "\n",
        "if not os.path.exists(LAB_PROD_PATH):\n",
        "    raise FileNotFoundError(\n",
        "        \"Expected stitched labels at artifacts/regimes/regime_labels.parquet from 2.8. \"\n",
        "        \"Run 2.8 first.\"\n",
        "    )\n",
        "lab_prod = pd.read_parquet(LAB_PROD_PATH).sort_values(\"date\").reset_index(drop=True)\n",
        "lab_prod[\"date\"] = pd.to_datetime(lab_prod[\"date\"])\n",
        "\n",
        "# ---- Choose the latest window meta/bundle --------------------\n",
        "def _pick_latest_window(win_index_json: str) -> Dict[str, Any]:\n",
        "    # Prefer windows_index.json (2.8 QoL). Fallback to single W0.\n",
        "    if os.path.exists(win_index_json):\n",
        "        with open(win_index_json, \"r\") as f:\n",
        "            idx = json.load(f)\n",
        "        if isinstance(idx, list) and len(idx) > 0:\n",
        "            # sort by test_end\n",
        "            idx_sorted = sorted(idx, key=lambda d: pd.to_datetime(d[\"test_end\"]))\n",
        "            return idx_sorted[-1]\n",
        "    # Fallback: try W0 paths\n",
        "    meta_fallback = os.path.join(WIN_DIR, \"regime_meta_W0.json\")\n",
        "    bundle_fallback = os.path.join(WIN_DIR, \"regime_hmm_W0.pkl\")\n",
        "    if os.path.exists(meta_fallback) and os.path.exists(bundle_fallback):\n",
        "        with open(meta_fallback, \"r\") as f:\n",
        "            m = json.load(f)\n",
        "        return {\n",
        "            \"win_id\": \"W0\",\n",
        "            \"train_start\": m[\"window\"][\"train_start\"],\n",
        "            \"train_end\": m[\"window\"][\"train_end\"],\n",
        "            \"test_start\": m[\"window\"][\"test_start\"],\n",
        "            \"test_end\": m[\"window\"][\"test_end\"],\n",
        "            \"bundle_path\": m[\"bundle_path\"],\n",
        "            \"labels_path\": os.path.join(WIN_DIR, f\"regime_labels_W0.parquet\"),\n",
        "            \"meta_path\": meta_fallback,\n",
        "        }\n",
        "    raise FileNotFoundError(\"No windowed bundles found. Run 2.8 first.\")\n",
        "\n",
        "latest = _pick_latest_window(WINDEX_PATH)\n",
        "bundle_path = latest[\"bundle_path\"]\n",
        "meta_path   = latest[\"meta_path\"]\n",
        "\n",
        "assert os.path.exists(bundle_path), f\"Missing bundle: {bundle_path}\"\n",
        "assert os.path.exists(meta_path),   f\"Missing meta:   {meta_path}\"\n",
        "\n",
        "bundle = joblib.load(bundle_path)\n",
        "with open(meta_path, \"r\") as f:\n",
        "    meta = json.load(f)\n",
        "\n",
        "features = bundle[\"features\"]\n",
        "scaler   = bundle.get(\"scaler\", None)\n",
        "if scaler is None and \"scaler_path\" in bundle:\n",
        "    scaler = joblib.load(bundle[\"scaler_path\"])\n",
        "if scaler is None:\n",
        "    raise RuntimeError(\"Bundle does not contain a scaler; ensure 2.8 saved the scaler in-bundle.\")\n",
        "\n",
        "state_label_map = {int(k): v for k, v in meta.get(\"state_label_map\", {}).items()}\n",
        "\n",
        "# ---- Build the smoke base (truncate last N days of labels) ---\n",
        "lab_prod_sorted = lab_prod.sort_values(\"date\")\n",
        "if len(lab_prod_sorted) <= SMOKE_N_DAYS:\n",
        "    raise RuntimeError(\"Not enough labeled rows to build a smoke base; reduce SMOKE_N_DAYS.\")\n",
        "\n",
        "smoke_cutoff = lab_prod_sorted[\"date\"].iloc[-SMOKE_N_DAYS-1]\n",
        "lab_smoke_base = lab_prod_sorted.loc[lab_prod_sorted[\"date\"] <= smoke_cutoff].copy()\n",
        "base_last_date = lab_smoke_base[\"date\"].max()\n",
        "\n",
        "base_path_parq = os.path.join(SMOKE_DIR, \"regime_labels_smoke_base.parquet\")\n",
        "lab_smoke_base.to_parquet(base_path_parq, index=False)\n",
        "try:\n",
        "    lab_smoke_base.to_csv(base_path_parq.replace(\".parquet\", \".csv\"), index=False)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# ---- Determine the forward tail to score ---------------------\n",
        "panel_tail = panel.loc[panel[\"date\"] > base_last_date].copy()\n",
        "if panel_tail.empty:\n",
        "    raise RuntimeError(\"Panel has no dates after the smoke base cutoff; cannot simulate forward.\")\n",
        "\n",
        "# Use only the first SMOKE_N_DAYS after base_last_date\n",
        "fwd_dates = sorted(panel_tail[\"date\"].unique())[:SMOKE_N_DAYS]\n",
        "panel_fwd = panel.loc[panel[\"date\"].isin(fwd_dates)].copy()\n",
        "panel_fwd = panel_fwd[[\"date\"] + features].dropna().sort_values(\"date\").reset_index(drop=True)\n",
        "\n",
        "if panel_fwd.empty or len(panel_fwd) < 1:\n",
        "    raise RuntimeError(\"No complete feature rows for smoke forward dates; check features coverage.\")\n",
        "\n",
        "# ---- Transform with saved scaler & predict posteriors --------\n",
        "X_fwd = scaler.transform(panel_fwd[features].to_numpy(dtype=float))\n",
        "post  = bundle[\"model\"].predict_proba(X_fwd)\n",
        "hard  = post.argmax(axis=1)\n",
        "\n",
        "# ---- Map to labels (no re-profiling; use saved mapping) ------\n",
        "if state_label_map:\n",
        "    reg_lbl = [state_label_map.get(int(s), f\"State{int(s)}\") for s in hard]\n",
        "else:\n",
        "    reg_lbl = [f\"State{int(s)}\" for s in hard]  # fallback\n",
        "\n",
        "# ---- (Optional) minimal debounce just within the smoke block -\n",
        "def _debounce_series(state_ids: np.ndarray, min_dwell_days: int=CFG.min_dwell_days):\n",
        "    out = state_ids.copy()\n",
        "    i = 1\n",
        "    while i < len(out)-1:\n",
        "        if out[i] != out[i-1] and out[i] != out[i+1]:\n",
        "            out[i] = out[i-1]\n",
        "            i += 1\n",
        "        i += 1\n",
        "    return out\n",
        "\n",
        "hard_db = _debounce_series(hard)\n",
        "\n",
        "# ---- Build the rows to append --------------------------------\n",
        "rows = {\n",
        "    \"date\": panel_fwd[\"date\"].to_numpy(),\n",
        "    \"state_id\": hard,\n",
        "    \"state_id_smoothed\": hard_db,\n",
        "    \"regime_label\": np.array(reg_lbl, dtype=object),\n",
        "    \"regime_label_smoothed\": np.array([reg_lbl[i] if hard_db[i]==hard[i] else state_label_map.get(int(hard_db[i]), f\"State{int(hard_db[i])}\") for i in range(len(hard))], dtype=object),\n",
        "}\n",
        "# Add posteriors p0..pK-1\n",
        "K = post.shape[1]\n",
        "for s in range(K):\n",
        "    rows[f\"p{s}\"] = post[:, s]\n",
        "\n",
        "lab_smoke_new = pd.DataFrame(rows).sort_values(\"date\").reset_index(drop=True)\n",
        "\n",
        "# ---- Append to smoke base & save ------------------------------\n",
        "lab_smoke_all = pd.concat([lab_smoke_base, lab_smoke_new], axis=0).sort_values(\"date\").reset_index(drop=True)\n",
        "\n",
        "out_parq = os.path.join(SMOKE_DIR, \"regime_labels_smoke.parquet\")\n",
        "lab_smoke_all.to_parquet(out_parq, index=False)\n",
        "try:\n",
        "    lab_smoke_all.to_csv(out_parq.replace(\".parquet\", \".csv\"), index=False)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# ---- Also emit a smoke policy map (same spirit as 2.7) -------\n",
        "def _entropy(p: np.ndarray) -> float:\n",
        "    p = np.clip(p, 1e-12, 1.0)\n",
        "    return float(-(p*np.log(p)).sum() / np.log(len(p)))\n",
        "\n",
        "# #TOCHANGE: smooth window for confidence in prod 5–10\n",
        "N_SMOOTH = 3\n",
        "tail = lab_smoke_all.tail(N_SMOOTH)\n",
        "p_cols = [c for c in lab_smoke_all.columns if c.startswith(\"p\")]\n",
        "p_tail = tail[p_cols].to_numpy(dtype=float)\n",
        "p_mean = p_tail.mean(axis=0) if p_tail.size else np.ones(K)/K\n",
        "c_max = float(p_mean.max())\n",
        "c_ent = 1.0 - _entropy(p_mean)\n",
        "c_comb = 0.5*(c_max + c_ent)\n",
        "g_min, g_max = 0.35, 1.00   # #TOCHANGE deeper throttling for prod\n",
        "g = g_min + (g_max-g_min)*c_comb\n",
        "\n",
        "latest_row = lab_smoke_all.iloc[-1]\n",
        "latest_label = str(latest_row[\"regime_label_smoothed\"] if \"regime_label_smoothed\" in lab_smoke_all.columns else latest_row[\"regime_label\"])\n",
        "\n",
        "policy_smoke = {\n",
        "    \"created_at\": pd.Timestamp.utcnow().isoformat()+\"Z\",\n",
        "    \"mode\": \"smoke\",\n",
        "    \"latest_date\": str(pd.to_datetime(latest_row[\"date\"]).date()),\n",
        "    \"k\": int(K),\n",
        "    \"latest_regime_label\": latest_label,\n",
        "    \"latest_state_id\": int(latest_row[\"state_id_smoothed\"] if \"state_id_smoothed\" in lab_smoke_all.columns else latest_row[\"state_id\"]),\n",
        "    \"latest_posteriors\": {f\"p{s}\": float(latest_row.get(f\"p{s}\", np.nan)) for s in range(K)},\n",
        "    \"confidence\": {\n",
        "        \"aggressiveness_scalar_g\": g,\n",
        "        \"confidence_components\": {\"c_max\": c_max, \"c_entropy\": c_ent, \"combined\": c_comb},\n",
        "        \"recommendations\": {\n",
        "            \"scale_position_sizes_by_g\": True,\n",
        "            \"scale_turnover_cap_by_g\": True,\n",
        "            \"scale_hedge_intensity_by_(1-g)\": True\n",
        "        }\n",
        "    },\n",
        "    \"inputs\": {\n",
        "        \"bundle_path\": bundle_path,\n",
        "        \"meta_path\": meta_path,\n",
        "        \"features\": features,\n",
        "        \"smoke_base_labels\": base_path_parq,\n",
        "        \"smoke_labels_out\": out_parq,\n",
        "        \"n_days_scored\": int(len(lab_smoke_new)),\n",
        "        \"window\": latest.get(\"window\", {\n",
        "            \"train_start\": latest.get(\"train_start\"),\n",
        "            \"train_end\":   latest.get(\"train_end\"),\n",
        "            \"test_start\":  latest.get(\"test_start\"),\n",
        "            \"test_end\":    latest.get(\"test_end\"),\n",
        "        })\n",
        "    }\n",
        "}\n",
        "with open(os.path.join(SMOKE_DIR, \"regime_policy_map_smoke.json\"), \"w\") as f:\n",
        "    json.dump(policy_smoke, f, indent=2)\n",
        "\n",
        "print(json.dumps({\n",
        "    \"status\": \"Forward smoke COMPLETE\",\n",
        "    \"base_last_date\": str(pd.to_datetime(base_last_date).date()),\n",
        "    \"scored_dates\": [str(pd.to_datetime(d).date()) for d in lab_smoke_new[\"date\"]],\n",
        "    \"out_labels_parquet\": out_parq,\n",
        "    \"out_policy_smoke\": os.path.join(SMOKE_DIR, \"regime_policy_map_smoke.json\"),\n",
        "    \"notes\": [\n",
        "        f\"Truncated prod labels by last {SMOKE_N_DAYS} day(s) to create smoke base.\",\n",
        "        \"Used latest windowed HMM bundle + scaler; no re-profiling (state map from meta).\",\n",
        "        \"For a heavier smoke, increase SMOKE_N_DAYS (#TOCHANGE).\"\n",
        "    ]\n",
        "}, indent=2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGl9UfNW_shE",
        "outputId": "067ca2a2-2792-40e4-b309-977eb9fba86f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"status\": \"Forward smoke COMPLETE\",\n",
            "  \"base_last_date\": \"2025-08-06\",\n",
            "  \"scored_dates\": [\n",
            "    \"2025-08-07\",\n",
            "    \"2025-08-08\",\n",
            "    \"2025-08-11\"\n",
            "  ],\n",
            "  \"out_labels_parquet\": \"artifacts/regimes/forward_smoketest/regime_labels_smoke.parquet\",\n",
            "  \"out_policy_smoke\": \"artifacts/regimes/forward_smoketest/regime_policy_map_smoke.json\",\n",
            "  \"notes\": [\n",
            "    \"Truncated prod labels by last 3 day(s) to create smoke base.\",\n",
            "    \"Used latest windowed HMM bundle + scaler; no re-profiling (state map from meta).\",\n",
            "    \"For a heavier smoke, increase SMOKE_N_DAYS (#TOCHANGE).\"\n",
            "  ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Section 2.10 — Configuration & Reproducibility\n",
        "# Tasks:\n",
        "#   • Snapshot effective config + key artifacts (hashes, sizes, dates)\n",
        "#   • Basic determinism check (same bundle, same scores)\n",
        "#   • Validations: posterior sums, date order, gaps, label semantics sanity\n",
        "#   • Emit run manifest + validation report for auditability\n",
        "#\n",
        "# Reuses:\n",
        "#   - CFG (from 2.0)\n",
        "#   - artifacts/regimes/market_panel.parquet (2.0)\n",
        "#   - artifacts/regimes/regime_labels.parquet (2.8)\n",
        "#   - artifacts/regimes/windowed/windows_index.json (2.8 QoL)\n",
        "#   - artifacts/regimes/windowed/regime_hmm_<win>.pkl + meta (2.8)\n",
        "#   - artifacts/regimes/diagnostics/state_profiles_table.csv (2.6; optional)\n",
        "#\n",
        "# Outputs:\n",
        "#   - artifacts/regimes/run_manifest.json\n",
        "#   - artifacts/regimes/validation_report.json\n",
        "#   - artifacts/regimes/run_fingerprint.txt  (short human-readable summary)\n",
        "#\n",
        "# Notes:\n",
        "#   - #TOCHANGE marks heavier/production settings.\n",
        "# ============================================================\n",
        "\n",
        "from __future__ import annotations\n",
        "import os, json, hashlib, sys\n",
        "from typing import Dict, Any\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "REGIME_DIR   = CFG.regime_dir\n",
        "PANEL_PATH   = os.path.join(REGIME_DIR, \"market_panel.parquet\")\n",
        "LABELS_PATH  = os.path.join(REGIME_DIR, \"regime_labels.parquet\")\n",
        "CONFIG_EFF   = os.path.join(REGIME_DIR, \"regime_config_effective.json\")\n",
        "WINDEX_PATH  = os.path.join(REGIME_DIR, \"windowed\", \"windows_index.json\")\n",
        "WIN_DIR      = os.path.join(REGIME_DIR, \"windowed\")\n",
        "DIAG_DIR     = os.path.join(REGIME_DIR, \"diagnostics\")\n",
        "PROF_TABLE   = os.path.join(DIAG_DIR, \"state_profiles_table.csv\")\n",
        "\n",
        "def _sha256_file(path: str) -> str:\n",
        "    h = hashlib.sha256()\n",
        "    with open(path, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(65536), b\"\"):\n",
        "            h.update(chunk)\n",
        "    return h.hexdigest()\n",
        "\n",
        "def _pick_latest_window(windex: str) -> Dict[str, Any]:\n",
        "    if os.path.exists(windex):\n",
        "        with open(windex, \"r\") as f:\n",
        "            idx = json.load(f)\n",
        "        if isinstance(idx, list) and len(idx) > 0:\n",
        "            idx_sorted = sorted(idx, key=lambda d: pd.to_datetime(d[\"test_end\"]))\n",
        "            return idx_sorted[-1]\n",
        "    # fallback to W0\n",
        "    meta_fallback = os.path.join(WIN_DIR, \"regime_meta_W0.json\")\n",
        "    if os.path.exists(meta_fallback):\n",
        "        with open(meta_fallback, \"r\") as f:\n",
        "            m = json.load(f)\n",
        "        return {\n",
        "            \"win_id\": \"W0\",\n",
        "            \"train_start\": m[\"window\"][\"train_start\"],\n",
        "            \"train_end\":   m[\"window\"][\"train_end\"],\n",
        "            \"test_start\":  m[\"window\"][\"test_start\"],\n",
        "            \"test_end\":    m[\"window\"][\"test_end\"],\n",
        "            \"bundle_path\": m[\"bundle_path\"],\n",
        "            \"meta_path\":   meta_fallback,\n",
        "            \"labels_path\": os.path.join(WIN_DIR, \"regime_labels_W0.parquet\"),\n",
        "        }\n",
        "    raise FileNotFoundError(\"No windowed artifacts found; run 2.8 first.\")\n",
        "\n",
        "# ── 1) Load essentials\n",
        "assert os.path.exists(PANEL_PATH), f\"Missing panel: {PANEL_PATH}\"\n",
        "assert os.path.exists(LABELS_PATH), f\"Missing stitched labels: {LABELS_PATH}\"\n",
        "panel  = pd.read_parquet(PANEL_PATH).sort_values(\"date\").reset_index(drop=True)\n",
        "labels = pd.read_parquet(LABELS_PATH).sort_values(\"date\").reset_index(drop=True)\n",
        "panel[\"date\"]  = pd.to_datetime(panel[\"date\"])\n",
        "labels[\"date\"] = pd.to_datetime(labels[\"date\"])\n",
        "\n",
        "latest = _pick_latest_window(WINDEX_PATH)\n",
        "bundle_path = latest[\"bundle_path\"]; meta_path = latest[\"meta_path\"]\n",
        "assert os.path.exists(bundle_path), f\"Missing bundle: {bundle_path}\"\n",
        "assert os.path.exists(meta_path),   f\"Missing meta:   {meta_path}\"\n",
        "bundle = joblib.load(bundle_path)\n",
        "\n",
        "\n",
        "# ── 2) Build run manifest (config + artifacts snapshot)\n",
        "manifest: Dict[str, Any] = {\n",
        "    \"created_at\": pd.Timestamp.utcnow().isoformat()+\"Z\",\n",
        "    \"config_effective_path\": CONFIG_EFF if os.path.exists(CONFIG_EFF) else None,\n",
        "    \"artifacts\": {\n",
        "        \"panel\": {\"path\": PANEL_PATH, \"rows\": int(len(panel)), \"sha256\": _sha256_file(PANEL_PATH)},\n",
        "        \"labels_stitched\": {\"path\": LABELS_PATH, \"rows\": int(len(labels)), \"sha256\": _sha256_file(LABELS_PATH)},\n",
        "        \"bundle_latest\": {\"path\": bundle_path, \"sha256\": _sha256_file(bundle_path)},\n",
        "        \"meta_latest\":   {\"path\": meta_path,   \"sha256\": _sha256_file(meta_path)},\n",
        "    },\n",
        "    \"latest_window\": {\n",
        "        \"win_id\": latest.get(\"win_id\"),\n",
        "        \"train_start\": latest.get(\"train_start\"), \"train_end\": latest.get(\"train_end\"),\n",
        "        \"test_start\":  latest.get(\"test_start\"),  \"test_end\":  latest.get(\"test_end\"),\n",
        "        \"labels_path\": latest.get(\"labels_path\"),\n",
        "    },\n",
        "    \"features\": bundle.get(\"features\", []),\n",
        "    \"hmm\": {\n",
        "        \"k\": int(bundle.get(\"k\", -1)),\n",
        "        \"covariance_type\": bundle.get(\"covariance_type\", \"full\"),\n",
        "        \"n_iter\": int(bundle.get(\"n_iter\", -1)),\n",
        "        \"n_init\": int(bundle.get(\"n_init\", -1)),\n",
        "        \"tol\": float(bundle.get(\"tol\", -1)),\n",
        "        \"random_state\": int(bundle.get(\"random_state\", -1)),\n",
        "        \"sticky_lambda\": float(bundle.get(\"sticky_lambda\", np.nan)),\n",
        "        \"recency\": {\n",
        "            \"enabled\": bool(bundle.get(\"recency_weighting\", False)),\n",
        "            \"half_life_days\": int(bundle.get(\"recency_half_life_days\", 0)),\n",
        "            \"seg_len\": int(bundle.get(\"recency_seg_len\", 0)),\n",
        "            \"n_segments\": int(bundle.get(\"recency_n_segments\", 0)),\n",
        "            \"epsilon_floor\": float(bundle.get(\"recency_epsilon_floor\", 0.0)),\n",
        "        },\n",
        "    }\n",
        "}\n",
        "if manifest[\"config_effective_path\"]:\n",
        "    manifest[\"config_effective_sha256\"] = _sha256_file(manifest[\"config_effective_path\"])\n",
        "\n",
        "with open(os.path.join(REGIME_DIR, \"run_manifest.json\"), \"w\") as f:\n",
        "    json.dump(manifest, f, indent=2)\n",
        "\n",
        "# --- Checks container + helper (must be defined before any _add_check calls)\n",
        "report = {\"checks\": [], \"status\": \"ok\"}\n",
        "\n",
        "def _add_check(name: str, ok: bool, details: Dict[str, Any] | None=None):\n",
        "    report[\"checks\"].append({\"name\": name, \"pass\": bool(ok), \"details\": details or {}})\n",
        "    if not ok:\n",
        "        report[\"status\"] = \"fail\"\n",
        "\n",
        "# Config keys presence (from CFG)\n",
        "try:\n",
        "    cfg_checks = {\n",
        "        \"hmm_features_present\": bool(getattr(CFG, \"hmm_features\", None)),\n",
        "        \"min_dwell_days_present\": hasattr(CFG, \"min_dwell_days\"),\n",
        "        \"posterior_thresh_present\": hasattr(CFG, \"posterior_thresh\"),\n",
        "        \"recency_weighting_flag_present\": hasattr(CFG, \"recency_weighting\"),\n",
        "    }\n",
        "    _add_check(\"config_keys_present\", all(cfg_checks.values()), cfg_checks)\n",
        "except Exception as e:\n",
        "    _add_check(\"config_keys_present_error\", False, {\"error\": repr(e)})\n",
        "\n",
        "# Optional: environment snapshot (helps reproducibility)  # TOCHANGE: expand in prod\n",
        "try:\n",
        "    import platform, sys\n",
        "    env = {\n",
        "        \"python\": sys.version.split()[0],\n",
        "        \"platform\": platform.platform(),\n",
        "        \"numpy\": np.__version__,\n",
        "        \"pandas\": pd.__version__,\n",
        "        \"hmmlearn\": __import__(\"hmmlearn\").__version__,\n",
        "        \"scikit_learn\": __import__(\"sklearn\").__version__,\n",
        "        # \"git_commit\": <inject if available>  # TOCHANGE\n",
        "    }\n",
        "    with open(os.path.join(REGIME_DIR, \"run_env.json\"), \"w\") as f:\n",
        "        json.dump(env, f, indent=2)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "\n",
        "# ── 3) Validations\n",
        "# deleted: report = {\"checks\": [], \"status\": \"ok\"}\n",
        "\n",
        "\n",
        "\n",
        "# 3.a Dates monotonic / duplicates\n",
        "is_sorted = labels[\"date\"].is_monotonic_increasing\n",
        "has_dupes = labels[\"date\"].duplicated().any()\n",
        "_add_check(\"dates_sorted\", is_sorted, {\"monotonic_increasing\": bool(is_sorted)})\n",
        "_add_check(\"dates_no_duplicates\", not has_dupes, {\"duplicates\": int(labels[\"date\"].duplicated().sum())})\n",
        "\n",
        "# 3.b Posterior rows sum ≈ 1 (if present)\n",
        "p_cols = [c for c in labels.columns if c.startswith(\"p\")]\n",
        "if p_cols:\n",
        "    row_sums = labels[p_cols].sum(axis=1).to_numpy(dtype=float)\n",
        "    max_dev = float(np.max(np.abs(row_sums - 1.0)))\n",
        "    _add_check(\"posteriors_sum_to_one\", bool(max_dev < 1e-6), {\"max_abs_deviation\": max_dev})\n",
        "else:\n",
        "    _add_check(\"posteriors_present\", False, {\"note\": \"No p* columns found.\"})\n",
        "\n",
        "# 3.c Gap check vs panel dates (labels ⊆ panel)\n",
        "panel_dates  = set(pd.to_datetime(panel[\"date\"]).dt.date)\n",
        "labels_dates = set(pd.to_datetime(labels[\"date\"]).dt.date)\n",
        "missing_in_panel = sorted([str(d) for d in (labels_dates - panel_dates)])\n",
        "_add_check(\"labels_dates_subset_of_panel\", len(missing_in_panel)==0, {\"labels_not_in_panel\": missing_in_panel[:10]})\n",
        "\n",
        "\n",
        "# Insert inside the validations block, before determinism check\n",
        "\n",
        "# 3.bis No gaps after stitching (business days)\n",
        "try:\n",
        "    bd = pd.bdate_range(start=labels[\"date\"].min(), end=labels[\"date\"].max(), freq=\"C\")  # business days\n",
        "    labd = pd.to_datetime(labels[\"date\"]).dt.normalize().unique()\n",
        "    labd = pd.DatetimeIndex(labd)\n",
        "    missing = bd.difference(labd)\n",
        "    # Allow known market holidays (we can’t list them here), so only flag if large consecutive gaps\n",
        "    # TOCHANGE: tighten policy (e.g., require an exchange calendar)\n",
        "    ok_nogaps = len(missing) == 0\n",
        "    report_missing = [str(d.date()) for d in missing[:10]]\n",
        "    _add_check(\"no_business_day_gaps_in_labels\", ok_nogaps, {\"missing_first_10\": report_missing, \"missing_count\": int(len(missing))})\n",
        "except Exception as e:\n",
        "    _add_check(\"no_business_day_gaps_in_labels_error\", False, {\"error\": repr(e)})\n",
        "\n",
        "# 3.d Label semantics sanity (Risk-On higher mean than Risk-Off; vol ordering)\n",
        "# Prefer 2.6 table if available; else quick compute on full labeled set (heuristic).\n",
        "try:\n",
        "    if os.path.exists(PROF_TABLE):\n",
        "        prof = pd.read_csv(PROF_TABLE)\n",
        "        # columns expected: state_id, ret_mean, rv20_mean, ...\n",
        "        ron_mean = float(prof[\"ret_mean\"].max())\n",
        "        roff_vol = float(prof[\"rv20_mean\"].max())\n",
        "        ok = np.isfinite(ron_mean) and np.isfinite(roff_vol)\n",
        "        _add_check(\"semantics_profiles_present\", ok, {})\n",
        "    else:\n",
        "        # crude fallback by state_id on labeled rows\n",
        "        tmp = labels.merge(panel[[\"date\",\"spy_ret\",\"spy_rv_20\"]], on=\"date\", how=\"left\")\n",
        "        grp = tmp.groupby(\"state_id\", dropna=True).agg(ret_mean=(\"spy_ret\",\"mean\"), rv20_mean=(\"spy_rv_20\",\"mean\"))\n",
        "        cond = (grp[\"ret_mean\"].max() == grp[\"ret_mean\"].max()) and (grp[\"rv20_mean\"].max() == grp[\"rv20_mean\"].max())\n",
        "        _add_check(\"semantics_profiles_fallback\", bool(cond), {\"n_states\": int(grp.shape[0])})\n",
        "except Exception as e:\n",
        "    _add_check(\"semantics_profiles_error\", False, {\"error\": repr(e)})\n",
        "\n",
        "# 3.e Determinism smoke: rescore a tiny slice with same bundle\n",
        "# #TOCHANGE: widen slice or repeat N times in prod\n",
        "try:\n",
        "    # take last ~200 rows available in panel∩labels\n",
        "    common = labels.merge(panel[[\"date\"]], on=\"date\", how=\"inner\").tail(200)\n",
        "    if not common.empty and \"scaler\" in bundle:\n",
        "        feats = bundle[\"features\"]\n",
        "        scaler = bundle[\"scaler\"]\n",
        "        # join features\n",
        "        X = panel.merge(common[[\"date\"]], on=\"date\", how=\"inner\").sort_values(\"date\")\n",
        "        X = X[feats].dropna().to_numpy(dtype=float)\n",
        "        post1 = bundle[\"model\"].predict_proba(scaler.transform(X))\n",
        "        post2 = bundle[\"model\"].predict_proba(scaler.transform(X))\n",
        "        max_diff = float(np.max(np.abs(post1 - post2)))\n",
        "        _add_check(\"determinism_same_inputs_same_scores\", bool(max_diff < 1e-10), {\"max_abs_diff\": max_diff})\n",
        "    else:\n",
        "        _add_check(\"determinism_skipped\", True, {\"reason\": \"No overlap or scaler missing in bundle.\"})\n",
        "except Exception as e:\n",
        "    _add_check(\"determinism_error\", False, {\"error\": repr(e)})\n",
        "\n",
        "# 3.f Leakage guard (structural): ensure features are present at t\n",
        "# (We can’t fully prove non-leakage without lineage; this is a structural check.)\n",
        "feats = bundle.get(\"features\", [])\n",
        "missing_feats = [f for f in feats if f not in panel.columns]\n",
        "_add_check(\"features_present_in_panel\", len(missing_feats)==0, {\"missing\": missing_feats})\n",
        "\n",
        "# ── 4) Save report + short fingerprint\n",
        "rep_path = os.path.join(REGIME_DIR, \"validation_report.json\")\n",
        "with open(rep_path, \"w\") as f:\n",
        "    json.dump(report, f, indent=2)\n",
        "\n",
        "# short human-readable fingerprint\n",
        "fp_lines = [\n",
        "    f\"Run: {manifest['created_at']}\",\n",
        "    f\"K={manifest['hmm']['k']}, cov={manifest['hmm']['covariance_type']}, sticky_lambda={manifest['hmm']['sticky_lambda']}\",\n",
        "    f\"Panel rows: {manifest['artifacts']['panel']['rows']}, Labels rows: {manifest['artifacts']['labels_stitched']['rows']}\",\n",
        "    f\"Latest window: {manifest['latest_window']['win_id']}  \"\n",
        "    f\"({manifest['latest_window']['train_start']}→{manifest['latest_window']['test_end']})\",\n",
        "    f\"Validation status: {report['status']}  (checks: {len(report['checks'])})\"\n",
        "]\n",
        "with open(os.path.join(REGIME_DIR, \"run_fingerprint.txt\"), \"w\") as f:\n",
        "    f.write(\"\\n\".join(fp_lines) + \"\\n\")\n",
        "\n",
        "print(json.dumps({\n",
        "    \"status\": \"2.10 reproducibility snapshot complete\",\n",
        "    \"manifest\": os.path.join(REGIME_DIR, \"run_manifest.json\"),\n",
        "    \"validation_report\": rep_path,\n",
        "    \"fingerprint\": os.path.join(REGIME_DIR, \"run_fingerprint.txt\"),\n",
        "    \"notes\": [\n",
        "        \"For production, expand determinism checks (multiple random restarts held fixed).\",  # TOCHANGE\n",
        "        \"Consider capturing git commit hash and Python/package versions for full lineage.\",  # TOCHANGE\n",
        "        \"Posterior-sum, date order, and feature presence checks included.\"\n",
        "    ]\n",
        "}, indent=2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7lWAdL5Migj",
        "outputId": "b618f2f1-3bef-4a34-cffd-099dbc49aa26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"status\": \"2.10 reproducibility snapshot complete\",\n",
            "  \"manifest\": \"artifacts/regimes/run_manifest.json\",\n",
            "  \"validation_report\": \"artifacts/regimes/validation_report.json\",\n",
            "  \"fingerprint\": \"artifacts/regimes/run_fingerprint.txt\",\n",
            "  \"notes\": [\n",
            "    \"For production, expand determinism checks (multiple random restarts held fixed).\",\n",
            "    \"Consider capturing git commit hash and Python/package versions for full lineage.\",\n",
            "    \"Posterior-sum, date order, and feature presence checks included.\"\n",
            "  ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>Summary of section 2</summary>\n",
        "\n",
        "# Section 2.0–2.1\n",
        "\n",
        "### **What We’ve Done**\n",
        "- **2.0**:  \n",
        "  - Defined configuration (`RegimeConfig`) for regime modeling — feature set, HMM params, recency weighting, I/O paths.\n",
        "  - Loaded `features_filtered.parquet` from Section 1 and **assembled a clean, date-aligned market panel** containing:\n",
        "    - SPY daily log returns (`spy_ret`)\n",
        "    - SPY realized volatility (20-day) (`spy_rv_20`)\n",
        "    - VIX level (`vix_close`) and change (`dvix`, optional)\n",
        "    - Market breadth (`breadth`)\n",
        "  - Saved this **market-level panel** to:\n",
        "    - `artifacts/regimes/market_panel.parquet` (+ CSV if enabled) — *core input for all subsequent regime modeling steps*.\n",
        "  - Wrote `regime_config_effective.json` — the **final config** used for reproducibility.\n",
        "\n",
        "- **2.1**:  \n",
        "  - Loaded the above market panel and **prepared train/test matrices for HMM** on a first walk-forward window:\n",
        "    - Features: `spy_rv_20`, `vix_close`, `breadth`, `dvix`\n",
        "    - Train period: `2007-02-06` → `2016-12-30`\n",
        "    - Test period: `2017-01-03` → latest date (`2025-08-08`)\n",
        "  - Standardized features **per train window** with `StandardScaler`; applied same transform to test.\n",
        "  - Saved:\n",
        "    - Per-window **scaler**: `scaler_<dates>.joblib`\n",
        "    - **QC JSON** with mean/std per feature in train vs test.\n",
        "    - **Window manifest** (`window_manifest.json`) describing date ranges, features, scaler path, and sample counts.\n",
        "\n",
        "---\n",
        "\n",
        "### **Artifacts for Reuse**\n",
        "| File | Contents | Purpose |\n",
        "|------|----------|---------|\n",
        "| `artifacts/regimes/market_panel.parquet` | Date-level DataFrame: `date`, `spy_ret`, `spy_rv_20`, `vix_close`, `breadth`, `dvix` (if enabled) | Core market context for HMM; already cleaned, aligned, NaN-free. |\n",
        "| `artifacts/regimes/regime_config_effective.json` | JSON dump of final `RegimeConfig` | Ensures downstream sections use same config; includes feature list, HMM params, I/O paths. |\n",
        "| `artifacts/regimes/scaler_<train>__<test>.joblib` | Fitted `StandardScaler` for the given walk-forward window | Apply same scaling to new data in this window. |\n",
        "| `artifacts/regimes/scaler_<train>__<test>_qc.json` | QC metrics: train/test row counts, means, stds per feature | For diagnostics and reproducibility. |\n",
        "| `artifacts/regimes/window_manifest.json` | Dict with train/test date ranges, feature list, scaler path, sample counts | Downstream code can iterate over these windows without recalculating splits. |\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Variables**\n",
        "| Variable | Value Example | Description |\n",
        "|----------|---------------|-------------|\n",
        "| `CFG` | `RegimeConfig(...)` | Active config object after merging defaults and `config.yaml`. |\n",
        "| `mkt` | Pandas DataFrame, ~4657 rows × 6 cols | Market panel from 2.0; date-indexed features for HMM. |\n",
        "| `hmm_feat_cols` | `[\"spy_rv_20\", \"vix_close\", \"breadth\", \"dvix\"]` | Feature list for HMM modeling. |\n",
        "| `window` | Dict with keys: `X_train`, `X_test`, `dates_train`, `dates_test`, `scaler_path`, `qc` | All matrices and metadata for one walk-forward window. |\n",
        "| `manifest` | Dict with `window`, `features`, `scaler_path`, `n_train`, `n_test` | Summary of the first walk-forward split, persisted for reuse. |\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# 2.2 — Model Choice & Configuration (Gaussian HMM)\n",
        "\n",
        "## What we did\n",
        "- Trained a **GaussianHMM (covariance_type=\"full\")** on the standardized train window from 2.1.\n",
        "- Used a **time-decayed “recency” sampler** to bias training toward recent data (segment length 60, 80 segments, half-life 756 days).\n",
        "- Enforced **regime persistence** with a sticky diagonal blend on the transition matrix (λ=0.15).\n",
        "- Tried a small **K grid** (test run K=[2]) and **multiple inits** (2 restarts); **picked the best** by train log-likelihood on the *full* train sequence.\n",
        "- Persisted a self-contained bundle for reuse.\n",
        "\n",
        "## Key hyperparams (test run; marked **#TOCHANGE** in code)\n",
        "- `K grid`: `[2]` (real run: `[2, 3]`)\n",
        "- `n_iter`: `200` (real run: `1000`)\n",
        "- `n_init`: `2` (real run: `10`)\n",
        "- `tol`: `1e-3` (real run: `1e-4`)\n",
        "- `sticky_lambda`: `0.15` (real run: `0.30–0.50`)\n",
        "- Recency: `half_life_days=756`, `seg_len=60`, `n_segments=80`, `epsilon_floor=0.10`\n",
        "\n",
        "## Files produced (and what’s inside)\n",
        "- **`artifacts/regimes/regime_hmm.pkl`** *(joblib bundle)*  \n",
        "  - `model` — fitted `GaussianHMM`  \n",
        "  - `k`, `random_state`, `covariance_type`, `n_iter`, `n_init`, `tol`  \n",
        "  - `features` — the exact feature list used  \n",
        "  - **Scaler info:** `scaler_path` (points to the 2.1 scaler)  \n",
        "  - **Windows:** `train_dates`, `test_dates`  \n",
        "  - **Stickiness:** `sticky_lambda`  \n",
        "  - **Recency config** and `fit_mode`  \n",
        "  - `created_at`\n",
        "- **`artifacts/regimes/hmm_kgrid.json`** — scores for all `(k, seed)` runs and the chosen combo.\n",
        "\n",
        "## Reusable variables/outputs for Section 3\n",
        "- **Bundle** (`regime_hmm.pkl`) — trained HMM, feature list, scaler path, recency/stickiness configs.\n",
        "- **Feature list** — `bundle[\"features\"]`\n",
        "- **Hyperparams** — recency and persistence knobs, if needed for downstream logic.\n",
        "\n",
        "---\n",
        "\n",
        "# 2.3 — State Labeling & Semantics\n",
        "\n",
        "## What we did\n",
        "- Loaded `regime_hmm.pkl` + 2.1 scaler, scored posteriors for **all dates** in `market_panel.parquet`.\n",
        "- Built **state profiles** on TRAIN only:\n",
        "  - `spy_ret` (mean & std), `spy_rv_20` (mean), `vix_close` (mean), `breadth` (mean), `dvix` (if present), `ret_q05` (5% tail).\n",
        "- **Assigned semantic labels**:\n",
        "  - **Risk-On**: highest mean return (tie-breakers: breadth↑, VIX↓, better tails)\n",
        "  - **Risk-Off**: highest vol & lowest return (tie-breakers: vol↑, ΔVIX↑, breadth↓)\n",
        "  - **Transition**: remaining state\n",
        "- Persisted labels + posteriors and saved the label map to ensure stability.\n",
        "\n",
        "## Files produced\n",
        "- **`artifacts/regimes/regime_labels.parquet`** (+ CSV):\n",
        "  - `date`\n",
        "  - `state_id` (hard assignment)\n",
        "  - `p0..pK-1` (posteriors)\n",
        "  - `regime_label`\n",
        "- **`artifacts/regimes/state_profiles.csv`**:\n",
        "  - Per-state TRAIN stats (returns, vol, breadth, tails).\n",
        "- **`artifacts/regimes/regime_meta.json`**:\n",
        "  - `state_label_map`\n",
        "  - `diagnostics.state_profiles_train`\n",
        "  - `features_used`\n",
        "  - Notes on labeling policy.\n",
        "\n",
        "## Reusable variables/outputs for Section 3\n",
        "- **`regime_labels.parquet`** — main regime feed for Section 3:\n",
        "  - `regime_label` or max-posterior state.\n",
        "  - `p*` columns for confidence metrics.\n",
        "- **`regime_meta.json → state_label_map`** — ensures consistent label meanings across windows.\n",
        "- **`state_profiles.csv`** — sanity check and seed values for regime-aware policies.\n",
        "\n",
        "---\n",
        "\n",
        "## Quick outputs recap from run\n",
        "- 2.2: `{\"chosen_k\": 2, \"fit_mode\": \"recency\", \"train_score\": -8178.567..., \"n_iter\": 200, \"n_init\": 2, \"sticky_lambda\": 0.15, \"half_life_days\": 756, \"seg_len\": 60, \"n_segments\": 80}`\n",
        "- 2.3: `{\"k\": 2, \"label_map\": {\"0\": \"Risk-On\", \"1\": \"Risk-Off\"}, \"profiles_path\": \".../state_profiles.csv\", \"labels_path\": \".../regime_labels.parquet\"}`\n",
        "\n",
        "# 2.4 — Smoothing, Persistence & Debounce\n",
        "\n",
        "## What we did\n",
        "- **Base path selection** (controlled by config):\n",
        "  - `SMOOTH_MTH` in `CFG` → `\"viterbi\"` uses `model.predict(...)`; `\"posterior\"` uses `post.argmax(...)`.\n",
        "- **Posterior threshold gating** (debounce step 1):\n",
        "  - If a day’s new state differs from the prior day but the **max posterior** that day `< P_THRESH`, keep the **previous** state.\n",
        "  - Config knobs:\n",
        "    - `P_THRESH = CFG.posterior_thresh` (default **0.55**)\n",
        "    - `MIN_DWELL = CFG.min_dwell_days` (default **3**)\n",
        "    - `SMOOTH_MTH = CFG.smoothing_method` (default **\"posterior\"**)\n",
        "- **Minimum dwell enforcement** (debounce step 2):\n",
        "  - Collapse any **short runs** (`run_len < MIN_DWELL`) to the better neighbor using average posterior over the short segment.\n",
        "- **Label mapping**:\n",
        "  - Map smoothed state IDs to labels via `state_label_map` from `regime_meta.json` (set in 2.3).\n",
        "- **Gap handling**:\n",
        "  - Dates are already business days from the panel; no forward-looking fills are introduced.\n",
        "\n",
        "## Inputs reused\n",
        "- `artifacts/regimes/market_panel.parquet` (from 2.0)  \n",
        "- `artifacts/regimes/window_manifest.json` (from 2.1)  \n",
        "- `artifacts/regimes/regime_hmm.pkl` (from 2.2)  \n",
        "- `artifacts/regimes/regime_labels.parquet` (from 2.3)  \n",
        "- `artifacts/regimes/regime_meta.json` (from 2.3)\n",
        "\n",
        "## Outputs\n",
        "- **`artifacts/regimes/regime_labels.parquet`** *(updated in-place)*  \n",
        "  - `date`\n",
        "  - `p0..pK-1` (posteriors)\n",
        "  - `state_id` (original argmax)\n",
        "  - `state_id_smoothed` (after threshold + dwell collapse)  \n",
        "  - `regime_label_smoothed` (`state_id_smoothed` → `state_label_map`)\n",
        "- **`artifacts/regimes/regime_labels.csv`**\n",
        "- **`artifacts/regimes/regime_meta.json`** *(updated)*  \n",
        "  - Adds `diagnostics.smoothing`:\n",
        "    - `method` (posterior|viterbi)\n",
        "    - `posterior_thresh`\n",
        "    - `min_dwell_days`\n",
        "    - `dwell_stats`\n",
        "\n",
        "## Reuse in Section 3+\n",
        "- Use **`regime_label_smoothed`** (or `state_id_smoothed`) as the regime signal.\n",
        "- Use **`p0..pK-1`** for confidence logic.\n",
        "- Read **`diagnostics.smoothing.dwell_stats`** for dwell/chattering monitoring.\n",
        "\n",
        "---\n",
        "\n",
        "# 2.5 — Robustness & Sensitivity\n",
        "\n",
        "## What we did\n",
        "- **Baseline context** from 2.2 and 2.1:\n",
        "  - `features_base = bundle[\"features\"]`\n",
        "  - `k_base = bundle[\"k\"]`\n",
        "  - Recency/sticky params from bundle.\n",
        "- **K sensitivity** (`K_GRID = [2, 3]`):\n",
        "  - Refit with recency-weighted subsequences.\n",
        "  - Score on train sequence; record best per K.\n",
        "  - Compute **agreement vs baseline**.\n",
        "- **Feature sensitivity**:\n",
        "  - Variants: `baseline`, `no_vix`, `no_breadth`, `no_dvix`, `core_rv_vix`.\n",
        "  - Refit at `k_base`, record label map, **agreement vs baseline**.\n",
        "- **Era stability**:\n",
        "  - Refit on `pre_2015`, `post_2015`, `crisis_2020`.\n",
        "  - Record profiles, label map, transition matrix.\n",
        "- **Bootstrap (block)**:\n",
        "  - `BLOCK_DAYS = 20`, `BOOT_REPS = 5` (light test).\n",
        "  - Refit and compute **agreement vs baseline**; report mean/std.\n",
        "\n",
        "> All refits fit a local `StandardScaler` on the relevant subset.\n",
        "\n",
        "## Inputs reused\n",
        "- `artifacts/regimes/market_panel.parquet` (2.0)  \n",
        "- `artifacts/regimes/window_manifest.json` (2.1)  \n",
        "- `artifacts/regimes/regime_hmm.pkl` (2.2)  \n",
        "- `artifacts/regimes/regime_labels.parquet` (2.3/2.4)\n",
        "\n",
        "## Outputs\n",
        "- **`artifacts/regimes/regime_sensitivity.json`**  \n",
        "  - `created_at`\n",
        "  - `inputs` (features_base, k_base, recency, sticky, etc.)\n",
        "  - `results`:\n",
        "    - **`k_sensitivity`**: best per K, agreement vs baseline, profiles, label map, transmat.\n",
        "    - **`feature_sensitivity`**: best per feature set, agreement, profiles, label map, transmat.\n",
        "    - **`era_stability`**: per era, profiles, label map, transmat.\n",
        "    - **`bootstrap`**: agreement list, mean, std.\n",
        "\n",
        "## Reuse in Section 3+\n",
        "- Pick **K** balancing separation and stability.\n",
        "- Decide on features based on agreement.\n",
        "- Adapt hedging/caps for era drift.\n",
        "- Gate production with bootstrap agreement thresholds.\n",
        "\n",
        "---\n",
        "\n",
        "## File Index for 2.4 & 2.5\n",
        "\n",
        "| File | Produced/Updated in | Purpose |\n",
        "|------|---------------------|---------|\n",
        "| `artifacts/regimes/regime_labels.parquet` (+ `.csv`) | 2.4 | Adds smoothed IDs/labels; ensures posteriors. |\n",
        "| `artifacts/regimes/regime_meta.json` | 2.4 | Smoothing diagnostics + state map. |\n",
        "| `artifacts/regimes/regime_sensitivity.json` | 2.5 | K/feature/era/bootstrap results with stability metrics. |\n",
        "\n",
        "# 2.6 & 2.7\n",
        "\n",
        "# 2.6 — Diagnostics & QA\n",
        "\n",
        "**What we did**\n",
        "- Computed core diagnostics from existing labels:\n",
        "  - Transition matrix, steady‐state distribution, dwell/run statistics, switch/chattering metrics.\n",
        "- Generated plots:\n",
        "  - `regime_timeline.png` (SPY price w/ regime shading), `timeline_drawdown.png`\n",
        "  - `regime_posteriors.png` (stacked p’s)\n",
        "  - Per-state return histograms `state_<s>_ret_hist.png` and QQ plots `state_<s>_qq.png`\n",
        "  - `transition_matrix_heatmap.png`, `dwell_time_distribution.png`\n",
        "- Emitted tables and alerts for QA (semantics, dwell < 3d, chattering, mapping flips).\n",
        "\n",
        "**Reused inputs (exact paths/objects)**\n",
        "- `artifacts/regimes/market_panel.parquet` → market series (`date, spy_ret, spy_rv_20, vix_close, breadth, dvix`)\n",
        "- `artifacts/regimes/window_manifest.json` → window bounds (train/test)\n",
        "- `artifacts/regimes/regime_hmm.pkl` → bundle with `features`, `k` (for K fallback)\n",
        "- `artifacts/regimes/regime_labels.parquet` → label timeline\n",
        "  - Columns used: `date`, `p0..pK-1` (if present), `state_id` (or `state_id_smoothed`), `regime_label` (or `regime_label_smoothed`)\n",
        "- `artifacts/regimes/state_profiles.csv` → state profile stats from 2.3 (fallback recompute if missing)\n",
        "- `artifacts/regimes/regime_meta.json` → optional label map/notes\n",
        "\n",
        "**Key variables (in-code names)**\n",
        "- `DIAG_DIR = artifacts/regimes/diagnostics`\n",
        "- `state_col` = `\"state_id_smoothed\"` if present else `\"state_id\"`\n",
        "- `label_col` = `\"regime_label_smoothed\"` if present else `\"regime_label\"`\n",
        "- `p_cols` = all columns starting with `\"p\"`; `K = len(p_cols)` (else fallback to bundle `k`)\n",
        "- Diagnostics computed:\n",
        "  - `Tmat` (K×K transition matrix), `ss_emp` (steady state)\n",
        "  - `dwell` (run lengths by state), `switches`, `switch_rate`\n",
        "  - `one_day_runs` (share of 1-day runs), `lt3_runs` (share runs <3d)\n",
        "  - `alerts` list (semantics, dwell, chattering, flips)\n",
        "\n",
        "**Outputs (exact filenames & contents)**\n",
        "- PNGs in `artifacts/regimes/diagnostics/`:\n",
        "  - `regime_timeline.png`, `timeline_drawdown.png`, `regime_posteriors.png`,\n",
        "    `state_<s>_ret_hist.png`, `state_<s>_qq.png`,\n",
        "    `transition_matrix_heatmap.png`, `dwell_time_distribution.png`\n",
        "- CSVs in `artifacts/regimes/diagnostics/`:\n",
        "  - `state_profiles_table.csv` (state_id, ret_mean, ret_std, rv20_mean, vix_mean, dvix_mean, breadth_mean, ret_q05)\n",
        "  - `transition_matrix.csv` (row=from_i, cols=to_j), `steady_state.csv` (state_id, steady_state_prob)\n",
        "  - `switches_by_year.csv` (year, n_switches), `summary_metrics.csv` (K, switches, switch_rate, one_day_runs_frac, lt3_runs_frac)\n",
        "- JSON:\n",
        "  - `alerts.json` (list of QA alerts)\n",
        "\n",
        "---\n",
        "\n",
        "# 2.7 — Regime-Aware Policy Hooks (Interfaces to Sections 3–5)\n",
        "\n",
        "**What we did**\n",
        "- Built a single hand-off file for downstream portfolio logic:\n",
        "  - Latest regime, smoothed confidence, and per-regime policy defaults (weights multipliers, turnover caps, risk targets, hedge intensity).\n",
        "- Confidence proxy combines **max posterior** and **(1 − normalized entropy)**, then maps to an **aggressiveness scalar `g` ∈ [0.35, 1.00]**.\n",
        "- If `p*` columns are missing, we rescore posteriors from the HMM bundle.\n",
        "\n",
        "**Reused inputs (exact paths/objects)**\n",
        "- `artifacts/regimes/regime_labels.parquet` → posteriors & (smoothed) states\n",
        "  - Uses `p_cols` when present; else rescored via bundle\n",
        "  - Picks `state_col` / `label_col` as in 2.6\n",
        "- `artifacts/regimes/regime_meta.json` → `state_label_map` (state → semantic label)\n",
        "- `artifacts/regimes/regime_hmm.pkl` → `model`, `features`, `k`, `scaler_path` (for fallback scoring)\n",
        "- `artifacts/regimes/window_manifest.json` → `scaler_path`, `window`\n",
        "- `artifacts/regimes/market_panel.parquet` → fallback features matrix for rescoring\n",
        "\n",
        "**Key variables (in-code names)**\n",
        "- `OUT_PATH = artifacts/regimes/regime_policy_map.json`\n",
        "- `N_SMOOTH = 3` (days) → average recent posteriors for confidence\n",
        "- `p_cols` (derived or rescored), `K` (len(p_cols) or bundle `k`)\n",
        "- `state_col`, `label_col` (same logic as 2.6)\n",
        "- Confidence helpers: `entropy(p)`, `aggressiveness_from_confidence(p)` → returns `{c_max, c_entropy, c, g}`\n",
        "\n",
        "**Output (exact file & schema)**\n",
        "- `artifacts/regimes/regime_policy_map.json`\n",
        "  - Top-level:\n",
        "    - `created_at`, `latest_date`, `k`, `latest_regime_label`, `latest_state_id`\n",
        "    - `latest_posteriors`: `{ \"p0\": float, ..., \"p{K-1}\": float }`\n",
        "    - `confidence`:\n",
        "      - `aggressiveness_scalar_g` (float in [0.35, 1.00])\n",
        "      - `confidence`: `{ \"c_max\", \"c_entropy\", \"c\", \"g\" }`\n",
        "      - `recommendations`: `{ \"scale_position_sizes_by_g\", \"scale_turnover_cap_by_g\", \"scale_hedge_intensity_by_(1-g)\" }`\n",
        "    - `policy_by_regime`:\n",
        "      - Keys: semantic labels present in data (e.g., `\"Risk-On\"`, `\"Transition\"`, `\"Risk-Off\"`) or synthesized `State<i>` when no mapping.\n",
        "      - Values per regime:\n",
        "        - `weights_multipliers`: `{ \"momentum\", \"quality\", \"value\", \"low_vol\" }`\n",
        "        - `turnover_cap` (float)\n",
        "        - `risk_target_vol_annual` (float, e.g., 0.10/0.08/0.06)\n",
        "        - `hedge_intensity` (float)\n",
        "    - `inputs`:\n",
        "      - `labels_path`, `meta_path`, `bundle_path`, `scaler_path`\n",
        "      - `features` (list), `window` (train/test bounds), `smoothing_window_days`\n",
        "      - If present: `sensitivity_path = artifacts/regimes/regime_sensitivity.json`\n",
        "      - If present: `diagnostics_dir = artifacts/regimes/diagnostics`\n",
        "    - `signature` (sha256 over features/window/k)\n",
        "\n",
        "**How Section 3–5 should reuse**\n",
        "- Read **one file**: `artifacts/regimes/regime_policy_map.json`.\n",
        "  - Use `latest_regime_label` to branch logic.\n",
        "  - Scale exposures and caps by `confidence.aggressiveness_scalar_g`.\n",
        "  - Pull regime-specific knobs from `policy_by_regime[<label>]`.\n",
        "  - Optionally reference `inputs.sensitivity_path` and `inputs.diagnostics_dir` for auditability.\n",
        "\n",
        "\n",
        "## **Section 2.8 — Walk-Forward Integration**\n",
        "\n",
        "This section implements **rolling or expanding window walk-forward evaluation** for regime detection, matching the methodology in Section 6 (when available).  \n",
        "It ensures **out-of-sample (OOS) scoring** and prevents **regime meaning drift** by preserving the `state → label` mapping per window.\n",
        "\n",
        "---\n",
        "\n",
        "### **Core Logic**\n",
        "1. **Window Handling**\n",
        "   - **Preferred:** Load `windows_manifest.json` (multi-window plan).\n",
        "   - **Fallback:** Wrap `window_manifest.json` (single-window).\n",
        "   - **Smoke Test Autogen:** Generate a small rolling test plan for quick runs.\n",
        "\n",
        "2. **Per-Window Workflow**\n",
        "   - **Train Phase:**\n",
        "     - Fit `StandardScaler` and `GaussianHMM` **only on training subset**.\n",
        "     - Apply **recency-weighted sampling** if enabled (`APPLY_RECENCY`).\n",
        "   - **Test Phase:**\n",
        "     - Transform features using the **train-fitted scaler**.\n",
        "     - Predict posteriors (`p0...pK-1`) and hard regime states.\n",
        "     - Map numeric states to semantic labels (`Risk-On`, `Risk-Off`, `Transition`) using training-set profiling.\n",
        "     - Apply light debouncing (`min_dwell_days`) to remove 1-day flips.\n",
        "   - **Save Artifacts:**\n",
        "     - Model bundle (`regime_hmm_<winid>.pkl`) with scaler + params.\n",
        "     - Labels (`regime_labels_<winid>.parquet` + `.csv`) with smoothed & raw states + posteriors.\n",
        "     - Metadata (`regime_meta_<winid>.json`) with window dates, features, mappings, and file paths.\n",
        "\n",
        "3. **Output Stitching**\n",
        "   - Concatenate all test chunks into a **continuous timeline** (`regime_labels.parquet` + `.csv`) for backtests.\n",
        "   - Save a **window index** (`windows_index.json` + `.csv`) with summary info.\n",
        "\n",
        "---\n",
        "\n",
        "### **Output Example**\n",
        "```json\n",
        "{\n",
        "  \"status\": \"2.8 walk-forward complete\",\n",
        "  \"n_windows\": 1,\n",
        "  \"windows\": [\n",
        "    {\n",
        "      \"win_id\": \"W0\",\n",
        "      \"train_start\": \"2007-02-06\",\n",
        "      \"train_end\": \"2016-12-30\",\n",
        "      \"test_start\": \"2017-01-03\",\n",
        "      \"test_end\": \"2025-08-08\",\n",
        "      \"n_train\": 2495,\n",
        "      \"n_test\": 2162,\n",
        "      \"bundle_path\": \"artifacts/regimes/windowed/regime_hmm_W0.pkl\",\n",
        "      \"labels_path\": \"artifacts/regimes/windowed/regime_labels_W0.parquet\",\n",
        "      \"meta_path\": \"artifacts/regimes/windowed/regime_meta_W0.json\"\n",
        "    }\n",
        "  ],\n",
        "  \"stitched_out\": {\n",
        "    \"parquet\": \"artifacts/regimes/regime_labels.parquet\",\n",
        "    \"csv\": \"artifacts/regimes/regime_labels.csv\"\n",
        "  },\n",
        "  \"windows_index\": {\n",
        "    \"json\": \"artifacts/regimes/windowed/windows_index.json\",\n",
        "    \"csv\": \"artifacts/regimes/windowed/windows_index.csv\"\n",
        "  },\n",
        "  \"notes\": [\n",
        "    \"Per-window scaler fitted on TRAIN only; TEST scored out-of-sample.\",\n",
        "    \"State→label semantics are saved per window and applied to the test chunk.\",\n",
        "    \"For real run: increase N_ITER/N_INIT and recency sampler size; align windows with Section 6.\"\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "# other details about 2.8:\n",
        "Files Reused\n",
        "market_panel.parquet (from 2.0):\n",
        "Main panel of market features (date, spy_ret, and CFG.hmm_features), sorted by date.\n",
        "\n",
        "window_manifest.json / windows_manifest.json (from 2.1):\n",
        "Defines rolling/expanding window splits with train/test boundaries.\n",
        "\n",
        "regime_sensitivity.json (from 2.5, optional):\n",
        "Stores results of regime sensitivity tests (e.g., best K values).\n",
        "\n",
        "Diagnostics directory (from 2.6, optional):\n",
        "Extra per-state statistics or plots for debugging.\n",
        "\n",
        "Variables Reused\n",
        "CFG.hmm_features — Feature names for the HMM (from config).\n",
        "\n",
        "CFG.min_dwell_days — Minimum days before switching regimes.\n",
        "\n",
        "APPLY_RECENCY / HALF_LIFE_DAYS / SEG_LEN / N_SEGMENTS — Recency sampling parameters (from 2.2).\n",
        "\n",
        "K — Number of HMM states (can come from sensitivity analysis in 2.5).\n",
        "\n",
        "LAMBDA_STICK — Sticky transition smoothing factor.\n",
        "\n",
        "Artifacts Produced\n",
        "Per-Window:\n",
        "\n",
        "Model + scaler bundle → regime_hmm_<winid>.pkl\n",
        "\n",
        "Regime labels → regime_labels_<winid>.parquet (+ .csv)\n",
        "\n",
        "Metadata → regime_meta_<winid>.json\n",
        "\n",
        "Global:\n",
        "\n",
        "Continuous labels → regime_labels.parquet (+ .csv)\n",
        "\n",
        "Windows index → windows_index.json (+ .csv)\n",
        "\n",
        "## 2.9 — Forward (Shadow) Mode\n",
        "\n",
        "### What we implemented\n",
        "- **Daily append loop**:\n",
        "  1) Load latest window metadata via `windows_index.json` (fallback: scan `regime_meta_*.json`), then load the **bundle** (`regime_hmm_<winid>.pkl`) → `MODEL`, `SCALER`, `FEATS`, `K`.\n",
        "  2) Read newest feature rows from `market_panel.parquet` **after** the last date in `regime_labels.parquet`.\n",
        "  3) `SCALER.transform` → `MODEL.predict_proba` → `argmax` to get `state_id`.\n",
        "  4) Map `state_id` → `regime_label` using meta’s `state_label_map` (no re-profiling).\n",
        "  5) Append rows (with `p0..p{K-1}`) to `regime_labels.parquet` (+ CSV), **no backfill**.\n",
        "  6) **Log** each new row to JSONL with a **model signature hash**.\n",
        "  7) **Alerts**: rolling last `ROLL_WINDOW_D` days; flag if switches ≥ `ROLL_MAX_SWITCH`.\n",
        "  8) **Optional** policy refresh (2.7-lite): compute confidence on last `N_CONF_TAIL` posteriors and write `regime_policy_map.json`.\n",
        "\n",
        "- **Retrain cadence**: not in the daily path — set to weekly/bi-weekly (**#TOCHANGE**).\n",
        "\n",
        "---\n",
        "\n",
        "### Reusable globals / config knobs\n",
        "- Paths:\n",
        "  - `REGIME_DIR`, `PANEL_PATH`, `LAB_PATH_PQ`, `LAB_PATH_CSV`, `WIN_DIR`, `WIN_INDEX`\n",
        "- Optional:\n",
        "  - `START_DATE` (taken from `globals()` if present)\n",
        "- Policy refresh:\n",
        "  - `UPDATE_POLICY_MAP` (bool), `POLICY_OUT`\n",
        "- Logging & alerts:\n",
        "  - `FWD_LOG`, `ALERTS_FP`\n",
        "- Guardrails / smoothing:\n",
        "  - `FWD_DEBOUNCE` (bool), `ROLL_WINDOW_D` (int), `ROLL_MAX_SWITCH` (int)\n",
        "- Confidence window:\n",
        "  - `N_CONF_TAIL` (int)\n",
        "\n",
        "**Bundle/meta fields (loaded, reused):**\n",
        "- From `regime_hmm_<winid>.pkl` → `BUNDLE`: `model`, `scaler`, `features`, `k`\n",
        "- From `regime_meta_<winid>.json` → `META`: `\"state_label_map\"`, `\"window\"` (for signature)\n",
        "\n",
        "---\n",
        "\n",
        "### Reusable helper functions\n",
        "- `_load_latest_window_meta()`: choose latest window from `windows_index.json` (fallback scan of `regime_meta_*.json`); returns keys like `win_id`, `bundle_path`, `meta_path`, `test_end`.\n",
        "- `_model_signature(features, k, window)`: SHA256 hash for audit/lineage.\n",
        "- `_entropy(p)`: normalized entropy of a posterior vector.\n",
        "- `_aggressiveness_from_posterior(p_mean)`: returns `{c_max, c_entropy, c, g}` for policy scaling.\n",
        "- `_append_jsonl(path, rec)`: append a record to JSONL log.\n",
        "\n",
        "*(Smoke-test only, but reusable if desired)*  \n",
        "- `_pick_latest_window(win_index_json)`: pick latest window from index (used in forward smoke test).\n",
        "- `_debounce_series(state_ids, min_dwell_days=CFG.min_dwell_days)`: minimal 1-day blip squash (used in smoke test).\n",
        "\n",
        "---\n",
        "\n",
        "### Files / artifacts (with exact paths) and what they contain\n",
        "- **`artifacts/regimes/market_panel.parquet`**: full feature panel (`date` + `FEATS`) used to find new rows to score.\n",
        "- **`artifacts/regimes/windowed/windows_index.json`**: list of window records; includes `win_id`, `train_*`, `test_*`, `bundle_path`, `meta_path`, `labels_path`.\n",
        "- **`artifacts/regimes/windowed/regime_meta_<winid>.json`**: per-window meta including `\"state_label_map\"`, `\"window\"`, `\"features\"`, `\"k\"`, recency/sticky knobs, `\"bundle_path\"`.\n",
        "- **`artifacts/regimes/windowed/regime_hmm_<winid>.pkl`**: joblib bundle with `model` (GaussianHMM), `scaler` (StandardScaler), `features` (list), `k` (int).\n",
        "- **`artifacts/regimes/regime_labels.parquet`** (+ `regime_labels.csv`): master labels time series (stitched history + new forward rows). Columns:  \n",
        "  `date`, `state_id`, `regime_label`, `p0..p{K-1}`, and if present, `state_id_smoothed`, `regime_label_smoothed`.\n",
        "- **`artifacts/regimes/regime_forward_log.jsonl`**: one JSON record per appended row:  \n",
        "  `{ \"ts\", \"date\", \"model_sig\", \"state_id\", \"regime_label\", \"posteriors\": { \"p0\":..., ... } }`.\n",
        "- **`artifacts/regimes/forward_alerts.json`**: alerts like `\"High switch count in last {ROLL_WINDOW_D}d: {sw} (>= {ROLL_MAX_SWITCH})\"`.\n",
        "- **`artifacts/regimes/regime_policy_map.json`** (optional refresh): latest label, posteriors, confidence `{g, c_max, c_entropy, c}`, and `\"inputs\"` with paths + `\"signature\"`.\n",
        "\n",
        "*(Smoke-test outputs — safe sandbox)*  \n",
        "- **`artifacts/regimes/forward_smoketest/regime_labels_smoke_base.parquet`**: truncated base.\n",
        "- **`artifacts/regimes/forward_smoketest/regime_labels_smoke.parquet`** (+ CSV): base + newly scored tail.\n",
        "- **`artifacts/regimes/forward_smoketest/regime_policy_map_smoke.json`**: policy map built from smoke labels tail.\n",
        "\n",
        "---\n",
        "\n",
        "### Key columns appended each day\n",
        "- `date`, `state_id`, `regime_label`, `p0..p{K-1}`  \n",
        "- (If the historical file already had them) `state_id_smoothed`, `regime_label_smoothed` are mirrored from raw.\n",
        "\n",
        "## 2.10 — Configuration & Reproducibility\n",
        "\n",
        "### What we implemented\n",
        "- **Snapshot effective config & artifacts**:\n",
        "  - Captures `CFG` keys (HMM features, min dwell, posterior threshold, recency flags).\n",
        "  - Records artifact paths, row counts, SHA256 hashes.\n",
        "  - Stores latest window metadata (`win_id`, train/test dates, bundle/meta paths).\n",
        "  - Saves model parameters (`k`, covariance, n_iter, sticky_lambda, recency params).\n",
        "  - Optional: environment snapshot (Python, platform, package versions).\n",
        "\n",
        "- **Validations**:\n",
        "  - Config key presence.\n",
        "  - Dates sorted, no duplicates.\n",
        "  - Posterior columns present and sum to 1.\n",
        "  - Labels’ dates ⊆ panel dates, no unexpected business day gaps.\n",
        "  - Label semantics sanity check (risk-on/off profiles).\n",
        "  - Determinism check (same inputs → identical posteriors).\n",
        "  - Features in bundle all present in panel (basic leakage guard).\n",
        "\n",
        "- **Auditability outputs**:\n",
        "  - `run_manifest.json` → full config & artifact snapshot.\n",
        "  - `validation_report.json` → pass/fail status of each check.\n",
        "  - `run_fingerprint.txt` → short human-readable summary.\n",
        "\n",
        "---\n",
        "\n",
        "### Reusable globals / config knobs\n",
        "- **Paths**:\n",
        "  - `REGIME_DIR`, `PANEL_PATH`, `LABELS_PATH`, `CONFIG_EFF`,  \n",
        "    `WINDEX_PATH`, `WIN_DIR`, `DIAG_DIR`, `PROF_TABLE`\n",
        "- **From CFG**:\n",
        "  - `hmm_features`, `min_dwell_days`, `posterior_thresh`, `recency_weighting`\n",
        "\n",
        "---\n",
        "\n",
        "### Reusable helper functions\n",
        "- `_sha256_file(path)`: file hash for artifact integrity.\n",
        "- `_pick_latest_window(windex)`: load latest window metadata (fallback to W0).\n",
        "- `_add_check(name, ok, details)`: append validation result to report.\n",
        "\n",
        "---\n",
        "\n",
        "### Files / artifacts produced\n",
        "- **`artifacts/regimes/run_manifest.json`** — config + artifact snapshot with hashes, sizes, dates.\n",
        "- **`artifacts/regimes/validation_report.json`** — structured validation results (`status`, per-check pass/fail).\n",
        "- **`artifacts/regimes/run_fingerprint.txt`** — concise run summary (K, sticky_lambda, row counts, latest window).\n",
        "- **`artifacts/regimes/run_env.json`** *(optional)* — Python & package versions, platform info.\n",
        "\n",
        "---\n",
        "\n",
        "### Reused artifacts from earlier sections\n",
        "- `artifacts/regimes/market_panel.parquet` (2.0)\n",
        "- `artifacts/regimes/regime_labels.parquet` (2.8)\n",
        "- `artifacts/regimes/windowed/windows_index.json` (2.8 QoL)\n",
        "- Latest `regime_hmm_<win>.pkl` + `regime_meta_<win>.json` (2.8)\n",
        "- `artifacts/regimes/diagnostics/state_profiles_table.csv` (2.6; optional)\n",
        "\n",
        "# Quick summary of everything\n",
        "\n",
        "# 📦 Section 2 — Regime Modeling (HMM → Regime Labels & Probabilities)\n",
        "\n",
        "**Goal:** Detect daily market regimes (**Risk-On**, **Risk-Off**, **Transition**) with posterior probabilities, for use in Sections 3–5 (alpha models, sizing, risk caps).\n",
        "\n",
        "---\n",
        "\n",
        "## 1️⃣ What We Built\n",
        "\n",
        "### 2.0–2.1 — Market Panel & Train/Test Prep\n",
        "- Created **clean, date-aligned market panel** from Section 1 features:\n",
        "  - `spy_ret` (SPY log returns)\n",
        "  - `spy_rv_20` (20d realized vol)\n",
        "  - `vix_close` (+ optional `dvix` daily change)\n",
        "  - `breadth` (% advancers in S&P)\n",
        "- Saved to: `artifacts/regimes/market_panel.parquet`\n",
        "- Generated **train/test matrices** for walk-forward window:\n",
        "  - Standardized **per-train-window** using `StandardScaler`\n",
        "  - QC checks: row counts, mean/std drift, NaNs\n",
        "  - Saved `scaler_<train>__<test>.joblib` + QC JSON + `window_manifest.json`\n",
        "\n",
        "### 2.2 — HMM Model Training\n",
        "- Trained **GaussianHMM** (`covariance_type=\"full\"`) with:\n",
        "  - Optional **recency-weighted sampling**\n",
        "  - Sticky transitions for persistence\n",
        "- Searched `K` in {2, 3}, picked best by log-likelihood\n",
        "- Saved self-contained bundle: model, scaler path, config, training dates\n",
        "\n",
        "### 2.3 — State Labeling\n",
        "- Profiled states on train set → assigned semantic labels:\n",
        "  - Risk-On: highest mean return, lowest vol\n",
        "  - Risk-Off: highest vol, lowest return\n",
        "  - Transition: remainder\n",
        "- Persisted mapping in `regime_meta.json`\n",
        "- Created regime timeline with posteriors\n",
        "\n",
        "### 2.4 — Smoothing & Debounce\n",
        "- Removed short noisy flips using:\n",
        "  - Posterior threshold (`posterior_thresh`)\n",
        "  - Min dwell days (`min_dwell_days`)\n",
        "- Updated regime labels with smoothed states\n",
        "\n",
        "### 2.5 — Robustness Tests\n",
        "- Sensitivity to:\n",
        "  - `K` choice\n",
        "  - Feature removal\n",
        "  - Era splits (pre/post-2015, 2020 crisis)\n",
        "- Block bootstrap stability check\n",
        "- Saved results for audit\n",
        "\n",
        "### 2.6 — Diagnostics & QA\n",
        "- Computed:\n",
        "  - Transition matrix, dwell-time stats, chattering metrics\n",
        "  - State return distributions & QQ plots\n",
        "- Generated plots + summary CSVs + QA alerts\n",
        "\n",
        "### 2.7 — Regime Policy Map\n",
        "- Created single JSON for downstream use:\n",
        "  - Latest regime + confidence score\n",
        "  - Per-regime weights, turnover caps, risk targets, hedge intensity\n",
        "  - Confidence scalar `g` ∈ [0.35, 1.00]\n",
        "\n",
        "### 2.8 — Walk-Forward Integration\n",
        "- Automated multi-window HMM training & stitching of test outputs\n",
        "- Ensured **state→label** stability across windows\n",
        "- Produced continuous regime timeline for backtests\n",
        "\n",
        "### 2.9 — Forward (Shadow) Mode\n",
        "- Daily append loop:\n",
        "  - Score new rows from `market_panel.parquet`\n",
        "  - Append regime + posteriors to `regime_labels.parquet`\n",
        "  - Optional policy map refresh\n",
        "  - Alerts if excessive regime switches\n",
        "\n",
        "### 2.10 — Config & Reproducibility\n",
        "- Snapshotted:\n",
        "  - Effective config\n",
        "  - Artifact paths & hashes\n",
        "  - Validation checks\n",
        "- Produced concise run fingerprint\n",
        "\n",
        "---\n",
        "\n",
        "## 2️⃣ Key Global Variables & Functions (Reusable)\n",
        "\n",
        "| Name | Type | Description |\n",
        "|------|------|-------------|\n",
        "| `CFG` | `RegimeConfig` | Loaded from `config.yaml` + defaults; holds all regime model params & paths |\n",
        "| `mkt` | `pd.DataFrame` | Clean market panel (`market_panel.parquet`) |\n",
        "| `hmm_feat_cols` | `list[str]` | Features used for HMM (e.g. `[\"spy_rv_20\",\"vix_close\",\"breadth\",\"dvix\"]`) |\n",
        "| `window` | `dict` | Train/test matrices & metadata for one walk-forward window |\n",
        "| `manifest` | `dict` | Summary of window bounds, features, scaler path, sample counts |\n",
        "| `_entropy(p)` | `func` | Normalized entropy from posterior vector |\n",
        "| `_aggressiveness_from_posterior(p)` | `func` | Returns `{c_max, c_entropy, c, g}` for sizing/risk scaling |\n",
        "| `_debounce_series(states, min_dwell)` | `func` | Remove short state flips |\n",
        "| `_model_signature(features,k,window)` | `func` | SHA256 signature for auditability |\n",
        "| `_load_latest_window_meta()` | `func` | Loads latest walk-forward model/meta paths |\n",
        "\n",
        "---\n",
        "\n",
        "## 3️⃣ Artifacts & Their Contents\n",
        "\n",
        "| File | Purpose | Key Fields |\n",
        "|------|---------|------------|\n",
        "| `market_panel.parquet` | Core HMM input features | `date, spy_ret, spy_rv_20, vix_close, breadth, dvix` |\n",
        "| `regime_hmm.pkl` | HMM model bundle | model, features, scaler_path, training dates, config |\n",
        "| `regime_labels.parquet` (+ CSV) | Regime timeline | `date, state_id, p0..pK-1, regime_label` (+ smoothed) |\n",
        "| `regime_meta.json` | State→label mapping & diagnostics | mapping, profiles, config |\n",
        "| `state_profiles.csv` | Per-state stats | mean/std returns, vol, VIX, breadth, tails |\n",
        "| `regime_sensitivity.json` | Robustness test results | k/feature/era/bootstrap outcomes |\n",
        "| `diagnostics/*.png` | Plots | timeline, posteriors, histograms, QQ, transmat, dwell dist |\n",
        "| `diagnostics/*.csv` | Metrics | state_profiles_table, transition_matrix, steady_state, run stats |\n",
        "| `regime_policy_map.json` | Per-regime strategy knobs | latest regime, g-scalar, per-regime caps & weights |\n",
        "| `windows_index.json` | Walk-forward plan | window IDs, dates, artifact paths |\n",
        "| `regime_forward_log.jsonl` | Forward mode log | date, state, posteriors, model signature |\n",
        "| `forward_alerts.json` | Alerts | excessive switching, anomalies |\n",
        "| `run_manifest.json` | Full config + artifact snapshot | cfg keys, paths, hashes |\n",
        "| `validation_report.json` | Pass/fail checks | leakage, dates, semantics |\n",
        "| `run_fingerprint.txt` | Short summary | key params & latest status |\n",
        "\n",
        "---\n",
        "\n",
        "## 4️⃣ How to Reuse in Later Sections\n",
        "\n",
        "- **For alpha models (Section 3)**  \n",
        "  - Read `regime_labels.parquet` (use smoothed label columns)  \n",
        "  - Use `p*` columns for regime-confidence scaling  \n",
        "  - Read `regime_policy_map.json` to set factor weights, turnover caps, hedge targets  \n",
        "\n",
        "- **For walk-forward runs**  \n",
        "  - Use `windows_index.json` to iterate windows  \n",
        "  - Load matching `regime_hmm_<win>.pkl` and `regime_meta_<win>.json`  \n",
        "\n",
        "- **For forward mode**  \n",
        "  - Extend `regime_labels.parquet` daily using `_load_latest_window_meta()` and scoring pipeline  \n",
        "  - Refresh `regime_policy_map.json` as needed  \n",
        "\n",
        "- **For diagnostics or tuning**  \n",
        "  - Use `regime_sensitivity.json` to choose stable `K` and feature set  \n",
        "  - Use `diagnostics/` CSVs for deeper QA or visual overlays  \n",
        "\n",
        "---\n",
        "\n",
        "## 5️⃣ Deliverables Checklist ✅\n",
        "\n",
        "- [x] `regime_labels.parquet` (+ CSV)  \n",
        "- [x] `regime_hmm.pkl`  \n",
        "- [x] `regime_meta.json`  \n",
        "- [x] `regime_timeline.png`, `regime_posteriors.png`  \n",
        "- [x] `state_profiles.csv`  \n",
        "- [x] `transition_matrix.csv`  \n",
        "- [x] `regime_sensitivity.json`  \n",
        "- [x] `regime_policy_map.json`  \n",
        "\n",
        "---\n",
        "\n",
        "**Next Dev Tip:**  \n",
        "All core regime logic is already modularized. Before coding, scan `regime_policy_map.json` and `regime_labels.parquet` — they cover 90% of what you’ll need without touching model code.\n",
        "\n",
        "\n",
        "\n",
        "</details>\n"
      ],
      "metadata": {
        "id": "i11zULMBWjdn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.11"
      ],
      "metadata": {
        "id": "QR0dVyT6SqgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Alpha Layer (Signals)"
      ],
      "metadata": {
        "id": "j3s49iJvw3VU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary><strong>Section 3 — Alpha Layer (Signals)</strong></summary>\n",
        "\n",
        "**Goal:**  \n",
        "Build the full **signal-generation layer** that produces **daily, regime-aware cross-sectional alpha forecasts** (mean + uncertainty) for each asset in the universe, with strict leakage control. Outputs must be clean, reproducible, and validated — ready for **Section 4 (Portfolio Construction & Risk)** and **Section 5 (RL Sizing)**.  \n",
        "This section transforms **modeling-ready features** (Section 1) and **regime context** (Section 2) into actionable, confidence-scored forecasts via **multifactor composites, ML overlays, regime-aware blending, and uncertainty aggregation**.\n",
        "\n",
        "---\n",
        "\n",
        "### **3.0 Scope & Inputs (reuse, don’t recompute)**  \n",
        "**Description:** Define what data and configurations are needed to run the alpha modeling. Pull in clean, preprocessed features from Section 1 and regime labels from Section 2, and set up the alpha modeling configuration (`AlphaConfig`) with horizons, models, CV parameters, and uncertainty estimation methods.  \n",
        "\n",
        "**From Section 1:**  \n",
        "- `features_filtered.parquet` — modeling-ready per-asset panel.  \n",
        "- `universe.csv` — canonical equities universe.  \n",
        "- `cs_cols`, `non_feature_cols`, `cols_to_shift`, `dates_all`, `px_daily_all`.\n",
        "\n",
        "**From Section 2:**  \n",
        "- `artifacts/regimes/regime_labels.parquet` — regimes & probabilities.  \n",
        "- `artifacts/regimes/regime_policy_map.json` — latest regime & confidence scalar `g`.  \n",
        "- `artifacts/regimes/window_manifest.json` or `windows_index.json` — walk-forward bounds.\n",
        "\n",
        "**New config:**  \n",
        "- `AlphaConfig` (horizons, target definition, models, CV, UQ, losses, regime hooks, artifact paths).\n",
        "\n",
        "---\n",
        "\n",
        "### **3.1 Targets & Panels**  \n",
        "**Description:** Generate prediction targets and assemble training/testing panels for each walk-forward window. Targets are excess returns over hedges (SPY, sector ETFs) for 5- and 10-day horizons. Ensure leakage-free construction.  \n",
        "\n",
        "**Outputs:**  \n",
        "- `targets.parquet` — per horizon.  \n",
        "- Per-window `panel_train` / `panel_test` parquet files with features, targets, regime info.\n",
        "\n",
        "---\n",
        "\n",
        "### **3.2 Feature Sets**  \n",
        "**Description:** Select and optionally enhance the clean features from Section 1. Add interaction features and regime-aware taps if configured. Save exact feature lists for reproducibility.  \n",
        "\n",
        "**Outputs:**  \n",
        "- `feature_list.json` — exact features used.  \n",
        "- `feature_importance_baseline.csv` — baseline feature importances.\n",
        "\n",
        "---\n",
        "\n",
        "### **3.3 Model Suite**  \n",
        "**Description:** Train multiple base models for each horizon and window:  \n",
        "1. Multifactor composite — value, momentum, quality blend per regime.  \n",
        "2. LSTM — sequence modeling with MC-Dropout for uncertainty.  \n",
        "3. Tabular ensembles — LightGBM, XGBoost, optional MLP with quantile heads.  \n",
        "4. Stacking meta-learner — combines base model outputs optimally.  \n",
        "\n",
        "**Outputs:**  \n",
        "- Model artifacts, OOF/TEST predictions, feature importances.\n",
        "\n",
        "---\n",
        "\n",
        "### **3.4 Uncertainty, Calibration, & Confidence**  \n",
        "**Description:** Aggregate uncertainty estimates from MC-Dropout, quantile spreads, and model dispersion. Calibrate probability outputs and compute expected Sharpe ratios as a confidence score.  \n",
        "\n",
        "**Outputs:**  \n",
        "- `uq_summary.json` — uncertainty stats.  \n",
        "- Calibration plots & CSVs.\n",
        "\n",
        "---\n",
        "\n",
        "### **3.5 Regime-Aware Blending**  \n",
        "**Description:** Fit per-regime weights on base model outputs and blend them according to regime probabilities. Smooth weights over time and cap risk-prone factors in adverse regimes.  \n",
        "\n",
        "**Outputs:**  \n",
        "- `blend_<h>_win<id>.json` — blending configs.  \n",
        "- Diagnostics CSVs.\n",
        "\n",
        "---\n",
        "\n",
        "### **3.6 Walk-Forward Integration**  \n",
        "**Description:** Execute the modeling loop over each walk-forward window, save OOF and TEST predictions, and stitch TEST predictions into continuous time-series files.  \n",
        "\n",
        "**Outputs:**  \n",
        "- `alpha_raw.parquet` — per model/horizon.  \n",
        "- `alpha_ensemble.parquet` — final blended forecasts with mean, sigma, e_sharpe, regime info.\n",
        "\n",
        "---\n",
        "\n",
        "### **3.7 Quality Gates & Diagnostics**  \n",
        "**Description:** Validate outputs against leakage, stability, and performance thresholds. Check information coefficients, spreads, residual betas, and uncertainty calibration.  \n",
        "\n",
        "**Outputs:**  \n",
        "- `validation_report.json` — pass/fail on all checks.  \n",
        "- Diagnostics plots and ablation studies.\n",
        "\n",
        "---\n",
        "\n",
        "### **3.8 Interfaces to §4 & §5**  \n",
        "**Description:** Export all necessary artifacts for the portfolio construction and RL sizing stages, including the stitched ensemble predictions, targets, and feature lists. Provide helper functions to build RL state inputs.  \n",
        "\n",
        "**Outputs:**  \n",
        "- `alpha_ensemble.parquet`  \n",
        "- `targets.parquet`  \n",
        "- `feature_list.json`\n",
        "\n",
        "---\n",
        "\n",
        "### **3.9 Reproducibility & Manifest**  \n",
        "**Description:** Save all configuration, run manifests, hashes, seeds, and metadata to guarantee deterministic reruns.  \n",
        "\n",
        "**Outputs:**  \n",
        "- `alpha_config_effective.json`  \n",
        "- `run_manifest.json`  \n",
        "- `run_fingerprint.txt`  \n",
        "- `cv_manifest_<winid>.json`\n",
        "\n",
        "---\n",
        "\n",
        "### **3.T — Test Plan & Definition of Done (DoD)**  \n",
        "**Description:** Define all validation checks required for completion and passing of Section 3.  \n",
        "\n",
        "**Checks:**  \n",
        "- **Data integrity & leakage** — no NaNs, unique keys, lag compliance.  \n",
        "- **Model integrity** — correct CV setup, acceptable OOF–TEST gap.  \n",
        "- **Performance** — IC ≥ 0.05 or spread ≥ 20 bps, per-regime IC ≥ 0.03 in ≥2 regimes.  \n",
        "- **Stability** — no long negative IC runs.  \n",
        "- **UQ** — monotonic relationship between confidence and realized returns.  \n",
        "- **Neutrality** — |β_SPY| ≤ 0.05, balanced sector exposure.  \n",
        "- **Blending** — regime weights bounded, stable over time.  \n",
        "- **Artifacts** — all required outputs present and match manifest.\n",
        "\n",
        "**DoD:** Section 3 is complete when:  \n",
        "- All artifacts are written.  \n",
        "- All tests in `validation_report.json` pass.  \n",
        "- Performance thresholds met.  \n",
        "- Reproducibility manifests are updated and correct.\n",
        "\n",
        "</details>\n"
      ],
      "metadata": {
        "id": "IOCm9QWEhRRM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.0 Scope & Inputs (Reuse, Don’t Recompute)**\n",
        "\n",
        "**Goal:**  \n",
        "Initialize the **Alpha Layer** workflow by defining **all inputs, configurations, and dependencies** required for model training and forecasting. This step reuses existing cleaned artifacts from Section 1 (features) and Section 2 (regime modeling) rather than recomputing them, ensuring consistency and reproducibility across the pipeline.\n",
        "\n",
        "---\n",
        "\n",
        "### **Description**\n",
        "The Alpha Layer relies on:\n",
        "- **Feature data** — modeling-ready, leakage-free cross-sectional features from Section 1.\n",
        "- **Regime data** — daily regime labels and probabilities from Section 2, plus regime-specific control parameters.\n",
        "- **Model configuration** — an `AlphaConfig` object specifying target horizons, models to train, cross-validation structure, uncertainty estimation methods, and artifact paths.\n",
        "\n",
        "This setup phase ensures that all subsequent modeling steps (Sections 3.1–3.9) have consistent, correctly versioned inputs and configuration files. No modeling occurs here — only input gathering, integrity checks, and config initialization.\n",
        "\n",
        "---\n",
        "\n",
        "### **Inputs from Section 1 — Feature Engineering**\n",
        "- `features_filtered.parquet`  \n",
        "  → Fully preprocessed, date-aligned, leakage-free per-asset panel for the trading universe.  \n",
        "- `universe.csv`  \n",
        "  → Canonical equity universe (S&P 100 or dynamically filtered).  \n",
        "- Supporting metadata:  \n",
        "  - `cs_cols` — cross-sectional feature names.  \n",
        "  - `non_feature_cols` — identifiers, timestamps, and metadata columns.  \n",
        "  - `cols_to_shift` — features requiring lag alignment.  \n",
        "  - `dates_all` — aligned date index for all assets.  \n",
        "  - `px_daily_all` — adjusted daily close prices for target computation.\n",
        "\n",
        "---\n",
        "\n",
        "### **Inputs from Section 2 — Regime Modeling**\n",
        "- `artifacts/regimes/regime_labels.parquet`  \n",
        "  → Daily regime classification (Risk-On, Risk-Off, Transition) with posterior probabilities.  \n",
        "- `artifacts/regimes/regime_policy_map.json`  \n",
        "  → Latest regime label and **confidence scalar** `g` for blending and model weighting.  \n",
        "- `artifacts/regimes/window_manifest.json` or `windows_index.json`  \n",
        "  → Walk-forward train/test window boundaries for reproducible modeling.\n",
        "\n",
        "---\n",
        "\n",
        "### **New Configuration — AlphaConfig**\n",
        "A single, explicit configuration object for the Alpha Layer, containing:\n",
        "- **Horizon settings**: prediction targets (e.g., `t+5`, `t+10` excess returns).  \n",
        "- **Target definition**: return computation method (hedged/unhedged, volatility adjustment).  \n",
        "- **Model suite**: multifactor composite, LSTM, tabular ensembles, stacking meta-learner.  \n",
        "- **Cross-validation**: purged & embargoed folds, windowing parameters.  \n",
        "- **Uncertainty estimation**: MC-Dropout, quantile regression, ensemble dispersion.  \n",
        "- **Loss functions & optimization**: per-model objective configuration.  \n",
        "- **Regime hooks**: regime-specific model weights, factor emphasis, and risk constraints.  \n",
        "- **Artifact paths**: output directories for models, predictions, diagnostics, and manifests.\n",
        "\n",
        "---\n",
        "\n",
        "### **Outputs of 3.0**\n",
        "- `alpha_config_effective.json` — frozen configuration used for this run.  \n",
        "- `alpha_input_manifest.json` — versioned list of all Section 1 & 2 inputs, with hashes for reproducibility.  \n",
        "- Initial log entry confirming that **all required inputs are present, consistent, and in sync** with the latest run of Sections 1 & 2.\n",
        "\n",
        "---\n",
        "\n",
        "**Definition of Done (3.0)**  \n",
        "✅ All inputs from Sections 1 & 2 successfully loaded and validated.  \n",
        "✅ `AlphaConfig` populated and saved.  \n",
        "✅ No recomputation of features or regimes — hashes match expected values.  \n",
        "✅ Input manifest written and logged for reproducibility.\n"
      ],
      "metadata": {
        "id": "cebUtfxmjv0T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load features_filtered.parquet\n",
        "# --- Inspect features_filtered.parquet & flag any \"SPY\" in column names ---\n",
        "\n",
        "# If needed in your Colab, uncomment the next line:\n",
        "# !pip -q install pyarrow fastparquet\n",
        "\n",
        "import re, pandas as pd, numpy as np\n",
        "\n",
        "PATH = \"features_filtered.parquet\"   # adjust if stored elsewhere\n",
        "df = pd.read_parquet(PATH)\n",
        "\n",
        "# Summary\n",
        "n_rows, n_cols = df.shape\n",
        "date_min = pd.to_datetime(df[\"date\"]).min() if \"date\" in df.columns else None\n",
        "date_max = pd.to_datetime(df[\"date\"]).max() if \"date\" in df.columns else None\n",
        "n_tickers = df[\"ticker\"].nunique() if \"ticker\" in df.columns else None\n",
        "print(f\"{PATH} → rows={n_rows:,}, cols={n_cols:,}, tickers={n_tickers}, dates=[{date_min} → {date_max}]\")\n",
        "\n",
        "# Columns that are *not* predictive features\n",
        "NON_FEATURE_COLS = {\"date\",\"ticker\",\"open\",\"high\",\"low\",\"close\",\"adj_close\",\"volume\"}\n",
        "\n",
        "# Buckets for readability (same logic as your pipeline)\n",
        "CONTEXT_COLS = {\"spy_rv_20\",\"vix_close\",\"breadth\",\"spy_ret\"}\n",
        "FUNDAMENTALS = {\"book_to_price\",\"earnings_yield\",\"cf_yield\",\"shareholder_yield\",\n",
        "                \"gross_profitability\",\"roe\",\"accruals\",\"leverage\"}\n",
        "\n",
        "all_cols = list(df.columns)\n",
        "feature_cols = [c for c in all_cols if c not in NON_FEATURE_COLS]\n",
        "\n",
        "# Group by type\n",
        "lags = sorted([c for c in feature_cols if re.fullmatch(r\"ret_lag_\\d+\", c)], key=lambda x: int(x.split(\"_\")[-1]))\n",
        "context = [c for c in feature_cols if c in CONTEXT_COLS]\n",
        "fundas = [c for c in feature_cols if c in FUNDAMENTALS]\n",
        "funda_masks = sorted([c for c in feature_cols if c.endswith(\"_is_missing\") and c.replace(\"_is_missing\",\"\") in FUNDAMENTALS])\n",
        "\n",
        "TECH_BASE = {\n",
        "    \"ret_1d\",\"rv_20\",\"atr_14\",\"mom_20\",\"mom_6m\",\"mom_12m\",\"mom_12_1\",\"mom_6_1\",\n",
        "    \"sma_20\",\"sma_50\",\"sma_20_gt_50\",\"slope_20\",\"mom_20_vs_vol\"\n",
        "}\n",
        "tech_known = sorted([c for c in feature_cols if c in TECH_BASE])\n",
        "tech_extra = sorted([\n",
        "    c for c in feature_cols\n",
        "    if c not in tech_known + lags + context + fundas + funda_masks\n",
        "])\n",
        "\n",
        "# Print clean inventory\n",
        "def show(title, cols):\n",
        "    print(f\"\\n{title} ({len(cols)}):\")\n",
        "    if cols: print(\", \".join(cols))\n",
        "    else:    print(\"—\")\n",
        "\n",
        "print(f\"\\nPredictive feature columns: {len(feature_cols)} of {n_cols}\")\n",
        "show(\"Market context\", context)\n",
        "show(\"Price/technical (known)\", tech_known)\n",
        "show(\"Price/technical (extra detected)\", tech_extra)\n",
        "show(\"Return lags\", lags)\n",
        "show(\"Fundamentals\", fundas)\n",
        "show(\"Fundamentals — missing masks\", funda_masks)\n",
        "\n",
        "# ----- SPY name scan -----\n",
        "# Flag any column that contains \"SPY\" anywhere (e.g., \"ESPY\"), case-insensitive.\n",
        "spy_name_hits = [c for c in feature_cols if \"SPY\" in c.upper()]\n",
        "\n",
        "# Allow-listed SPY references (expected market context)\n",
        "SPY_WHITELIST = {\"spy_rv_20\",\"spy_ret\"}  # extend if you intentionally keep others\n",
        "spy_suspect = [c for c in spy_name_hits if c.lower() not in SPY_WHITELIST]\n",
        "\n",
        "print(\"\\nColumns containing 'SPY' (case-insensitive):\", spy_name_hits or \"None\")\n",
        "if spy_suspect:\n",
        "    print(\"⚠️ Unexpected 'SPY' in feature names (not in whitelist):\", spy_suspect)\n",
        "else:\n",
        "    print(\"✅ No unexpected 'SPY' tokens in feature names (only allowed context present).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r00dK0yFKWcU",
        "outputId": "79183d0f-2d29-4eeb-869a-99c320cf6bab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "features_filtered.parquet → rows=2,223,976, cols=94, tickers=514, dates=[2007-02-05 00:00:00 → 2025-08-11 00:00:00]\n",
            "\n",
            "Predictive feature columns: 86 of 94\n",
            "\n",
            "Market context (3):\n",
            "spy_rv_20, vix_close, breadth\n",
            "\n",
            "Price/technical (known) (13):\n",
            "atr_14, mom_12_1, mom_12m, mom_20, mom_20_vs_vol, mom_6_1, mom_6m, ret_1d, rv_20, slope_20, sma_20, sma_20_gt_50, sma_50\n",
            "\n",
            "Price/technical (extra detected) (0):\n",
            "—\n",
            "\n",
            "Return lags (60):\n",
            "ret_lag_1, ret_lag_2, ret_lag_3, ret_lag_4, ret_lag_5, ret_lag_6, ret_lag_7, ret_lag_8, ret_lag_9, ret_lag_10, ret_lag_11, ret_lag_12, ret_lag_13, ret_lag_14, ret_lag_15, ret_lag_16, ret_lag_17, ret_lag_18, ret_lag_19, ret_lag_20, ret_lag_21, ret_lag_22, ret_lag_23, ret_lag_24, ret_lag_25, ret_lag_26, ret_lag_27, ret_lag_28, ret_lag_29, ret_lag_30, ret_lag_31, ret_lag_32, ret_lag_33, ret_lag_34, ret_lag_35, ret_lag_36, ret_lag_37, ret_lag_38, ret_lag_39, ret_lag_40, ret_lag_41, ret_lag_42, ret_lag_43, ret_lag_44, ret_lag_45, ret_lag_46, ret_lag_47, ret_lag_48, ret_lag_49, ret_lag_50, ret_lag_51, ret_lag_52, ret_lag_53, ret_lag_54, ret_lag_55, ret_lag_56, ret_lag_57, ret_lag_58, ret_lag_59, ret_lag_60\n",
            "\n",
            "Fundamentals (5):\n",
            "book_to_price, cf_yield, shareholder_yield, gross_profitability, leverage\n",
            "\n",
            "Fundamentals — missing masks (5):\n",
            "book_to_price_is_missing, cf_yield_is_missing, gross_profitability_is_missing, leverage_is_missing, shareholder_yield_is_missing\n",
            "\n",
            "Columns containing 'SPY' (case-insensitive): ['spy_rv_20']\n",
            "✅ No unexpected 'SPY' tokens in feature names (only allowed context present).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.1 — Config Primer (globals only; safe to import everywhere)\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "ARTIFACTS_DIR = Path(\"artifacts\")\n",
        "ALPHA_DIR     = ARTIFACTS_DIR / \"alpha\"\n",
        "PANELS_DIR    = ALPHA_DIR / \"panels\"\n",
        "REGIME_DIR    = ARTIFACTS_DIR / \"regimes\"\n",
        "\n",
        "ALPHA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "PANELS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Section-1 inputs\n",
        "FEATURES_FP   = Path(\"features_filtered.parquet\")\n",
        "UNIVERSE_FP   = Path(\"universe.csv\")\n",
        "META_YAML_FP  = Path(\"meta.yaml\")  # optional\n",
        "\n",
        "# Section-2 inputs\n",
        "REGIME_LABELS_FP     = REGIME_DIR / \"regime_labels.parquet\"\n",
        "REGIME_POLICY_MAP_FP = REGIME_DIR / \"regime_policy_map.json\"\n",
        "WIN_DIR              = REGIME_DIR / \"windowed\"\n",
        "WINDEX_FP            = WIN_DIR / \"windows_index.json\"      # preferred\n",
        "WMANIFEST_FP         = REGIME_DIR / \"window_manifest.json\" # fallback\n",
        "\n",
        "# Config knobs (#TOCHANGE for real runs)\n",
        "HORIZONS         = [5, 10]      # TOCHANGE real: [5, 10, 20]\n",
        "ROLL_LOOKBACK_D  = 126          # TOCHANGE real: 252\n",
        "SECTOR_NEUTRAL   = False        # TOCHANGE real: True (with sector_map.csv)\n",
        "DEBUG_MAX_TICKERS= None         # TOCHANGE e.g., 120 for smoke; real: None\n",
        "N_SMOOTH_G       = 3            # TOCHANGE real: 5–10\n",
        "LAMBDA_RIDGE     = 1e-8\n",
        "\n",
        "# Optional sector map\n",
        "SECTOR_ETF_MAP_FP = Path(\"sector_map.csv\")\n",
        "\n",
        "# Planned outputs used by later cells\n",
        "PANEL_MASTER_FP = ALPHA_DIR / \"panel_master.parquet\"\n",
        "TARGETS_FP      = ALPHA_DIR / \"targets.parquet\"\n",
        "TARGETS_QC_FP   = ALPHA_DIR / \"targets_qc.json\"\n",
        "FEATURE_LIST_FP = ALPHA_DIR / \"feature_list.json\"\n",
        "LEAK_SCAN_FP    = ALPHA_DIR / \"leakage_scan.json\""
      ],
      "metadata": {
        "id": "ktItA-RdJkkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature inventory + SPY token scan (no side effects)\n",
        "\n",
        "import pandas as pd, re\n",
        "\n",
        "df = pd.read_parquet(FEATURES_FP)\n",
        "n_rows, n_cols = df.shape\n",
        "date_min = pd.to_datetime(df[\"date\"]).min()\n",
        "date_max = pd.to_datetime(df[\"date\"]).max()\n",
        "n_tickers = df[\"ticker\"].nunique()\n",
        "print(f\"{FEATURES_FP} → rows={n_rows:,}, cols={n_cols:,}, tickers={n_tickers}, dates=[{date_min} → {date_max}]\")\n",
        "\n",
        "NON_FEATURE_COLS = {\"date\",\"ticker\",\"open\",\"high\",\"low\",\"close\",\"adj_close\",\"volume\"}\n",
        "feature_cols = [c for c in df.columns if c not in NON_FEATURE_COLS]\n",
        "\n",
        "TECH_BASE = {\"ret_1d\",\"rv_20\",\"atr_14\",\"mom_20\",\"mom_6m\",\"mom_12m\",\"mom_12_1\",\"mom_6_1\",\n",
        "             \"sma_20\",\"sma_50\",\"sma_20_gt_50\",\"slope_20\",\"mom_20_vs_vol\"}\n",
        "lags = sorted([c for c in feature_cols if re.fullmatch(r\"ret_lag_\\d+\", c)], key=lambda x: int(x.split(\"_\")[-1]))\n",
        "tech_known = sorted([c for c in feature_cols if c in TECH_BASE])\n",
        "\n",
        "print(f\"\\nPredictive feature columns: {len(feature_cols)} of {n_cols}\")\n",
        "print(f\"Price/technical (known) ({len(tech_known)}): {', '.join(tech_known) or '—'}\")\n",
        "print(f\"Return lags ({len(lags)}): {', '.join(lags[:12])}{' …' if len(lags)>12 else ''}\")\n",
        "\n",
        "spy_hits = [c for c in feature_cols if 'SPY' in c.upper()]\n",
        "whitelist = {'spy_rv_20','spy_ret'}  # allowed context, if present\n",
        "suspect = [c for c in spy_hits if c.lower() not in whitelist]\n",
        "print(\"\\nColumns containing 'SPY':\", spy_hits or \"None\")\n",
        "print(\"✅ No unexpected 'SPY' tokens.\" if not suspect else f\"⚠️ Unexpected 'SPY' features: {suspect}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIfLPVN5SrY9",
        "outputId": "08b486b8-942d-42ef-e6cc-9aafa9e44421"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "features_filtered.parquet → rows=2,223,976, cols=94, tickers=514, dates=[2007-02-05 00:00:00 → 2025-08-11 00:00:00]\n",
            "\n",
            "Predictive feature columns: 86 of 94\n",
            "Price/technical (known) (13): atr_14, mom_12_1, mom_12m, mom_20, mom_20_vs_vol, mom_6_1, mom_6m, ret_1d, rv_20, slope_20, sma_20, sma_20_gt_50, sma_50\n",
            "Return lags (60): ret_lag_1, ret_lag_2, ret_lag_3, ret_lag_4, ret_lag_5, ret_lag_6, ret_lag_7, ret_lag_8, ret_lag_9, ret_lag_10, ret_lag_11, ret_lag_12 …\n",
            "\n",
            "Columns containing 'SPY': ['spy_rv_20']\n",
            "✅ No unexpected 'SPY' tokens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- Hedge loaders (new) ----\n",
        "RAW_PRICES_FP = Path(\"raw_prices.parquet\")                    # from Section 1\n",
        "MKT_PANEL_FP  = REGIME_DIR / \"market_panel.parquet\"           # from Section 2\n",
        "\n",
        "def load_spy_returns() -> pd.Series:\n",
        "    \"\"\"\n",
        "    Prefer Section 2's market_panel (column 'spy_ret').\n",
        "    Fallback: compute from Section 1's raw_prices for 'SPY'.\n",
        "    Returns a pd.Series indexed by date named 'spy_ret'.\n",
        "    \"\"\"\n",
        "    # Preferred source\n",
        "    if MKT_PANEL_FP.exists():\n",
        "        mp = pd.read_parquet(MKT_PANEL_FP)\n",
        "        mp[\"date\"] = pd.to_datetime(mp[\"date\"])\n",
        "        mp = mp.sort_values(\"date\")\n",
        "        if \"spy_ret\" in mp.columns:\n",
        "            return mp.set_index(\"date\")[\"spy_ret\"].rename(\"spy_ret\")\n",
        "\n",
        "    # Fallback\n",
        "    if RAW_PRICES_FP.exists():\n",
        "        rp = pd.read_parquet(RAW_PRICES_FP)  # expected long format: date, ticker, adj_close (at minimum)\n",
        "        rp[\"date\"] = pd.to_datetime(rp[\"date\"])\n",
        "        spy = rp[rp[\"ticker\"] == \"SPY\"][[\"date\", \"adj_close\"]].sort_values(\"date\")\n",
        "        if spy.empty:\n",
        "            raise RuntimeError(\"SPY not found in raw_prices.parquet; cannot compute spy_ret.\")\n",
        "        spy[\"spy_ret\"] = np.log(spy[\"adj_close\"]).diff()\n",
        "        return spy.set_index(\"date\")[\"spy_ret\"].rename(\"spy_ret\")\n",
        "\n",
        "    raise RuntimeError(\"Neither market_panel.parquet nor raw_prices.parquet available for SPY returns.\")\n",
        "\n",
        "\n",
        "def load_sector_etf_returns(etf_list: list[str]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Load sector ETF daily log returns from raw_prices.parquet for tickers in etf_list.\n",
        "    Returns a DataFrame with columns: date, ticker, sector_ret_1d\n",
        "    \"\"\"\n",
        "    if not RAW_PRICES_FP.exists():\n",
        "        raise RuntimeError(\"raw_prices.parquet missing; required for sector ETF returns.\")\n",
        "    rp = pd.read_parquet(RAW_PRICES_FP)\n",
        "    rp[\"date\"] = pd.to_datetime(rp[\"date\"])\n",
        "    etf_px = rp[rp[\"ticker\"].isin(etf_list)][[\"date\", \"ticker\", \"adj_close\"]].sort_values([\"ticker\",\"date\"])\n",
        "    if etf_px.empty:\n",
        "        raise RuntimeError(f\"No sector ETF prices found in raw_prices.parquet for: {etf_list}\")\n",
        "    etf_px[\"sector_ret_1d\"] = etf_px.groupby(\"ticker\")[\"adj_close\"].transform(lambda x: np.log(x).diff())\n",
        "    return etf_px[[\"date\", \"ticker\", \"sector_ret_1d\"]]\n"
      ],
      "metadata": {
        "id": "ipAw-ehLMz4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Section 3.1.1 + 3.1.2 — Data Load/Trim & Regime Context\n",
        "# What this provides:\n",
        "#   - load_base_data():   loads Section-1 features, trims to universe, extracts price panel,\n",
        "#                         determines X feature columns (cs_cols), and runs light QC.\n",
        "#   - load_regime_context(): loads Section-2 regime labels, picks smoothed label when present,\n",
        "#                         computes historical 'g' from posteriors (if needed), smooths it, light QC.\n",
        "#\n",
        "# Notes:\n",
        "#   • Reuses artifacts from Sections 1–2. No recomputation of features or regimes here.\n",
        "#   • Low-compute defaults are set for smoke tests; real-run values are marked with #TOCHANGE.\n",
        "#   • Outputs are returned as in-memory DataFrames; optional QC artifacts are written to disk.\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Optional: pull cs_cols from meta.yaml if present\n",
        "try:\n",
        "    import yaml  # type: ignore\n",
        "    HAVE_YAML = True\n",
        "except Exception:\n",
        "    HAVE_YAML = False\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Paths (reuse from earlier sections where possible)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "ARTIFACTS_DIR = Path(\"artifacts\")\n",
        "ALPHA_DIR = ARTIFACTS_DIR / \"alpha\"\n",
        "PANELS_DIR = ALPHA_DIR / \"panels\"\n",
        "REGIME_DIR = ARTIFACTS_DIR / \"regimes\"\n",
        "\n",
        "ALPHA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "PANELS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "FEATURES_FP = Path(\"features_filtered.parquet\")          # Section 1 output\n",
        "UNIVERSE_FP = Path(\"universe.csv\")                       # Section 1 output\n",
        "META_YAML_FP = Path(\"meta.yaml\")                         # Section 1 optional meta\n",
        "\n",
        "REGIME_LABELS_FP = REGIME_DIR / \"regime_labels.parquet\"  # Section 2 output\n",
        "REGIME_POLICY_MAP_FP = REGIME_DIR / \"regime_policy_map.json\"  # Section 2 (latest-only g)\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Config knobs for 3.1.1 / 3.1.2\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "DEBUG_MAX_TICKERS: Optional[int] = None   # #TOCHANGE real-run: None; smoke: e.g., 120\n",
        "PX_FALLBACK_COL = \"adj_close\"             # must exist in features parquet\n",
        "N_SMOOTH_G = 3                            # #TOCHANGE real-run: 5–10 (smoother 'g')\n",
        "\n",
        "# Non-feature columns (from Section 1 conventions)\n",
        "NON_FEATURE_COLS_BASE = {\n",
        "    \"date\", \"ticker\", \"open\", \"high\", \"low\", \"close\", \"adj_close\", \"volume\"\n",
        "}\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Utilities\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "def _to_datetime(df: pd.DataFrame, col: str = \"date\") -> pd.DataFrame:\n",
        "    if col in df.columns:\n",
        "        df[col] = pd.to_datetime(df[col])\n",
        "    return df\n",
        "\n",
        "def _load_meta_cs_cols(meta_yaml_fp: Path) -> Optional[List[str]]:\n",
        "    if meta_yaml_fp.exists() and HAVE_YAML:\n",
        "        try:\n",
        "            meta = yaml.safe_load(meta_yaml_fp.read_text())\n",
        "            cs_cols = meta.get(\"cs_cols\", None)\n",
        "            if isinstance(cs_cols, list) and cs_cols:\n",
        "                return cs_cols\n",
        "        except Exception:\n",
        "            pass\n",
        "    return None\n",
        "\n",
        "def _derive_cs_cols(df: pd.DataFrame) -> List[str]:\n",
        "    # Heuristic: everything that is not in known non-feature cols and not a mask label\n",
        "    # (Section 1 already standardized & winsorized these)\n",
        "    return [\n",
        "        c for c in df.columns\n",
        "        if c not in NON_FEATURE_COLS_BASE\n",
        "        and not c.lower().startswith(\"mask_\")\n",
        "        and c not in {\"regime_label\", \"regime_label_smoothed\"}  # safety\n",
        "    ]\n",
        "\n",
        "def _assert_unique_keys(df: pd.DataFrame, keys=(\"date\", \"ticker\")) -> None:\n",
        "    if df.duplicated(list(keys)).any():\n",
        "        dups = int(df.duplicated(list(keys)).sum())\n",
        "        raise AssertionError(f\"Found {dups} duplicate ({','.join(keys)}) rows in features panel.\")\n",
        "\n",
        "def _check_monotonic_dates(df: pd.DataFrame) -> None:\n",
        "    # Light check: sample a few tickers to verify monotonicity\n",
        "    sample = df[\"ticker\"].drop_duplicates().sample(min(10, df[\"ticker\"].nunique()), random_state=42)\n",
        "    for t in sample:\n",
        "        s = df.loc[df[\"ticker\"] == t, \"date\"]\n",
        "        if not s.is_monotonic_increasing:\n",
        "            raise AssertionError(f\"Non-monotonic dates for ticker {t}.\")\n",
        "\n",
        "def _write_qc_csv(path: Path, stats: Dict) -> None:\n",
        "    pd.DataFrame([stats]).to_csv(path, index=False)\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 3.1.1 — Load & Trim Base Data (reuse)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "def load_base_data(\n",
        "    features_fp: Path = FEATURES_FP,\n",
        "    universe_fp: Path = UNIVERSE_FP,\n",
        "    meta_yaml_fp: Path = META_YAML_FP,\n",
        "    debug_max_tickers: Optional[int] = DEBUG_MAX_TICKERS,\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame, List[str]]:\n",
        "    \"\"\"\n",
        "    Load modeling-ready features (Section 1), trim to universe, optionally\n",
        "    downsample tickers for smoke runs, extract a minimal price panel, and\n",
        "    determine X feature columns (cs_cols).\n",
        "\n",
        "    Returns:\n",
        "        feats  : DataFrame — trimmed features (ready for merge in 3.1.6)\n",
        "        px     : DataFrame — minimal price panel ['date','ticker','adj_close']\n",
        "        cs_cols: List[str] — feature columns to feed models later\n",
        "    \"\"\"\n",
        "    # Load features\n",
        "    feats = pd.read_parquet(features_fp)\n",
        "    feats = _to_datetime(feats, \"date\")\n",
        "    feats[\"ticker\"] = feats[\"ticker\"].astype(str)\n",
        "\n",
        "    # Join to canonical universe\n",
        "    uni = pd.read_csv(universe_fp)[\"ticker\"].astype(str)\n",
        "    feats = feats[feats[\"ticker\"].isin(uni)].copy()\n",
        "\n",
        "    # Optional: downsample for smoke runs\n",
        "    if debug_max_tickers is not None:\n",
        "        keep = feats[\"ticker\"].drop_duplicates().head(debug_max_tickers)\n",
        "        feats = feats[feats[\"ticker\"].isin(keep)].copy()\n",
        "\n",
        "    # Basic sort & schema assertions\n",
        "    feats = feats.sort_values([\"ticker\", \"date\"])\n",
        "    _assert_unique_keys(feats, keys=(\"date\", \"ticker\"))\n",
        "    _check_monotonic_dates(feats)\n",
        "\n",
        "    # Determine cs_cols\n",
        "    cs_cols = _load_meta_cs_cols(meta_yaml_fp)\n",
        "    if cs_cols is None:\n",
        "        cs_cols = _derive_cs_cols(feats)\n",
        "\n",
        "    # Extract minimal price panel for later target calc\n",
        "    if PX_FALLBACK_COL not in feats.columns:\n",
        "        raise KeyError(\n",
        "            f\"'{PX_FALLBACK_COL}' not found in features parquet — \"\n",
        "            f\"please ensure Section 1 wrote adj_close. \"\n",
        "            f\"(Alternatively, pass a separate price panel from raw_prices.parquet.)\"\n",
        "        )\n",
        "    px = feats[[\"date\", \"ticker\", PX_FALLBACK_COL]].drop_duplicates().copy()\n",
        "\n",
        "    # QC write (optional)\n",
        "    qc_stats = {\n",
        "        \"rows\": int(len(feats)),\n",
        "        \"dates\": int(feats[\"date\"].nunique()),\n",
        "        \"tickers\": int(feats[\"ticker\"].nunique()),\n",
        "        \"debug_max_tickers\": debug_max_tickers,\n",
        "        \"adj_close_missing_frac\": float(px[PX_FALLBACK_COL].isna().mean()),\n",
        "        \"n_cs_cols\": len(cs_cols),\n",
        "    }\n",
        "    _write_qc_csv(PANELS_DIR / \"panel_base_qc.csv\", qc_stats)\n",
        "\n",
        "    return feats, px, cs_cols\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 3.1.2 — Regime Context (reuse + light transform)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "def _entropy_norm(p: np.ndarray, eps: float = 1e-12) -> float:\n",
        "    \"\"\"Normalized entropy in [0,1].\"\"\"\n",
        "    p = np.clip(p, eps, 1.0)\n",
        "    p = p / p.sum()\n",
        "    H = -(p * np.log(p)).sum()\n",
        "    return float(H / np.log(len(p)))  # normalize by log(K)\n",
        "\n",
        "def _compute_g_from_posteriors(df: pd.DataFrame, n_smooth: int = N_SMOOTH_G) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Build a per-date aggressiveness scalar 'g' from posteriors p*:\n",
        "      c_max = max(p)\n",
        "      c_ent = 1 - entropy_norm(p)\n",
        "      c = 0.5*c_max + 0.5*c_ent\n",
        "      g = 0.35 + (1.00 - 0.35) * c  (same as §2.7)\n",
        "    Then smooth with a trailing mean over n_smooth days.\n",
        "    \"\"\"\n",
        "    p_cols = [c for c in df.columns if c.startswith(\"p\")]\n",
        "    if not p_cols:\n",
        "        # No posteriors to compute g from\n",
        "        return pd.Series(index=df[\"date\"], dtype=float, name=\"g\")\n",
        "\n",
        "    P = df[p_cols].to_numpy()\n",
        "    c_max = P.max(axis=1)\n",
        "    c_ent = np.array([1.0 - _entropy_norm(P[i, :]) for i in range(P.shape[0])])\n",
        "    c = 0.5 * c_max + 0.5 * c_ent\n",
        "    g = 0.35 + (1.00 - 0.35) * c\n",
        "    g_ser = pd.Series(g, index=df.index, name=\"g\")\n",
        "    if n_smooth and n_smooth > 1:\n",
        "        g_ser = g_ser.rolling(n_smooth, min_periods=1).mean()\n",
        "    return g_ser\n",
        "\n",
        "def load_regime_context(\n",
        "    regime_labels_fp: Path = REGIME_LABELS_FP,\n",
        "    regime_policy_map_fp: Path = REGIME_POLICY_MAP_FP,\n",
        "    n_smooth_g: int = N_SMOOTH_G,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Load regime labels from Section 2, pick smoothed label if present,\n",
        "    compute historical per-date 'g' from posteriors if needed, and\n",
        "    return a compact DataFrame for merging in 3.1.6.\n",
        "\n",
        "    Returns:\n",
        "        regimes_keep: DataFrame with columns:\n",
        "            ['date','regime_label_use', 'p0..pK-1'(if any), 'g']\n",
        "            (+ 'state_id_smoothed' optionally for debugging)\n",
        "    \"\"\"\n",
        "    rg = pd.read_parquet(regime_labels_fp).copy()\n",
        "    rg[\"date\"] = pd.to_datetime(rg[\"date\"])\n",
        "    rg = rg.sort_values(\"date\").reset_index(drop=True)\n",
        "\n",
        "    # Choose label column\n",
        "    if \"regime_label_smoothed\" in rg.columns:\n",
        "        rg[\"regime_label_use\"] = rg[\"regime_label_smoothed\"]\n",
        "    elif \"regime_label\" in rg.columns:\n",
        "        rg[\"regime_label_use\"] = rg[\"regime_label\"]\n",
        "    else:\n",
        "        raise KeyError(\"Regime labels missing both 'regime_label_smoothed' and 'regime_label'.\")\n",
        "\n",
        "    # If historical g is not stored, compute it from posteriors\n",
        "    p_cols = [c for c in rg.columns if c.startswith(\"p\")]\n",
        "    if \"g\" not in rg.columns:\n",
        "        rg[\"g\"] = _compute_g_from_posteriors(rg, n_smooth=n_smooth_g)\n",
        "\n",
        "    # Keep compact schema for downstream merges\n",
        "    keep_cols = [\"date\", \"regime_label_use\"]\n",
        "    if \"state_id_smoothed\" in rg.columns:\n",
        "        keep_cols.append(\"state_id_smoothed\")\n",
        "    keep_cols += p_cols\n",
        "    if \"g\" in rg.columns:\n",
        "        keep_cols.append(\"g\")\n",
        "\n",
        "    regimes_keep = rg[keep_cols].copy()\n",
        "\n",
        "    # Optional QC snapshot\n",
        "    qc = {\n",
        "        \"rows\": int(len(regimes_keep)),\n",
        "        \"dates\": int(regimes_keep[\"date\"].nunique()),\n",
        "        \"has_posteriors\": bool(len(p_cols) > 0),\n",
        "        \"K\": int(len(p_cols)) if p_cols else 0,\n",
        "        \"g_present\": bool(\"g\" in regimes_keep.columns),\n",
        "        \"g_stats\": None,\n",
        "        \"n_smooth_g\": n_smooth_g,\n",
        "        \"label_used\": \"regime_label_smoothed\" if \"regime_label_smoothed\" in rg.columns else \"regime_label\",\n",
        "    }\n",
        "    if \"g\" in regimes_keep.columns:\n",
        "        gvals = regimes_keep[\"g\"].dropna()\n",
        "        if len(gvals):\n",
        "            qc[\"g_stats\"] = {\n",
        "                \"min\": float(gvals.min()),\n",
        "                \"mean\": float(gvals.mean()),\n",
        "                \"max\": float(gvals.max()),\n",
        "            }\n",
        "    (ALPHA_DIR / \"regimes_qc.json\").write_text(json.dumps(qc, indent=2))\n",
        "\n",
        "    return regimes_keep\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Example (manual) usage:\n",
        "# feats, px, cs_cols = load_base_data()\n",
        "# regimes_keep = load_regime_context()\n",
        "# Next steps in 3.1.x: compute residual daily returns (vs SPY/sector), build forward\n",
        "# r_ex_h targets, merge features×targets×regimes, and split per walk-forward window.\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "# 3.1.1 + 3.1.2 glue (optional helper)\n",
        "def prep_3_1_3_context(\n",
        "    debug_max_tickers: int | None = DEBUG_MAX_TICKERS,\n",
        "    n_smooth_g: int = N_SMOOTH_G,\n",
        "):\n",
        "    \"\"\"\n",
        "    One-call handoff for 3.1.3:\n",
        "      - Loads trimmed features & price panel\n",
        "      - Loads regime context with per-date g\n",
        "      - Performs a couple of readiness checks\n",
        "    Returns:\n",
        "      feats, px, cs_cols, regimes_keep\n",
        "    \"\"\"\n",
        "    feats, px, cs_cols = load_base_data(\n",
        "        features_fp=FEATURES_FP,\n",
        "        universe_fp=UNIVERSE_FP,\n",
        "        meta_yaml_fp=META_YAML_FP,\n",
        "        debug_max_tickers=debug_max_tickers,\n",
        "    )\n",
        "    regimes_keep = load_regime_context(\n",
        "        regime_labels_fp=REGIME_LABELS_FP,\n",
        "        regime_policy_map_fp=REGIME_POLICY_MAP_FP,\n",
        "        n_smooth_g=n_smooth_g,\n",
        "    )\n",
        "\n",
        "    # readiness checks for 3.1.3\n",
        "    if \"adj_close\" not in px.columns:\n",
        "        raise RuntimeError(\"adj_close column is required in px for return computation.\")\n",
        "\n",
        "    return feats, px, cs_cols, regimes_keep\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Persist\n",
        "    RUN_SUMMARY_FP.write_text(json.dumps(s, indent=2))\n",
        "\n",
        "    # Human-readable fingerprint\n",
        "    txt = []\n",
        "    txt.append(\"=== Section 3.1 Run Summary ===\")\n",
        "    txt.append(f\"Horizons: {s['params']['horizons']}; Sector-neutral: {s['params']['sector_neutral']}; \"\n",
        "               f\"Lookback: {s['params']['roll_lookback_days']}d; Debug tickers: {s['params']['debug_max_tickers']}\")\n",
        "    txt.append(f\"Features rows: {s['shapes']['feats_rows']:,} | Panel rows: {s['shapes']['panel_master_rows']:,} | \"\n",
        "               f\"Targets rows: {s['shapes']['targets_rows']:,}\")\n",
        "    if qc_excess:\n",
        "        txt.append(f\"Sector-neutral enabled: {qc_excess.get('sector_neutral_enabled')}, \"\n",
        "                   f\"sector_beta_coverage: {qc_excess.get('sector_beta_coverage'):.3f}\")\n",
        "    txt.append(f\"Validation (3.1): {s['qc']['val_3_1_status']}  Notes: {', '.join(s['qc']['val_3_1_notes']) if s['qc']['val_3_1_notes'] else '-'}\")\n",
        "    txt.append(\"Artifacts:\")\n",
        "    for k, v in s[\"artifacts\"].items():\n",
        "        mark = \"✓\" if v[\"exists\"] else \"✗\"\n",
        "        size = f\"{v['size']:,}B\" if v[\"size\"] else \"-\"\n",
        "        txt.append(f\"  {mark} {k}: {v['path']} ({size})\")\n",
        "    if s[\"windows\"]:\n",
        "        txt.append(\"Windows:\")\n",
        "        for w in s[\"windows\"]:\n",
        "            txt.append(f\"  {w['win_id']}  train[{w['train_start']} → {w['train_end']}] rows={w['train_rows']:,} | \"\n",
        "                       f\"test[{w['test_start']} → {w['test_end']}] rows={w['test_rows']:,}\")\n",
        "    RUN_FINGERPRINT_FP.write_text(\"\\n\".join(txt))\n",
        "\n",
        "    if print_summary:\n",
        "        print(\"\\n\".join(txt))\n",
        "\n",
        "    return s\n",
        "\n",
        "# --- Guard the orchestrator so it doesn't run yet ---\n",
        "RUN_ORCHESTRATOR = False  # set to True when 3.1.3+ are implemented\n",
        "\n",
        "# Allow `python yourfile.py --run-3-1` style usage\n",
        "if __name__ == \"__main__\" and RUN_ORCHESTRATOR:\n",
        "    run_section_3_1(\n",
        "        horizons=HORIZONS,\n",
        "        sector_neutral=SECTOR_NEUTRAL,\n",
        "        roll_lookback=ROLL_LOOKBACK_D,\n",
        "        debug_max_tickers=DEBUG_MAX_TICKERS,\n",
        "        n_smooth_g=N_SMOOTH_G,\n",
        "        print_summary=True,\n",
        "    )\n",
        "\n",
        "\n",
        "# Example usage after running prep_3_1_3_context:\n",
        "feats, px, cs_cols, regimes_keep = prep_3_1_3_context()\n",
        "log_3_1_1_2_summary(feats, px, cs_cols, regimes_keep)"
      ],
      "metadata": {
        "id": "ErnR5GetzRjB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "786eb51a-0a2c-4b50-a856-c4add4845b1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[3.1.1 + 3.1.2] SUMMARY\n",
            "------------------------------------------------------------\n",
            "Features loaded: 2,174,912 rows, 86 features\n",
            "Price panel:     2,174,912 rows (503 tickers)\n",
            "Regimes loaded:  2,163 rows\n",
            "Feature cols:    ['ret_1d', 'ret_lag_1', 'ret_lag_2', 'ret_lag_3', 'ret_lag_4']...\n",
            "\n",
            "Artifacts written:\n",
            "  ✔ artifacts/alpha/panels/panel_base_qc.csv (exists)\n",
            "  ✔ artifacts/alpha/regimes_qc.json (exists)\n",
            "\n",
            "Preview feats:\n",
            "      date      open      high       low     close  adj_close    volume ticker    ret_1d  ret_lag_1  ret_lag_2  ret_lag_3  ret_lag_4  ret_lag_5  ret_lag_6  ret_lag_7  ret_lag_8  ret_lag_9  ret_lag_10  ret_lag_11  ret_lag_12  ret_lag_13  ret_lag_14  ret_lag_15  ret_lag_16  ret_lag_17  ret_lag_18  ret_lag_19  ret_lag_20  ret_lag_21  ret_lag_22  ret_lag_23  ret_lag_24  ret_lag_25  ret_lag_26  ret_lag_27  ret_lag_28  ret_lag_29  ret_lag_30  ret_lag_31  ret_lag_32  ret_lag_33  ret_lag_34  ret_lag_35  ret_lag_36  ret_lag_37  ret_lag_38  ret_lag_39  ret_lag_40  ret_lag_41  ret_lag_42  ret_lag_43  ret_lag_44  ret_lag_45  ret_lag_46  ret_lag_47  ret_lag_48  ret_lag_49  ret_lag_50  ret_lag_51  ret_lag_52  ret_lag_53  ret_lag_54  ret_lag_55  ret_lag_56  ret_lag_57  ret_lag_58  ret_lag_59  ret_lag_60     rv_20    atr_14    mom_20   mom_6m   mom_12m  mom_12_1  mom_6_1    sma_20    sma_50  sma_20_gt_50  slope_20  mom_20_vs_vol  spy_rv_20  vix_close  breadth  book_to_price  cf_yield  shareholder_yield  gross_profitability  leverage  book_to_price_is_missing  cf_yield_is_missing  shareholder_yield_is_missing  gross_profitability_is_missing  leverage_is_missing\n",
            "2007-02-05 22.632332 22.804007 22.346209 22.768240  20.269119 3511916.0      A -0.018182  -0.162254  -0.560152  -0.418691  -1.111428   0.907078  -0.527060  -0.883009  -0.092266  -0.122859    1.115720   -0.834329   -1.804219   -0.336530   -0.658734   -0.734745   -1.083497   -0.076312   -0.450011   -0.012867    0.021583   -0.987592   -1.350211    3.093866    0.887372    1.187232   -1.286924    1.033030   -0.210652    0.043518    0.507771    0.195931   -0.028773   -0.028100    0.911678    0.363630     0.52308    0.850099   -0.258323    0.950287    0.557336    0.746581   -0.761319   -2.340760   -0.216795   -0.852151    0.775552   -1.012093   -0.487226   -0.120186    0.455014    1.002764   -0.066891   -2.357531    1.252302    0.154025    1.196515   -0.119037   -1.207760     1.97936   -0.969236 -0.554801 -0.567838 -2.009398 0.035101 -1.093452 -0.454145 0.947029 20.907189 21.205207     -1.360777  0.181353      -2.257066   0.081537      10.08 0.475149      -0.291495 -0.646985          -1.163384             0.272108  0.346791                         0                    0                             0                               0                    0\n",
            "2007-02-06 22.975679 23.011444 22.711016 22.889843  20.377382 3516110.0      A  0.006936  -1.915723  -0.162254  -0.560152  -0.418691  -1.111428   0.907078  -0.527060  -0.883009  -0.092266   -0.122859    1.115720   -0.834329   -1.804219   -0.336530   -0.658734   -0.734745   -1.083497   -0.076312   -0.450011   -0.012867    0.021583   -0.987592   -1.350211    3.093866    0.887372    1.187232   -1.286924    1.033030   -0.210652    0.043518    0.507771    0.195931   -0.028773   -0.028100    0.911678     0.36363    0.523080    0.850099   -0.258323    0.950287    0.557336    0.746581   -0.761319   -2.340760   -0.216795   -0.852151    0.775552   -1.012093   -0.487226   -0.120186    0.455014    1.002764   -0.066891   -2.357531    1.252302    0.154025    1.196515   -0.119037    -1.20776    1.979360 -0.474666 -0.574843 -1.937246 0.105836 -0.970905 -0.380861 0.957120 20.835231 21.180881     -1.382566  0.131204      -2.191686   0.074245      10.55 0.397614      -0.297212 -0.646439          -1.155885             0.272108  0.346791                         0                    0                             0                               0                    0\n",
            "\n",
            "Preview regimes:\n",
            "      date regime_label_use  state_id_smoothed       p0       p1           p2        g\n",
            "2017-01-03          Risk-On                  0 0.999677 0.000323 8.475993e-09 0.999032\n",
            "2017-01-04          Risk-On                  0 0.999960 0.000040 8.460463e-12 0.999445\n",
            "------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Section 3.1.1–3.1.2 — Summary Helper & QC Utilities (no orchestrator yet)\n",
        "# Purpose:\n",
        "#   • log_3_1_1_2_summary(...) → human-readable printout of loads from 3.1.1–3.1.2\n",
        "#   • _exists_size(...)        → tiny file helper used by later summaries\n",
        "# Notes:\n",
        "#   This cell does NOT run the 3.1 orchestrator; that comes after 3.1.3+ are implemented.\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "from __future__ import annotations\n",
        "import json\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "# Reuse config/paths from earlier cells/files\n",
        "# HORIZONS, ROLL_LOOKBACK_D, SECTOR_NEUTRAL, N_SMOOTH_G, etc.\n",
        "# PANEL_MASTER_FP, TARGETS_FP, TARGETS_QC_FP, FEATURE_LIST_FP, LEAK_SCAN_FP\n",
        "# WINDEX_FP, WMANIFEST_FP, ALPHA_DIR, PANELS_DIR\n",
        "\n",
        "RUN_SUMMARY_FP = ALPHA_DIR / \"3_1_run_summary.json\"\n",
        "RUN_FINGERPRINT_FP = ALPHA_DIR / \"3_1_run_fingerprint.txt\"\n",
        "\n",
        "def log_3_1_1_2_summary(feats, px, cs_cols, regimes_keep):\n",
        "    created_files = [\n",
        "        str(PANELS_DIR / \"panel_base_qc.csv\"),\n",
        "        str(ALPHA_DIR / \"regimes_qc.json\"),\n",
        "    ]\n",
        "    print(\"\\n[3.1.1 + 3.1.2] SUMMARY\")\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"Features loaded: {len(feats):,} rows, {len(cs_cols):,} features\")\n",
        "    print(f\"Price panel:     {len(px):,} rows ({px['ticker'].nunique()} tickers)\")\n",
        "    print(f\"Regimes loaded:  {len(regimes_keep):,} rows\")\n",
        "    print(f\"Feature cols:    {cs_cols[:5]}{'...' if len(cs_cols) > 5 else ''}\")\n",
        "    print(\"\\nArtifacts written:\")\n",
        "    for f in created_files:\n",
        "        print(f\"  ✔ {f} {'(exists)' if Path(f).exists() else '(missing!)'}\")\n",
        "\n",
        "    # Optional: quick preview\n",
        "    print(\"\\nPreview feats:\")\n",
        "    print(feats.head(2).to_string(index=False))\n",
        "    print(\"\\nPreview regimes:\")\n",
        "    print(regimes_keep.head(2).to_string(index=False))\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "def _exists_size(path: Path) -> dict:\n",
        "    return {\"exists\": path.exists(), \"size\": (path.stat().st_size if path.exists() else 0), \"path\": str(path)}\n",
        "\n",
        "# ── 3.1.1–3.1.2 smoke summary (prints shapes + created files) ──\n",
        "RUN_3_1_12_SMOKE = True  # set False to silence\n",
        "\n",
        "if RUN_3_1_12_SMOKE:\n",
        "    try:\n",
        "        feats, px, cs_cols, regimes_keep = prep_3_1_3_context()\n",
        "        log_3_1_1_2_summary(feats, px, cs_cols, regimes_keep)\n",
        "\n",
        "        # Extra: show that the two QC artifacts exist and their sizes\n",
        "        files = [\n",
        "            PANELS_DIR / \"panel_base_qc.csv\",\n",
        "            ALPHA_DIR / \"regimes_qc.json\",\n",
        "        ]\n",
        "        print(\"\\n[Artifacts check]\")\n",
        "        for f in files:\n",
        "            es = _exists_size(f)\n",
        "            mark = \"✓\" if es[\"exists\"] else \"✗\"\n",
        "            size = f\"{es['size']:,}B\" if es[\"size\"] else \"-\"\n",
        "            print(f\"  {mark} {es['path']} ({size})\")\n",
        "    except Exception as e:\n",
        "        print(\"[3.1.1–3.1.2 smoke] failed:\", repr(e))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jg_LvTJGbTUb",
        "outputId": "e43a7a62-264d-46c6-d8ea-6cc8fc232fdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[3.1.1 + 3.1.2] SUMMARY\n",
            "------------------------------------------------------------\n",
            "Features loaded: 2,174,912 rows, 86 features\n",
            "Price panel:     2,174,912 rows (503 tickers)\n",
            "Regimes loaded:  2,163 rows\n",
            "Feature cols:    ['ret_1d', 'ret_lag_1', 'ret_lag_2', 'ret_lag_3', 'ret_lag_4']...\n",
            "\n",
            "Artifacts written:\n",
            "  ✔ artifacts/alpha/panels/panel_base_qc.csv (exists)\n",
            "  ✔ artifacts/alpha/regimes_qc.json (exists)\n",
            "\n",
            "Preview feats:\n",
            "      date      open      high       low     close  adj_close    volume ticker    ret_1d  ret_lag_1  ret_lag_2  ret_lag_3  ret_lag_4  ret_lag_5  ret_lag_6  ret_lag_7  ret_lag_8  ret_lag_9  ret_lag_10  ret_lag_11  ret_lag_12  ret_lag_13  ret_lag_14  ret_lag_15  ret_lag_16  ret_lag_17  ret_lag_18  ret_lag_19  ret_lag_20  ret_lag_21  ret_lag_22  ret_lag_23  ret_lag_24  ret_lag_25  ret_lag_26  ret_lag_27  ret_lag_28  ret_lag_29  ret_lag_30  ret_lag_31  ret_lag_32  ret_lag_33  ret_lag_34  ret_lag_35  ret_lag_36  ret_lag_37  ret_lag_38  ret_lag_39  ret_lag_40  ret_lag_41  ret_lag_42  ret_lag_43  ret_lag_44  ret_lag_45  ret_lag_46  ret_lag_47  ret_lag_48  ret_lag_49  ret_lag_50  ret_lag_51  ret_lag_52  ret_lag_53  ret_lag_54  ret_lag_55  ret_lag_56  ret_lag_57  ret_lag_58  ret_lag_59  ret_lag_60     rv_20    atr_14    mom_20   mom_6m   mom_12m  mom_12_1  mom_6_1    sma_20    sma_50  sma_20_gt_50  slope_20  mom_20_vs_vol  spy_rv_20  vix_close  breadth  book_to_price  cf_yield  shareholder_yield  gross_profitability  leverage  book_to_price_is_missing  cf_yield_is_missing  shareholder_yield_is_missing  gross_profitability_is_missing  leverage_is_missing\n",
            "2007-02-05 22.632332 22.804007 22.346209 22.768240  20.269119 3511916.0      A -0.018182  -0.162254  -0.560152  -0.418691  -1.111428   0.907078  -0.527060  -0.883009  -0.092266  -0.122859    1.115720   -0.834329   -1.804219   -0.336530   -0.658734   -0.734745   -1.083497   -0.076312   -0.450011   -0.012867    0.021583   -0.987592   -1.350211    3.093866    0.887372    1.187232   -1.286924    1.033030   -0.210652    0.043518    0.507771    0.195931   -0.028773   -0.028100    0.911678    0.363630     0.52308    0.850099   -0.258323    0.950287    0.557336    0.746581   -0.761319   -2.340760   -0.216795   -0.852151    0.775552   -1.012093   -0.487226   -0.120186    0.455014    1.002764   -0.066891   -2.357531    1.252302    0.154025    1.196515   -0.119037   -1.207760     1.97936   -0.969236 -0.554801 -0.567838 -2.009398 0.035101 -1.093452 -0.454145 0.947029 20.907189 21.205207     -1.360777  0.181353      -2.257066   0.081537      10.08 0.475149      -0.291495 -0.646985          -1.163384             0.272108  0.346791                         0                    0                             0                               0                    0\n",
            "2007-02-06 22.975679 23.011444 22.711016 22.889843  20.377382 3516110.0      A  0.006936  -1.915723  -0.162254  -0.560152  -0.418691  -1.111428   0.907078  -0.527060  -0.883009  -0.092266   -0.122859    1.115720   -0.834329   -1.804219   -0.336530   -0.658734   -0.734745   -1.083497   -0.076312   -0.450011   -0.012867    0.021583   -0.987592   -1.350211    3.093866    0.887372    1.187232   -1.286924    1.033030   -0.210652    0.043518    0.507771    0.195931   -0.028773   -0.028100    0.911678     0.36363    0.523080    0.850099   -0.258323    0.950287    0.557336    0.746581   -0.761319   -2.340760   -0.216795   -0.852151    0.775552   -1.012093   -0.487226   -0.120186    0.455014    1.002764   -0.066891   -2.357531    1.252302    0.154025    1.196515   -0.119037    -1.20776    1.979360 -0.474666 -0.574843 -1.937246 0.105836 -0.970905 -0.380861 0.957120 20.835231 21.180881     -1.382566  0.131204      -2.191686   0.074245      10.55 0.397614      -0.297212 -0.646439          -1.155885             0.272108  0.346791                         0                    0                             0                               0                    0\n",
            "\n",
            "Preview regimes:\n",
            "      date regime_label_use  state_id_smoothed       p0       p1           p2        g\n",
            "2017-01-03          Risk-On                  0 0.999677 0.000323 8.475993e-09 0.999032\n",
            "2017-01-04          Risk-On                  0 0.999960 0.000040 8.460463e-12 0.999445\n",
            "------------------------------------------------------------\n",
            "\n",
            "[Artifacts check]\n",
            "  ✓ artifacts/alpha/panels/panel_base_qc.csv (95B)\n",
            "  ✓ artifacts/alpha/regimes_qc.json (265B)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Section 3.1.3 + 3.1.4 — Daily Returns & Excess-Return Model (SPY-only / Sector-Neutral)\n",
        "#\n",
        "# What this provides:\n",
        "#   3.1.3\n",
        "#     - compute_daily_returns(px): per-ticker log returns from adj_close\n",
        "#     - extract_spy_returns(px_ret): SPY daily log returns\n",
        "#     - prepare_sector_returns(px_ret, sector_map): long-form sector ETF returns (optional)\n",
        "#\n",
        "#   3.1.4\n",
        "#     - rolling two-factor beta helper (fast, covariance-based)\n",
        "#     - compute_excess_returns(...): residual daily returns vs SPY (default) or vs SPY+sector (if enabled)\n",
        "#     - coverage/QC summary for sector betas when sector-neutral is used\n",
        "#\n",
        "# Reuse:\n",
        "#   - Assumes you already have from 3.1.1/3.1.2:\n",
        "#       feats, px, cs_cols = load_base_data(...)\n",
        "#       regimes_keep       = load_regime_context(...)\n",
        "#\n",
        "# #TOCHANGE knobs:\n",
        "#   - ROLL_LOOKBACK_D   = 126  (real run: 252)\n",
        "#   - SECTOR_NEUTRAL    = False (real run: True, plus sector_map.csv)\n",
        "#   - SECTOR_ETF_MAP_FP = Path(\"sector_map.csv\")  (provide/commit this mapping)\n",
        "#   - LAMBDA_RIDGE      = 0.0  (real run: 1e-6 if your 2x2 determinant gets unstable)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "from pathlib import Path\n",
        "from typing import Dict, Iterable, Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# ── Config (light for smoke; mark #TOCHANGE for real runs) ────────────────────\n",
        "ROLL_LOOKBACK_D = 126               # #TOCHANGE real run: 252\n",
        "SECTOR_NEUTRAL = False              # #TOCHANGE real run: True (requires sector_map.csv)\n",
        "SECTOR_ETF_MAP_FP = Path(\"sector_map.csv\")  # ticker,sector_etf mapping file (equities only)\n",
        "SPY_TICKER = \"SPY\"\n",
        "PX_FALLBACK_COL = \"adj_close\"\n",
        "LAMBDA_RIDGE = 0.0                  # #TOCHANGE real run: 1e-6 if needed for numerical stability\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 3.1.3 — Daily returns for assets and hedges (reuse data, new calc)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "# Returns\n",
        "def compute_daily_returns(px: pd.DataFrame, price_col: str = PX_FALLBACK_COL) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Per-ticker daily *log* returns from adjusted close.\n",
        "    Input:\n",
        "        px : ['date','ticker', price_col], unique per (date,ticker)\n",
        "    Output:\n",
        "        ['date','ticker', price_col, 'ret_1d'] (log returns t-1 -> t)\n",
        "    \"\"\"\n",
        "    req = {\"date\", \"ticker\", price_col}\n",
        "    if not req.issubset(px.columns):\n",
        "        missing = req - set(px.columns)\n",
        "        raise KeyError(f\"compute_daily_returns: missing columns {missing}\")\n",
        "    df = px.sort_values([\"ticker\", \"date\"]).copy()\n",
        "    df[\"ret_1d\"] = np.log(df[price_col]) - np.log(df.groupby(\"ticker\")[price_col].shift(1))\n",
        "    return df\n",
        "\n",
        "feats, px, cs_cols, regimes_keep = prep_3_1_3_context()\n",
        "px_ret  = compute_daily_returns(px)\n",
        "print(px_ret.head()[[\"date\",\"ticker\",\"ret_1d\"]])\n",
        "\n",
        "def extract_spy_returns(px_ret: pd.DataFrame) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Extract SPY ret_1d series indexed by date.\n",
        "    Input:\n",
        "        px_ret : output of compute_daily_returns\n",
        "    \"\"\"\n",
        "    if SPY_TICKER not in px_ret[\"ticker\"].unique():\n",
        "        raise RuntimeError(\n",
        "            f\"extract_spy_returns: '{SPY_TICKER}' not found in px; include SPY in features or \"\n",
        "            f\"load it from raw_prices.parquet before 3.1.3.\"\n",
        "        )\n",
        "    spy = (\n",
        "        px_ret.loc[px_ret[\"ticker\"].eq(SPY_TICKER), [\"date\", \"ret_1d\"]]\n",
        "        .drop_duplicates()\n",
        "        .sort_values(\"date\")\n",
        "        .set_index(\"date\")[\"ret_1d\"]\n",
        "    )\n",
        "    return spy\n",
        "\n",
        "\n",
        "def load_sector_map(fp: Path = SECTOR_ETF_MAP_FP) -> Optional[pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Load ticker->sector_etf mapping (equity rows only). File format:\n",
        "        ticker,sector_etf\n",
        "        AAPL,XLK\n",
        "        XOM,XLE\n",
        "        ...\n",
        "    Returns None if file does not exist.\n",
        "    \"\"\"\n",
        "    if not fp.exists():\n",
        "        return None\n",
        "    sm = pd.read_csv(fp, dtype={\"ticker\": str, \"sector_etf\": str})\n",
        "    # Keep clean, drop empty rows\n",
        "    sm = sm.dropna(subset=[\"ticker\", \"sector_etf\"]).copy()\n",
        "    sm[\"ticker\"] = sm[\"ticker\"].astype(str)\n",
        "    sm[\"sector_etf\"] = sm[\"sector_etf\"].astype(str)\n",
        "    return sm\n",
        "\n",
        "\n",
        "def prepare_sector_returns(\n",
        "    px_ret: pd.DataFrame,\n",
        "    sector_map: pd.DataFrame,\n",
        "    sector_etfs: Optional[Iterable[str]] = None,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Build long-form sector ETF daily returns from px_ret for the ETFs referenced by sector_map.\n",
        "    Input:\n",
        "        px_ret     : ['date','ticker','ret_1d'] for all tickers inc. hedges\n",
        "        sector_map : ['ticker','sector_etf'] for equities\n",
        "        sector_etfs: optional explicit list of ETFs to pull; else inferred from sector_map\n",
        "    Output:\n",
        "        DataFrame: ['date','sector_etf','sector_ret_1d'] (one row per ETF/date)\n",
        "    \"\"\"\n",
        "    if sector_etfs is None:\n",
        "        sector_etfs = sector_map[\"sector_etf\"].dropna().unique().tolist()\n",
        "    etf_ret = (\n",
        "        px_ret[px_ret[\"ticker\"].isin(set(sector_etfs))][[\"date\", \"ticker\", \"ret_1d\"]]\n",
        "        .rename(columns={\"ticker\": \"sector_etf\", \"ret_1d\": \"sector_ret_1d\"})\n",
        "        .sort_values([\"sector_etf\", \"date\"])\n",
        "    )\n",
        "    return etf_ret\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 3.1.4 — Excess-return model (SPY-only / Sector-Neutral)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "def _rolling_two_factor_betas(\n",
        "    y: pd.Series, x1: pd.Series, x2: pd.Series, lookback: int, lambda_ridge: float = LAMBDA_RIDGE\n",
        ") -> Tuple[pd.Series, pd.Series]:\n",
        "    \"\"\"\n",
        "    Rolling OLS betas for y ~ b1*x1 + b2*x2 using 2x2 normal equations estimated by\n",
        "    rolling covariances. Returns (b1,b2). Stable and fast.\n",
        "    \"\"\"\n",
        "    # rolling means (min_periods ~ half window helps early segments)\n",
        "    m_y  = y.rolling(lookback, min_periods=lookback//2).mean()\n",
        "    m_x1 = x1.rolling(lookback, min_periods=lookback//2).mean()\n",
        "    m_x2 = x2.rolling(lookback, min_periods=lookback//2).mean()\n",
        "\n",
        "    # centered\n",
        "    yc  = y  - m_y\n",
        "    x1c = x1 - m_x1\n",
        "    x2c = x2 - m_x2\n",
        "\n",
        "    # covariances/variances\n",
        "    cov_yx1  = (yc * x1c).rolling(lookback, min_periods=lookback//2).mean()\n",
        "    cov_yx2  = (yc * x2c).rolling(lookback, min_periods=lookback//2).mean()\n",
        "    cov_x1x2 = (x1c * x2c).rolling(lookback, min_periods=lookback//2).mean()\n",
        "    var_x1   = (x1c * x1c).rolling(lookback, min_periods=lookback//2).mean()\n",
        "    var_x2   = (x2c * x2c).rolling(lookback, min_periods=lookback//2).mean()\n",
        "\n",
        "    # Solve:\n",
        "    # [var_x1+λ   cov_x1x2] [b1] = [cov_yx1]\n",
        "    # [cov_x1x2   var_x2+λ] [b2]   [cov_yx2]\n",
        "    var_x1_r = var_x1 + lambda_ridge\n",
        "    var_x2_r = var_x2 + lambda_ridge\n",
        "    det = var_x1_r * var_x2_r - cov_x1x2 * cov_x1x2\n",
        "    eps = 1e-12\n",
        "    det = det.where(det.abs() > eps, np.nan)\n",
        "\n",
        "    b1 = ( var_x2_r * cov_yx1 - cov_x1x2 * cov_yx2) / det\n",
        "    b2 = (-cov_x1x2 * cov_yx1 + var_x1_r   * cov_yx2) / det\n",
        "    return b1, b2\n",
        "\n",
        "    def _per_ticker(g: pd.DataFrame) -> pd.DataFrame:\n",
        "        # choose 2-factor if we have sector_ret_1d and enough non-NaNs\n",
        "        if have_sector and (\"sector_ret_1d\" in g.columns) and (g[\"sector_ret_1d\"].notna().sum() >= lookback // 2):\n",
        "            b1, b2 = _rolling_two_factor_betas(\n",
        "                y=g[\"ret_1d\"], x1=g[\"spy_ret_1d\"], x2=g[\"sector_ret_1d\"],\n",
        "                lookback=lookback, lambda_ridge=lambda_ridge\n",
        "            )\n",
        "            g[\"beta_spy\"]    = b1\n",
        "            g[\"beta_sector\"] = b2\n",
        "            g[\"resid_1d\"]    = g[\"ret_1d\"] - (g[\"beta_spy\"]*g[\"spy_ret_1d\"] + g[\"beta_sector\"]*g[\"sector_ret_1d\"])\n",
        "        else:\n",
        "            # fallback: SPY-only\n",
        "            cov = g[\"ret_1d\"].rolling(lookback, min_periods=lookback//2).cov(g[\"spy_ret_1d\"])\n",
        "            var = g[\"spy_ret_1d\"].rolling(lookback, min_periods=lookback//2).var()\n",
        "            beta = cov / var.replace(0, np.nan)\n",
        "            g[\"beta_spy\"]    = beta\n",
        "            g[\"beta_sector\"] = np.nan\n",
        "            g[\"resid_1d\"]    = g[\"ret_1d\"] - g[\"beta_spy\"] * g[\"spy_ret_1d\"]\n",
        "        return g\n",
        "\n",
        "    out = df.groupby(\"ticker\", group_keys=False).apply(_per_ticker)\n",
        "\n",
        "    # Coverage metric: fraction of rows with valid sector beta\n",
        "    if have_sector:\n",
        "        sector_cov_frac = float(out[\"beta_sector\"].notna().mean())\n",
        "\n",
        "    qc = {\n",
        "        \"lookback\": lookback,\n",
        "        \"sector_neutral_enabled\": bool(have_sector),\n",
        "        \"sector_beta_coverage\": sector_cov_frac,\n",
        "        \"lambda_ridge\": lambda_ridge,\n",
        "    }\n",
        "    return out[[\"date\", \"ticker\", \"resid_1d\", \"beta_spy\", \"beta_sector\"]], qc\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Example usage inside your 3.1 pipeline:\n",
        "#\n",
        "# feats, px, cs_cols, regimes_keep = prep_3_1_3_context(...)\n",
        "# px_ret  = compute_daily_returns(px)\n",
        "# spy_ret = extract_spy_returns(px_ret)\n",
        "# sector_map = load_sector_map(SECTOR_ETF_MAP_FP) if SECTOR_NEUTRAL else None\n",
        "# sector_rets = prepare_sector_returns(px_ret, sector_map) if (SECTOR_NEUTRAL and sector_map is not None) else None\n",
        "# resid_df, qc = compute_excess_returns(\n",
        "#     px_ret=px_ret,\n",
        "#     spy_ret=spy_ret,\n",
        "#     lookback=ROLL_LOOKBACK_D,\n",
        "#     sector_neutral=SECTOR_NEUTRAL,\n",
        "#     sector_map=sector_map,\n",
        "#     sector_returns=sector_rets,\n",
        "#     lambda_ridge=LAMBDA_RIDGE,\n",
        "# )\n",
        "#\n",
        "# Next (3.1.5): roll forward residual sums to build r_ex_h targets; compute ranks & valid_mask;\n",
        "# then 3.1.6 merge feats × targets × regimes and split per walk-forward window.\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "# smoke test\n",
        "# 3.1.3 smoke summary + optional artifacts\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "px_ret = compute_daily_returns(px)\n",
        "\n",
        "n_rows = len(px_ret)\n",
        "n_tickers = px_ret[\"ticker\"].nunique()\n",
        "date_min = px_ret[\"date\"].min()\n",
        "date_max = px_ret[\"date\"].max()\n",
        "nan_frac = float(px_ret[\"ret_1d\"].isna().mean())\n",
        "\n",
        "has_spy = (SPY_TICKER in px_ret[\"ticker\"].unique())\n",
        "first_nan_by_ticker = int(px_ret.sort_values([\"ticker\",\"date\"])\n",
        "                          .groupby(\"ticker\")[\"ret_1d\"].first().isna().sum())\n",
        "\n",
        "print(\"\\n[3.1.3] RETURNS SUMMARY\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"Rows={n_rows:,}  Tickers={n_tickers}  Dates=[{date_min} → {date_max}]\")\n",
        "print(f\"ret_1d NaN fraction: {nan_frac:.4f} (should be ~1/avg_days_per_ticker for first obs)\")\n",
        "print(f\"SPY present: {has_spy}  | tickers with first ret_1d = NaN: {first_nan_by_ticker}\")\n",
        "\n",
        "# Optional: write tiny artifacts so we can eyeball later\n",
        "(ALPHA_DIR / \"alpha\").mkdir(parents=True, exist_ok=True)  # safe if already exists\n",
        "qc = {\n",
        "    \"rows\": n_rows,\n",
        "    \"tickers\": n_tickers,\n",
        "    \"date_min\": str(date_min),\n",
        "    \"date_max\": str(date_max),\n",
        "    \"nan_frac\": nan_frac,\n",
        "    \"spy_present\": has_spy,\n",
        "    \"first_nan_tickers\": first_nan_by_ticker,\n",
        "}\n",
        "(ALPHA_DIR / \"px_returns_qc.json\").write_text(json.dumps(qc, indent=2))\n",
        "px_ret.head(200).to_csv(ALPHA_DIR / \"px_returns_head.csv\", index=False)\n",
        "\n",
        "print(\"\\nArtifacts:\")\n",
        "print(f\"  ✓ {ALPHA_DIR / 'px_returns_qc.json'}\")\n",
        "print(f\"  ✓ {ALPHA_DIR / 'px_returns_head.csv'}\")\n",
        "\n",
        "# quick consistency check with Section-1 ret_1d on overlapping rows\n",
        "chk = (px_ret.merge(feats[[\"date\",\"ticker\",\"ret_1d\"]], on=[\"date\",\"ticker\"], how=\"inner\", suffixes=(\"_new\",\"_s1\"))\n",
        "              .dropna(subset=[\"ret_1d_new\",\"ret_1d_s1\"]))\n",
        "corr = chk[\"ret_1d_new\"].corr(chk[\"ret_1d_s1\"])\n",
        "print(f\"[check] corr(ret_1d_new, ret_1d_s1) = {corr:.6f}\")"
      ],
      "metadata": {
        "id": "Zj4JO4PTk9oB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3844c60e-4c27-4fac-f6e6-3149a5f1f8de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        date ticker    ret_1d\n",
            "0 2007-02-05      A       NaN\n",
            "1 2007-02-06      A  0.005327\n",
            "2 2007-02-07      A  0.017656\n",
            "3 2007-02-08      A  0.003066\n",
            "4 2007-02-09      A -0.000306\n",
            "\n",
            "[3.1.3] RETURNS SUMMARY\n",
            "------------------------------------------------------------\n",
            "Rows=2,174,912  Tickers=503  Dates=[2007-02-05 00:00:00 → 2025-08-11 00:00:00]\n",
            "ret_1d NaN fraction: 0.0002 (should be ~1/avg_days_per_ticker for first obs)\n",
            "SPY present: False  | tickers with first ret_1d = NaN: 0\n",
            "\n",
            "Artifacts:\n",
            "  ✓ artifacts/alpha/px_returns_qc.json\n",
            "  ✓ artifacts/alpha/px_returns_head.csv\n",
            "[check] corr(ret_1d_new, ret_1d_s1) = -0.034104\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Section 3.1.5 + 3.1.6 — Forward Targets (no leakage) & Unified Modeling Panel\n",
        "#\n",
        "# What this provides:\n",
        "#   3.1.5\n",
        "#     - forward_excess_targets(resid_df, horizons): builds r_ex_h from resid_1d (t+1..t+h),\n",
        "#       cross-sectional ranks per date, and a valid_mask. Writes targets.parquet + targets_qc.json.\n",
        "#\n",
        "#   3.1.6\n",
        "#     - build_unified_model_panel(feats, targets, regimes_keep, cs_cols): merges X×Y×R into a\n",
        "#       single modeling panel (no split yet), writes feature_list.json and panel_master.parquet.\n",
        "#     - assert_leakage_free(panel, horizons): essential schema/leakage checks, writes leakage_scan.json.\n",
        "#\n",
        "# Reuse from earlier steps:\n",
        "#   - feats, px, cs_cols = load_base_data(...)\n",
        "#   - regimes_keep       = load_regime_context(...)\n",
        "#   - px_ret, spy_ret, sector_map, sector_returns\n",
        "#   - resid_df, qc_excess = compute_excess_returns(...)\n",
        "#\n",
        "# #TOCHANGE knobs:\n",
        "#   - HORIZONS        = [5, 10]     (real run: add 20 → [5,10,20])\n",
        "#   - RANK_METHOD     = \"average\"   (ok; or \"dense\")\n",
        "#   - RANK_CENTER     = False       (real run: consider centering to [-0.5, +0.5] for rank-loss)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Reuse shared dirs from earlier files\n",
        "ARTIFACTS_DIR = Path(\"artifacts\")\n",
        "ALPHA_DIR = ARTIFACTS_DIR / \"alpha\"\n",
        "PANELS_DIR = ALPHA_DIR / \"panels\"\n",
        "REGIME_DIR = ARTIFACTS_DIR / \"regimes\"\n",
        "\n",
        "ALPHA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "PANELS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ── Config (mark #TOCHANGE for real runs) ─────────────────────────────────────\n",
        "HORIZONS: List[int] = [5, 10]   # #TOCHANGE real run: [5, 10, 20]\n",
        "RANK_METHOD = \"average\"         # \"average\"|\"min\"|\"max\"|\"first\"|\"dense\"\n",
        "RANK_CENTER = False             # #TOCHANGE real run: True to map pct→(pct-0.5)\n",
        "\n",
        "TARGETS_FP       = ALPHA_DIR / \"targets.parquet\"\n",
        "TARGETS_QC_FP    = ALPHA_DIR / \"targets_qc.json\"\n",
        "FEATURE_LIST_FP  = ALPHA_DIR / \"feature_list.json\"\n",
        "LEAK_SCAN_FP     = ALPHA_DIR / \"leakage_scan.json\"\n",
        "PANEL_MASTER_FP  = PANELS_DIR / \"panel_master.parquet\"  # unified (pre-split) panel\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 3.1.5 — Forward targets (no leakage)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "def _future_sum(series: pd.Series, h: int) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Sum of next h observations, i.e., Σ_{t+1..t+h} s.\n",
        "    Implemented as: shift(-1).rolling(h).sum()\n",
        "    \"\"\"\n",
        "    return series.shift(-1).rolling(window=h, min_periods=h).sum()\n",
        "\n",
        "\n",
        "def forward_excess_targets(\n",
        "    resid_df: pd.DataFrame,\n",
        "    horizons: List[int] = HORIZONS,\n",
        "    write_artifacts: bool = True,\n",
        "    extra_qc: Dict | None = None,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Build forward targets from residual daily returns (resid_1d).\n",
        "\n",
        "    Inputs:\n",
        "        resid_df: ['date','ticker','resid_1d', 'beta_spy', 'beta_sector'(optional)]\n",
        "    Outputs:\n",
        "        targets_df with columns:\n",
        "          - 'date','ticker'\n",
        "          - r_ex_<h> for each h in horizons\n",
        "          - y<h>_rank  (cross-sectional percentile per date, in [0,1])\n",
        "          - valid_mask (True iff all r_ex_* present)\n",
        "    Artifacts:\n",
        "        targets.parquet, targets_qc.json\n",
        "    \"\"\"\n",
        "    req = {\"date\", \"ticker\", \"resid_1d\"}\n",
        "    if not req.issubset(resid_df.columns):\n",
        "        raise KeyError(f\"forward_excess_targets: missing columns {req - set(resid_df.columns)}\")\n",
        "\n",
        "    df = resid_df[[\"date\", \"ticker\", \"resid_1d\"]].sort_values([\"ticker\", \"date\"]).copy()\n",
        "\n",
        "    # Per-ticker future sums for each horizon\n",
        "    out = df[[\"date\", \"ticker\"]].copy()\n",
        "    for h in horizons:\n",
        "        col = f\"r_ex_{h}\"\n",
        "        out[col] = (\n",
        "            df.groupby(\"ticker\", group_keys=False)[\"resid_1d\"]\n",
        "              .apply(lambda s, _h=h: _future_sum(s, _h))\n",
        "        )\n",
        "\n",
        "    # Cross-sectional ranks per date\n",
        "    for h in horizons:\n",
        "        rx = f\"r_ex_{h}\"\n",
        "        rk = f\"y{h}_rank\"\n",
        "        # rank in [0,1]; optionally center to [-0.5,+0.5]\n",
        "        pct = out.groupby(\"date\")[rx].rank(pct=True, method=RANK_METHOD)\n",
        "        out[rk] = pct - 0.5 if RANK_CENTER else pct\n",
        "\n",
        "    # Valid mask: rows where all r_ex_* are present\n",
        "    r_cols = [f\"r_ex_{h}\" for h in horizons]\n",
        "    out[\"valid_mask\"] = out[r_cols].notna().all(axis=1)\n",
        "\n",
        "    # Optional QC + artifacts\n",
        "    if write_artifacts:\n",
        "        qc = {\n",
        "            \"rows\": int(len(out)),\n",
        "            \"dates\": int(out[\"date\"].nunique()),\n",
        "            \"tickers\": int(out[\"ticker\"].nunique()),\n",
        "            \"horizons\": list(horizons),\n",
        "            \"valid_frac\": float(out[\"valid_mask\"].mean()),\n",
        "        }\n",
        "        if extra_qc:\n",
        "            qc.update(extra_qc)\n",
        "\n",
        "        out.sort_values([\"date\", \"ticker\"]).to_parquet(TARGETS_FP, index=False)\n",
        "        TARGETS_QC_FP.write_text(json.dumps(qc, indent=2))\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 3.1.6 — Merge unified modeling panel (X × Y × R)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "def build_unified_model_panel(\n",
        "    feats: pd.DataFrame,\n",
        "    targets: pd.DataFrame,\n",
        "    regimes_keep: pd.DataFrame,\n",
        "    cs_cols: List[str],\n",
        "    write_artifacts: bool = True,\n",
        ") -> Tuple[pd.DataFrame, Dict]:\n",
        "    \"\"\"\n",
        "    Merge:\n",
        "      X (features from §1) × Y (targets from 3.1.5) × R (regimes from §2)\n",
        "    and produce a single modeling panel. No re-standardization is done\n",
        "    (features are already CS-standardized in §1).\n",
        "\n",
        "    Inputs:\n",
        "        feats        : features_filtered subset with ['date','ticker', <cs_cols>, 'adj_close', ...]\n",
        "        targets      : output of forward_excess_targets\n",
        "        regimes_keep : ['date','regime_label_use', 'p*'(opt), 'g'(opt)]\n",
        "        cs_cols      : feature columns to keep as X\n",
        "\n",
        "    Outputs:\n",
        "        panel        : unified panel with X, Y, R (pre-split)\n",
        "        meta         : dict with X_cols, Y_cols, R_cols for feature_list.json\n",
        "\n",
        "    Artifacts:\n",
        "        feature_list.json, panel_master.parquet, leakage_scan.json (via assert)\n",
        "    \"\"\"\n",
        "    # Keep only the columns we truly need from feats: identifiers + X\n",
        "    keep_X = [\"date\", \"ticker\"] + list(cs_cols)\n",
        "    missing_in_feats = set(keep_X) - set(feats.columns)\n",
        "    if missing_in_feats:\n",
        "        raise KeyError(f\"build_unified_model_panel: missing feature columns: {missing_in_feats}\")\n",
        "\n",
        "    X_df = feats[keep_X].copy()\n",
        "\n",
        "    # Targets\n",
        "    req_tgt = {\"date\", \"ticker\"}\n",
        "    if not req_tgt.issubset(targets.columns):\n",
        "        raise KeyError(f\"build_unified_model_panel: targets missing {req_tgt - set(targets.columns)}\")\n",
        "    Y_cols = [c for c in targets.columns if c.startswith(\"r_ex_\")] + \\\n",
        "             [c for c in targets.columns if c.startswith(\"y\") and c.endswith(\"_rank\")] + [\"valid_mask\"]\n",
        "    Y_df = targets[[\"date\", \"ticker\"] + Y_cols].copy()\n",
        "\n",
        "    # Regimes (per-date)\n",
        "    req_rg = {\"date\", \"regime_label_use\"}\n",
        "    if not req_rg.issubset(regimes_keep.columns):\n",
        "        raise KeyError(f\"build_unified_model_panel: regimes missing {req_rg - set(regimes_keep.columns)}\")\n",
        "    R_cols_extra = [c for c in regimes_keep.columns if c.startswith(\"p\")] + \\\n",
        "                   ([\"g\"] if \"g\" in regimes_keep.columns else [])\n",
        "    R_df = regimes_keep[[\"date\", \"regime_label_use\"] + R_cols_extra].copy()\n",
        "\n",
        "    # Merge: ((X ⨝ Y) ⨝ R_on_date)\n",
        "    panel = (\n",
        "        X_df.merge(Y_df, on=[\"date\", \"ticker\"], how=\"inner\")\n",
        "            .merge(R_df, on=\"date\", how=\"left\")\n",
        "            .sort_values([\"date\", \"ticker\"])\n",
        "            .reset_index(drop=True)\n",
        "    )\n",
        "\n",
        "    # Column groups\n",
        "    X_cols = cs_cols\n",
        "    R_cols = [\"regime_label_use\"] + R_cols_extra\n",
        "\n",
        "    meta = {\n",
        "        \"X_cols\": X_cols,\n",
        "        \"Y_cols\": Y_cols,\n",
        "        \"R_cols\": R_cols,\n",
        "        \"notes\": \"CS features from §1 (already winsorized & CS-z-scored); regimes from §2; targets are forward sums of residual daily returns.\",\n",
        "    }\n",
        "\n",
        "    # Leakage/safety checks\n",
        "    scan = assert_leakage_free(panel, horizons=[int(c.split(\"_\")[-1]) for c in Y_cols if c.startswith(\"r_ex_\")])\n",
        "\n",
        "    # Write artifacts\n",
        "    if write_artifacts:\n",
        "        FEATURE_LIST_FP.write_text(json.dumps(meta, indent=2))\n",
        "        panel.to_parquet(PANEL_MASTER_FP, index=False)\n",
        "        LEAK_SCAN_FP.write_text(json.dumps(scan, indent=2))\n",
        "\n",
        "    return panel, meta\n",
        "\n",
        "\n",
        "def assert_leakage_free(panel: pd.DataFrame, horizons: List[int]) -> Dict:\n",
        "    \"\"\"\n",
        "    Minimal-yet-critical validations for §3.1:\n",
        "      • Unique (date,ticker)\n",
        "      • No target defined on last max(h) days per ticker (should be NaN→valid_mask=False)\n",
        "      • No forward fill (spot-check last obs per ticker)\n",
        "      • Basic NA/coverage summaries\n",
        "    Returns a JSON-serializable dict; also used to write leakage_scan.json upstream.\n",
        "    \"\"\"\n",
        "    res: Dict = {\"status\": \"PASS\", \"notes\": []}\n",
        "\n",
        "    # Unique keys\n",
        "    if panel.duplicated([\"date\", \"ticker\"]).any():\n",
        "        n_dup = int(panel.duplicated([\"date\", \"ticker\"]).sum())\n",
        "        res[\"status\"] = \"FAIL\"\n",
        "        res[\"notes\"].append(f\"Duplicate (date,ticker) rows: {n_dup}\")\n",
        "\n",
        "    # Tail guard: on the last available date per ticker, targets must be NaN\n",
        "    r_cols = [c for c in panel.columns if c.startswith(\"r_ex_\")]\n",
        "    if r_cols:\n",
        "        last_rows = panel.groupby(\"ticker\", as_index=False).tail(1)\n",
        "        non_nan_last = {c: int(last_rows[c].notna().sum()) for c in r_cols}\n",
        "        # It’s OK if some last rows exist for shorter horizons; but conservative guard:\n",
        "        if any(v > 0 for v in non_nan_last.values()):\n",
        "            res[\"status\"] = \"WARN\"\n",
        "            res[\"notes\"].append(f\"Some last ticker rows have non-NaN targets (check rolling logic): {non_nan_last}\")\n",
        "\n",
        "    # Spot-check: last date overall should have NaNs in all r_ex_* (train tail)\n",
        "    if r_cols:\n",
        "        last_date = panel[\"date\"].max()\n",
        "        last_slice = panel.loc[panel[\"date\"].eq(last_date), r_cols]\n",
        "        if last_slice.notna().any().any():\n",
        "            res[\"status\"] = \"WARN\"\n",
        "            res[\"notes\"].append(\"Latest calendar date contains non-NaN targets; confirm tail exclusion in §3.1.7 split.\")\n",
        "\n",
        "    # NA/coverage\n",
        "    cov = {\n",
        "        \"rows\": int(len(panel)),\n",
        "        \"dates\": int(panel[\"date\"].nunique()),\n",
        "        \"tickers\": int(panel[\"ticker\"].nunique()),\n",
        "        \"valid_frac\": float(panel[\"valid_mask\"].mean()) if \"valid_mask\" in panel.columns else None,\n",
        "        \"target_na_frac\": {c: float(panel[c].isna().mean()) for c in r_cols},\n",
        "    }\n",
        "    res[\"coverage\"] = cov\n",
        "    return res\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Example orchestration for 3.1.5 + 3.1.6 (call these from your section runner):\n",
        "#\n",
        "# 1) From 3.1.1/3.1.2:\n",
        "# feats, px, cs_cols, regimes_keep = prep_3_1_3_context(...)\n",
        "#\n",
        "# 2) From 3.1.3/3.1.4:\n",
        "# px_ret  = compute_daily_returns(px)\n",
        "# spy_ret = extract_spy_returns(px_ret)\n",
        "# sector_map  = load_sector_map(SECTOR_ETF_MAP_FP) if SECTOR_NEUTRAL else None\n",
        "# sector_rets = prepare_sector_returns(px_ret, sector_map) if (SECTOR_NEUTRAL and sector_map is not None) else None\n",
        "# resid_df, qc_excess = compute_excess_returns(\n",
        "#     px_ret=px_ret,\n",
        "#     spy_ret=spy_ret,\n",
        "#     lookback=ROLL_LOOKBACK_D,\n",
        "#     sector_neutral=SECTOR_NEUTRAL,\n",
        "#     sector_map=sector_map,\n",
        "#     sector_returns=sector_rets,\n",
        "# )\n",
        "#\n",
        "# 3) 3.1.5 — targets\n",
        "# targets = forward_excess_targets(resid_df, horizons=HORIZONS, write_artifacts=True, extra_qc=qc_excess)\n",
        "#\n",
        "# 4) 3.1.6 — unified modeling panel\n",
        "# panel, meta = build_unified_model_panel(feats, targets, regimes_keep, cs_cols, write_artifacts=True)\n",
        "#\n",
        "# Next (3.1.7): read windows_index.json / window_manifest.json to slice `panel_master.parquet`\n",
        "# into `panel_train_<winid>.parquet` and `panel_test_<winid>.parquet`, plus per-window QC.\n",
        "# ──────────────────────────────────────────────────────────────────────────────"
      ],
      "metadata": {
        "id": "tzkZzesR_57y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Section 3.1.7 (+ optional 3.1.8) — Walk-Forward Split & Minimal Validation\n",
        "#\n",
        "# What this provides:\n",
        "#   • load_windows_plan(): load windows_index.json (preferred) or window_manifest.json (fallback)\n",
        "#   • split_panel_by_windows(): slice panel_master → per-window train/test parquet files\n",
        "#       - Applies defensive tail drop: remove rows after (last_train_date − max(HORIZONS))\n",
        "#       - Writes per-window QC CSVs (row/date/ticker counts, NA rates)\n",
        "#   • (optional) build_3_1_validation_report(): roll up 3.1 checks into one JSON\n",
        "#\n",
        "# Reuse:\n",
        "#   - PANEL_MASTER_FP from 3.1.6 (unified X×Y×R panel)\n",
        "#   - HORIZONS from 3.1.5 (to compute defensive tail)\n",
        "#   - windows_index.json / window_manifest.json from Section 2.8\n",
        "#\n",
        "# #TOCHANGE knobs:\n",
        "#   - HORIZONS        = [5, 10] (real run: add 20)\n",
        "#   - FORCE_STITCHED_SPLIT = False (leave False; for diagnostics you can set True to re-stitch strict OOS slices)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "ARTIFACTS_DIR = Path(\"artifacts\")\n",
        "REGIME_DIR    = ARTIFACTS_DIR / \"regimes\"\n",
        "ALPHA_DIR     = ARTIFACTS_DIR / \"alpha\"\n",
        "PANELS_DIR    = ALPHA_DIR / \"panels\"\n",
        "\n",
        "PANELS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# From 3.1.5/3.1.6\n",
        "HORIZONS: List[int] = [5, 10]  # #TOCHANGE real run: [5,10,20]\n",
        "PANEL_MASTER_FP  = PANELS_DIR / \"panel_master.parquet\"\n",
        "FEATURE_LIST_FP  = ALPHA_DIR / \"feature_list.json\"\n",
        "TARGETS_FP       = ALPHA_DIR / \"targets.parquet\"\n",
        "TARGETS_QC_FP    = ALPHA_DIR / \"targets_qc.json\"\n",
        "LEAK_SCAN_FP     = ALPHA_DIR / \"leakage_scan.json\"\n",
        "\n",
        "# WF plan inputs (from Section 2.8)\n",
        "WINDEX_FP    = REGIME_DIR / \"windowed\" / \"windows_index.json\"\n",
        "WMANIFEST_FP = REGIME_DIR / \"window_manifest.json\"  # fallback (single-window)\n",
        "\n",
        "# Output QC\n",
        "VAL_3_1_FP   = ALPHA_DIR / \"3_1_validation_report.json\"\n",
        "\n",
        "# Optional behavior\n",
        "FORCE_STITCHED_SPLIT = False  # #TOCHANGE: keep False. If True, enforces hard OOS no-overlap beyond provided plan.\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 3.1.7 — Walk-Forward split helpers\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "def load_windows_plan(\n",
        "    windex_fp: Path = WINDEX_FP,\n",
        "    wmanifest_fp: Path = WMANIFEST_FP\n",
        ") -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Load a walk-forward plan from:\n",
        "      1) windows_index.json (preferred, possibly multi-window)\n",
        "      2) window_manifest.json (fallback, single-window)\n",
        "    Returns a list of window dicts with keys:\n",
        "      win_id, train_start, train_end, test_start, test_end\n",
        "    \"\"\"\n",
        "    if windex_fp.exists():\n",
        "        data = json.loads(windex_fp.read_text())\n",
        "        # Expected schema (from §2.8): {\"windows\": [{...}, ...]}\n",
        "        windows = data.get(\"windows\", [])\n",
        "        if not windows:\n",
        "            raise ValueError(\"windows_index.json present but 'windows' empty.\")\n",
        "        # Normalize\n",
        "        outs = []\n",
        "        for i, w in enumerate(windows):\n",
        "            outs.append({\n",
        "                \"win_id\": w.get(\"win_id\", f\"W{i}\"),\n",
        "                \"train_start\": w[\"train_start\"],\n",
        "                \"train_end\":   w[\"train_end\"],\n",
        "                \"test_start\":  w[\"test_start\"],\n",
        "                \"test_end\":    w[\"test_end\"],\n",
        "            })\n",
        "        return outs\n",
        "\n",
        "    if wmanifest_fp.exists():\n",
        "        w = json.loads(wmanifest_fp.read_text()).get(\"window\", None)\n",
        "        if not w:\n",
        "            raise ValueError(\"window_manifest.json present but no 'window' key.\")\n",
        "        return [{\n",
        "            \"win_id\": \"W0\",\n",
        "            \"train_start\": w[\"train_start\"],\n",
        "            \"train_end\":   w[\"train_end\"],\n",
        "            \"test_start\":  w[\"test_start\"],\n",
        "            \"test_end\":    w[\"test_end\"],\n",
        "        }]\n",
        "\n",
        "    raise FileNotFoundError(\n",
        "        \"No windows_index.json or window_manifest.json found. \"\n",
        "        \"Run Section 2.8 to generate the walk-forward plan.\"\n",
        "    )\n",
        "\n",
        "\n",
        "def _defensive_tail_drop(df_train: pd.DataFrame, max_h: int) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Defensive tail: drop any training rows after (last_train_date − max_h business days).\n",
        "    Rationale: ensure no accidental leakage from partially formed forward targets.\n",
        "    \"\"\"\n",
        "    if df_train.empty:\n",
        "        return df_train\n",
        "    last_train_date = df_train[\"date\"].max()\n",
        "    # Use calendar days minus a buffer; fine for defense (targets themselves are already shifted)\n",
        "    cutoff = pd.to_datetime(last_train_date) - pd.tseries.offsets.BDay(max_h)\n",
        "    return df_train[df_train[\"date\"] <= cutoff]\n",
        "\n",
        "\n",
        "def _write_panel_qc(panel: pd.DataFrame, out_csv: Path, label: str) -> Dict:\n",
        "    \"\"\"\n",
        "    Emit basic QC: row counts, NA rates for targets, valid_mask share, unique keys, date/ticker counts.\n",
        "    \"\"\"\n",
        "    r_cols = [c for c in panel.columns if c.startswith(\"r_ex_\")]\n",
        "    qc = {\n",
        "        \"label\": label,\n",
        "        \"rows\": int(len(panel)),\n",
        "        \"dates\": int(panel[\"date\"].nunique()),\n",
        "        \"tickers\": int(panel[\"ticker\"].nunique()),\n",
        "        \"duplicate_keys\": int(panel.duplicated([\"date\",\"ticker\"]).sum()),\n",
        "        \"valid_frac\": float(panel[\"valid_mask\"].mean()) if \"valid_mask\" in panel.columns else None,\n",
        "        \"target_na_frac\": {c: float(panel[c].isna().mean()) for c in r_cols},\n",
        "    }\n",
        "    pd.DataFrame([qc]).to_csv(out_csv, index=False)\n",
        "    return qc\n",
        "\n",
        "\n",
        "def split_panel_by_windows(\n",
        "    panel_master_fp: Path = PANEL_MASTER_FP,\n",
        "    windex_fp: Path = WINDEX_FP,\n",
        "    wmanifest_fp: Path = WMANIFEST_FP,\n",
        "    horizons: List[int] = HORIZONS,\n",
        ") -> Dict:\n",
        "    \"\"\"\n",
        "    Split the unified modeling panel into per-window train/test parquet files.\n",
        "    Also writes per-window QC CSVs.\n",
        "    Returns a summary dict with file paths and counts.\n",
        "    \"\"\"\n",
        "    if not panel_master_fp.exists():\n",
        "        raise FileNotFoundError(f\"Unified panel not found at {panel_master_fp}. Run 3.1.6 first.\")\n",
        "\n",
        "    panel = pd.read_parquet(panel_master_fp)\n",
        "    panel[\"date\"] = pd.to_datetime(panel[\"date\"])\n",
        "\n",
        "    windows = load_windows_plan(windex_fp, wmanifest_fp)\n",
        "    max_h = int(max(horizons)) if horizons else 0\n",
        "\n",
        "    summary = {\"windows\": [], \"max_h\": max_h, \"panel_master\": str(panel_master_fp)}\n",
        "\n",
        "    for w in windows:\n",
        "        win_id = w[\"win_id\"]\n",
        "        t0, t1 = pd.to_datetime(w[\"train_start\"]), pd.to_datetime(w[\"train_end\"])\n",
        "        u0, u1 = pd.to_datetime(w[\"test_start\"]),  pd.to_datetime(w[\"test_end\"])\n",
        "\n",
        "        # Train/Test slices (by date only; (date,ticker) are already unique)\n",
        "        df_train = panel[(panel[\"date\"] >= t0) & (panel[\"date\"] <= t1)].copy()\n",
        "        df_test  = panel[(panel[\"date\"] >= u0) & (panel[\"date\"] <= u1)].copy()\n",
        "\n",
        "        # Defensive tail for train\n",
        "        df_train = _defensive_tail_drop(df_train, max_h=max_h)\n",
        "\n",
        "        # Optional enforcement (usually not required if §2.8 plan is clean)\n",
        "        if FORCE_STITCHED_SPLIT:\n",
        "            # Drop any accidental overlaps\n",
        "            max_train_date = df_train[\"date\"].max() if not df_train.empty else t1\n",
        "            df_test = df_test[df_test[\"date\"] > max_train_date]\n",
        "\n",
        "        # Write files\n",
        "        train_fp = PANELS_DIR / f\"panel_train_{win_id}.parquet\"\n",
        "        test_fp  = PANELS_DIR / f\"panel_test_{win_id}.parquet\"\n",
        "        df_train.to_parquet(train_fp, index=False)\n",
        "        df_test.to_parquet(test_fp, index=False)\n",
        "\n",
        "        # Per-window QC\n",
        "        qc_train = _write_panel_qc(df_train, PANELS_DIR / f\"panel_train_{win_id}_QC.csv\", f\"train_{win_id}\")\n",
        "        qc_test  = _write_panel_qc(df_test,  PANELS_DIR / f\"panel_test_{win_id}_QC.csv\",  f\"test_{win_id}\")\n",
        "\n",
        "        summary[\"windows\"].append({\n",
        "            \"win_id\": win_id,\n",
        "            \"train_start\": w[\"train_start\"],\n",
        "            \"train_end\":   w[\"train_end\"],\n",
        "            \"test_start\":  w[\"test_start\"],\n",
        "            \"test_end\":    w[\"test_end\"],\n",
        "            \"train_rows\":  int(len(df_train)),\n",
        "            \"test_rows\":   int(len(df_test)),\n",
        "            \"train_fp\":    str(train_fp),\n",
        "            \"test_fp\":     str(test_fp),\n",
        "            \"qc_train\":    qc_train,\n",
        "            \"qc_test\":     qc_test,\n",
        "        })\n",
        "\n",
        "    # Save a simple index for Section 3 consumers\n",
        "    (ALPHA_DIR / \"panels_index.json\").write_text(json.dumps(summary, indent=2))\n",
        "    return summary\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 3.1.8 (optional) — Minimal roll-up validation for §3.1\n",
        "# This aggregates what we already wrote in earlier steps (targets_qc, leakage scan,\n",
        "# per-window QC) so you get a single line in CI telling you whether §3.1 is healthy.\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "def build_3_1_validation_report() -> Dict:\n",
        "    \"\"\"\n",
        "    Collects and summarizes key QC from §3.1:\n",
        "      - targets_qc.json (coverage, sector-neutral coverage if enabled)\n",
        "      - leakage_scan.json (schema/tail checks)\n",
        "      - panels_index.json (per-window row counts)\n",
        "    Writes artifacts/alpha/3_1_validation_report.json\n",
        "    \"\"\"\n",
        "    report = {\"status\": \"PASS\", \"checks\": {}}\n",
        "\n",
        "    # Targets QC\n",
        "    if TARGETS_QC_FP.exists():\n",
        "        tgt_qc = json.loads(TARGETS_QC_FP.read_text())\n",
        "        report[\"checks\"][\"targets_qc\"] = tgt_qc\n",
        "        # Basic guard: decent valid fraction\n",
        "        if tgt_qc.get(\"valid_frac\", 0.0) < 0.75:  # #TOCHANGE threshold if needed\n",
        "            report[\"status\"] = \"WARN\"\n",
        "            report.setdefault(\"notes\", []).append(\"Low valid_frac in targets.\")\n",
        "    else:\n",
        "        report[\"status\"] = \"FAIL\"\n",
        "        report[\"checks\"][\"targets_qc\"] = \"missing\"\n",
        "        report.setdefault(\"notes\", []).append(\"targets_qc.json not found.\")\n",
        "\n",
        "    # Leakage scan\n",
        "    if LEAK_SCAN_FP.exists():\n",
        "        leak = json.loads(LEAK_SCAN_FP.read_text())\n",
        "        report[\"checks\"][\"leakage_scan\"] = leak\n",
        "        if leak.get(\"status\") == \"FAIL\":\n",
        "            report[\"status\"] = \"FAIL\"\n",
        "        elif leak.get(\"status\") == \"WARN\" and report[\"status\"] != \"FAIL\":\n",
        "            report[\"status\"] = \"WARN\"\n",
        "    else:\n",
        "        report[\"status\"] = \"FAIL\"\n",
        "        report[\"checks\"][\"leakage_scan\"] = \"missing\"\n",
        "        report.setdefault(\"notes\", []).append(\"leakage_scan.json not found.\")\n",
        "\n",
        "    # Panels index\n",
        "    panels_idx_fp = ALPHA_DIR / \"panels_index.json\"\n",
        "    if panels_idx_fp.exists():\n",
        "        panels_idx = json.loads(panels_idx_fp.read_text())\n",
        "        report[\"checks\"][\"panels_index\"] = {\n",
        "            \"n_windows\": len(panels_idx.get(\"windows\", [])),\n",
        "            \"max_h\": panels_idx.get(\"max_h\"),\n",
        "        }\n",
        "        # Sanity: all windows must have some test rows\n",
        "        for w in panels_idx.get(\"windows\", []):\n",
        "            if w.get(\"test_rows\", 0) <= 0:\n",
        "                report[\"status\"] = \"WARN\" if report[\"status\"] != \"FAIL\" else \"FAIL\"\n",
        "                report.setdefault(\"notes\", []).append(f\"No TEST rows for {w['win_id']}.\")\n",
        "    else:\n",
        "        report[\"status\"] = \"FAIL\"\n",
        "        report[\"checks\"][\"panels_index\"] = \"missing\"\n",
        "        report.setdefault(\"notes\", []).append(\"panels_index.json not found.\")\n",
        "\n",
        "    VAL_3_1_FP.write_text(json.dumps(report, indent=2))\n",
        "    return report\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Example end-to-end for §3.1.7–3.1.8:\n",
        "#\n",
        "# 1) Ensure you’ve run:\n",
        "#    - 3.1.1/3.1.2 (load_base_data, load_regime_context)\n",
        "#    - 3.1.3/3.1.4 (compute_daily_returns, compute_excess_returns)\n",
        "#    - 3.1.5       (forward_excess_targets → targets.parquet/targets_qc.json)\n",
        "#    - 3.1.6       (build_unified_model_panel → panel_master.parquet, feature_list.json, leakage_scan.json)\n",
        "#\n",
        "# 2) Split into windows:\n",
        "#    summary = split_panel_by_windows(\n",
        "#        panel_master_fp=PANEL_MASTER_FP,\n",
        "#        windex_fp=WINDEX_FP,\n",
        "#        wmanifest_fp=WMANIFEST_FP,\n",
        "#        horizons=HORIZONS,\n",
        "#    )\n",
        "#\n",
        "# 3) Optional: roll-up validation\n",
        "#    report = build_3_1_validation_report()\n",
        "#    print(report[\"status\"], report.get(\"notes\"))\n",
        "#\n",
        "# Deliverables from this step:\n",
        "#   - artifacts/alpha/panels/panel_train_<winid>.parquet\n",
        "#   - artifacts/alpha/panels/panel_test_<winid>.parquet\n",
        "#   - artifacts/alpha/panels/panel_train_<winid>_QC.csv\n",
        "#   - artifacts/alpha/panels/panel_test_<winid>_QC.csv\n",
        "#   - artifacts/alpha/panels_index.json\n",
        "#   - artifacts/alpha/3_1_validation_report.json\n",
        "# ──────────────────────────────────────────────────────────────────────────────"
      ],
      "metadata": {
        "id": "TTDtqE-_A8xb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## this needs work later\n",
        "# 3.1 — Orchestrator (stub)\n",
        "\n",
        "RUN_ORCHESTRATOR = False\n",
        "\n",
        "def run_section_3_1(\n",
        "    horizons=None,\n",
        "    sector_neutral=None,\n",
        "    roll_lookback=None,\n",
        "    debug_max_tickers=None,\n",
        "    n_smooth_g=None,\n",
        "    print_summary: bool = True,\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    End-to-end 3.1 runner:\n",
        "      1) 3.1.1/3.1.2 load base & regimes\n",
        "      2) 3.1.3 daily returns (assets/hedges)\n",
        "      3) 3.1.4 residuals (SPY-only or sector-neutral)\n",
        "      4) 3.1.5 forward targets + ranks\n",
        "      5) 3.1.6 unified modeling panel (X×Y×R)\n",
        "      6) 3.1.7 windowed split + per-window QC\n",
        "      7) 3.1.8 (optional) roll-up validation for §3.1\n",
        "    \"\"\"\n",
        "    # honor overrides (use #TOCHANGE defaults otherwise)\n",
        "    _H = horizons or HORIZONS\n",
        "    _SN = SECTOR_NEUTRAL if sector_neutral is None else bool(sector_neutral)\n",
        "    _RL = ROLL_LOOKBACK_D if roll_lookback is None else int(roll_lookback)\n",
        "    _DM = DEBUG_MAX_TICKERS if debug_max_tickers is None else int(debug_max_tickers)\n",
        "    _NG = N_SMOOTH_G if n_smooth_g is None else int(n_smooth_g)\n",
        "\n",
        "    # 1) Base + regimes\n",
        "    feats, px, cs_cols, regimes_keep = prep_3_1_3_context(\n",
        "        debug_max_tickers=DEBUG_MAX_TICKERS,\n",
        "        n_smooth_g=N_SMOOTH_G,\n",
        "    )\n",
        "    log_3_1_1_2_summary(feats, px, cs_cols, regimes_keep)\n",
        "\n",
        "    px_ret = compute_daily_returns(px)   # equities only\n",
        "    spy_ret = load_spy_returns()         # from S2 market_panel or S1 raw_prices\n",
        "\n",
        "    # 3) Sector map/returns (only if enabled)\n",
        "    sector_map = load_sector_map(SECTOR_ETF_MAP_FP) if _SN else None\n",
        "    if _SN and sector_map is not None:\n",
        "        etfs = sector_map[\"sector_etf\"].dropna().unique().tolist()\n",
        "        sector_rets = load_sector_etf_returns(etfs)   # from raw_prices.parquet\n",
        "    else:\n",
        "        sector_rets = None\n",
        "\n",
        "    # 4) Excess returns (residuals)\n",
        "    def compute_excess_returns(\n",
        "        px_ret: pd.DataFrame,\n",
        "        spy_ret: pd.Series,\n",
        "        lookback: int = ROLL_LOOKBACK_D,\n",
        "        sector_neutral: bool = SECTOR_NEUTRAL,\n",
        "        sector_map: Optional[pd.DataFrame] = None,\n",
        "        sector_returns: Optional[pd.DataFrame] = None,\n",
        "        lambda_ridge: float = LAMBDA_RIDGE,\n",
        "    ) -> Tuple[pd.DataFrame, Dict]:\n",
        "        \"\"\"\n",
        "        Compute residual daily log returns ('excess') for each equity:\n",
        "          • SPY-only mode: resid_1d = ret_1d - β_SPY * spy_ret\n",
        "          • Sector-neutral mode: resid_1d = ret_1d - (β_SPY*spy + β_SECTOR*sector)\n",
        "\n",
        "        Inputs:\n",
        "            px_ret        : ['date','ticker','ret_1d'] for equities + hedges\n",
        "            spy_ret       : SPY series indexed by date\n",
        "            lookback      : rolling window length for betas\n",
        "            sector_neutral: enable sector model (requires sector_map and ETF returns)\n",
        "            sector_map    : ['ticker','sector_etf'] for equities (if None → SPY-only)\n",
        "            sector_returns: ['date','sector_etf','sector_ret_1d'] (if None, inferred from px_ret)\n",
        "            lambda_ridge  : small ridge (stabilizer) for 2x2 inverse #TOCHANGE real run: 1e-6\n",
        "\n",
        "        Outputs:\n",
        "            resid_df: ['date','ticker','resid_1d','beta_spy','beta_sector'] (beta_sector may be NaN)\n",
        "            qc      : dict with coverage info (e.g., sector_beta_coverage)\n",
        "        \"\"\"\n",
        "        # Base frame with asset ret_1d and SPY ret on each date\n",
        "        df = px_ret[[\"date\", \"ticker\", \"ret_1d\"]].copy()\n",
        "        df = df.merge(spy_ret.rename(\"spy_ret_1d\"), left_on=\"date\", right_index=True, how=\"left\")\n",
        "\n",
        "        # Defaults\n",
        "        have_sector = False\n",
        "        sector_cov_frac = 0.0\n",
        "\n",
        "        if sector_neutral and (sector_map is not None) and (\"sector_etf\" in sector_map.columns):\n",
        "            # Attach sector ETF daily ret to each equity row\n",
        "            if sector_returns is None:\n",
        "                # infer sector_returns from px_ret\n",
        "                etfs = sector_map[\"sector_etf\"].dropna().unique().tolist()\n",
        "                sector_returns = (\n",
        "                    px_ret[px_ret[\"ticker\"].isin(etfs)][[\"date\", \"ticker\", \"ret_1d\"]]\n",
        "                    .rename(columns={\"ticker\": \"sector_etf\", \"ret_1d\": \"sector_ret_1d\"})\n",
        "                )\n",
        "            df = df.merge(sector_map[[\"ticker\", \"sector_etf\"]], on=\"ticker\", how=\"left\")\n",
        "            df = df.merge(sector_returns, on=[\"date\", \"sector_etf\"], how=\"left\")\n",
        "            have_sector = True\n",
        "\n",
        "    resid_df, qc_excess = compute_excess_returns(\n",
        "        px_ret=px_ret,\n",
        "        spy_ret=spy_ret,\n",
        "        lookback=_RL,\n",
        "        sector_neutral=_SN,\n",
        "        sector_map=sector_map,\n",
        "        sector_returns=sector_rets,\n",
        "    )\n",
        "\n",
        "    # 5) Forward targets + ranks\n",
        "    targets = forward_excess_targets(resid_df, horizons=_H, write_artifacts=True, extra_qc=qc_excess)\n",
        "\n",
        "    # 6) Unified panel (X×Y×R)\n",
        "    panel, meta = build_unified_model_panel(feats, targets, regimes_keep, cs_cols, write_artifacts=True)\n",
        "\n",
        "    # 7) Split by walk-forward windows + per-window QC\n",
        "    split_summary = split_panel_by_windows(\n",
        "        panel_master_fp=PANEL_MASTER_FP, windex_fp=WINDEX_FP, wmanifest_fp=WMANIFEST_FP, horizons=_H\n",
        "    )\n",
        "\n",
        "    # 8) Optional: roll-up validation for §3.1\n",
        "    val = build_3_1_validation_report()\n",
        "\n",
        "    # Compose run summary\n",
        "    s = {\n",
        "        \"params\": {\n",
        "            \"horizons\": _H,\n",
        "            \"sector_neutral\": _SN,\n",
        "            \"roll_lookback_days\": _RL,\n",
        "            \"debug_max_tickers\": _DM,\n",
        "            \"n_smooth_g\": _NG,\n",
        "        },\n",
        "        \"artifacts\": {\n",
        "            \"targets\": _exists_size(TARGETS_FP),\n",
        "            \"targets_qc\": _exists_size(TARGETS_QC_FP),\n",
        "            \"feature_list\": _exists_size(FEATURE_LIST_FP),\n",
        "            \"leakage_scan\": _exists_size(LEAK_SCAN_FP),\n",
        "            \"panel_master\": _exists_size(PANEL_MASTER_FP),\n",
        "            \"panels_index\": _exists_size(ALPHA_DIR / \"panels_index.json\"),\n",
        "            \"val_3_1\": _exists_size(ALPHA_DIR / \"3_1_validation_report.json\"),\n",
        "        },\n",
        "        \"shapes\": {\n",
        "            \"feats_rows\": int(len(feats)),\n",
        "            \"panel_master_rows\": int(len(panel)),\n",
        "            \"targets_rows\": int(len(targets)),\n",
        "        },\n",
        "        \"qc\": {\n",
        "            \"excess_qc\": qc_excess,\n",
        "            \"val_3_1_status\": val.get(\"status\"),\n",
        "            \"val_3_1_notes\": val.get(\"notes\", []),\n",
        "        },\n",
        "        \"windows\": split_summary.get(\"windows\", []),\n",
        "    }"
      ],
      "metadata": {
        "id": "L2Qus1xCifeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Portfolio Construction & Risk"
      ],
      "metadata": {
        "id": "GB56azskw8x_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XIo2hIhCzR14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. RL Sizing Policy (PPO)"
      ],
      "metadata": {
        "id": "OVTV8iNaxcly"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nR9-doWYzSSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Backtesting (Backward Testing) — Rigor"
      ],
      "metadata": {
        "id": "MxL1szv9x19g"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_8u_WWZvzStF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Forward Testing (No Orders; Shadow Runs)"
      ],
      "metadata": {
        "id": "24zaJ3GOx5RJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "45bA-KpizTFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Cost Model & Execution Assumptions"
      ],
      "metadata": {
        "id": "o_yX0Y24x8qb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H-8_FoZ5zTl-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Reproducibility & Testability"
      ],
      "metadata": {
        "id": "ic1uXdOax_ak"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jTVyA9gyzT-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. Visualization & Reporting"
      ],
      "metadata": {
        "id": "NJVGj0mKyBZX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VYhvh69GzUaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11. Automation Options (Optional, no trading)"
      ],
      "metadata": {
        "id": "cL8XA8jqyDfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t1tL4b7EzU3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12. Optional Alpaca Integration (disabled by default)"
      ],
      "metadata": {
        "id": "0Ywf60UCyFbB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ab8-P8WEzWq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 13. File/Module Structure (Colab-friendly)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "/project\n",
        "  config.yaml\n",
        "  data/\n",
        "    universe.csv\n",
        "    features.parquet\n",
        "    regime_labels.parquet\n",
        "  models/\n",
        "    lstm_*.pt / .h5\n",
        "    gbm_*.txt\n",
        "    stacker_*.pkl\n",
        "    rl_policy_*.pkl\n",
        "  runs/YYYY-MM-DD/\n",
        "    signals.parquet\n",
        "    weights.parquet\n",
        "    hedges.parquet\n",
        "    daily_pnl.csv\n",
        "    risk.json\n",
        "  reports/\n",
        "    backtest_tearsheet.html\n",
        "    forward_tearsheet_YYYY-MM.html\n",
        "  src/\n",
        "    data_loader.py\n",
        "    feature_engineering.py\n",
        "    regime.py\n",
        "    models_lstm.py\n",
        "    models_tabular.py\n",
        "    stacking.py\n",
        "    uncertainty.py\n",
        "    portfolio_bl_rp.py\n",
        "    hedging.py\n",
        "    rl_policy.py\n",
        "    backtest.py\n",
        "    forward_shadow.py\n",
        "    risk_metrics.py\n",
        "    stats_tests.py  # DM, SPA/White RC, Sharpe inference\n",
        "    monte_carlo.py  # block bootstrap\n",
        "    reporting.py    # plots & HTML/PDF\n",
        "  main.py          # CLI: daily-shadow / weekly-train / monthly-report\n",
        "  notebook.ipynb   # Colab master: end-to-end run with toggles\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "Jb1M33e5yJKy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 14. More info\n",
        "\n",
        "- Suggested stack: pandas, numpy, scikit-learn, lightgbm, xgboost, tensorflow/PyTorch (choose one for LSTM), hmmlearn, stable-baselines3, cvxpy (for BL/optimization), arch (optional), statsmodels, scipy, matplotlib/plotly.\n",
        "\n",
        "Compute plan (fits $50–$100):\n",
        "\n",
        "- S&P 100, 5–8 walk-forward windows.\n",
        "\n",
        "- LSTM 1–2 layers (64–128 units), MC-dropout 20 samples.\n",
        "\n",
        "- PPO with modest timesteps per window.\n",
        "\n",
        "- 200–400 Monte Carlo bootstrap paths.\n",
        "\n",
        "- 1–3 GPU hours on Colab Pro/Pro+; RAM < 24GB."
      ],
      "metadata": {
        "id": "LYsWzmxjyXy8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 15. Build Order (fastest to value)\n",
        "\n",
        "1. Data + Features + Regimes → validate leakage & plots.\n",
        "\n",
        "2. Multifactor composite → baseline cross-sec L/S backtest.\n",
        "\n",
        "3. GBM/MLP + LSTM → stacking + uncertainty; re-run backtest.\n",
        "\n",
        "4. BL + RP + Dynamic hedge → re-run backtest & stress.\n",
        "\n",
        "5. RL sizing → ablation vs no-RL; finalize backtest.\n",
        "\n",
        "6. Forward shadow loop (daily), weekly retrain, monthly reports.\n",
        "\n",
        "7. Automation (Actions/cron), optional Alpaca paper stub (off)."
      ],
      "metadata": {
        "id": "KZCqpKiHynK6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 16. What you'll see in the first results\n",
        "- Backtest tear sheet with OO-S equity curve, MC bands, by-regime tables, SPA/DM outcomes, VaR/CVaR & stress.\n",
        "\n",
        "- Ablation:\n",
        "\n",
        "  - Multifactor only → +ML → +ML+RL;\n",
        "\n",
        "  - Market-neutral vs long-only w/ hedging;\n",
        "\n",
        "  - Cost sensitivity 5–20 bps.\n",
        "\n",
        "- A live forward dashboard (from Day 1) accumulating daily PnL + monthly report.\n",
        "\n"
      ],
      "metadata": {
        "id": "GR2YWGzNy6Ip"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 17. Forward-Testing Duration Recommendation\n",
        "\n",
        "- Run at least 4 weeks forward shadow to confirm plumbing & stability.\n",
        "\n",
        "- Prefer 8–12 weeks to evaluate regime adaptation, RL sizing behavior under drawdowns, and cost realism.\n",
        "\n",
        "- Only after the forward period matches backtest risk/return within expected error bands should you consider paper-trading execution.\n",
        "\n"
      ],
      "metadata": {
        "id": "2Y4YzjnzzF_r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2rj3UeBraWs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary><strong>Outline Details</strong></summary>\n",
        "\n",
        "# Project Outline — Regime-Aware Multifactor + LSTM/Ensembles + RL (with rigorous back & forward testing)\n",
        "\n",
        "## 0) Objectives & Success Criteria\n",
        "**Primary objective:** Generate statistically significant pure alpha (market-neutral) with controlled drawdowns after transaction costs.  \n",
        "\n",
        "**Secondary objective:** Build a repeatable process capable of ongoing, unattended forward testing that outputs monthly tear sheets.  \n",
        "\n",
        "**Pass/Fail gates (OO-S):**  \n",
        "- Annualized Sharpe ≥ 1.0 (cost-adjusted) across walk-forward windows.  \n",
        "- SPA/White Reality Check non-rejection vs family of alternatives at 5–10% level.  \n",
        "- Max DD ≤ 15–20% (tunable) in backtests.  \n",
        "- Forward test (4–8+ weeks): positive return, rolling Sharpe > 0.8, tail losses consistent with backtest VaR/CVaR.  \n",
        "\n",
        "---\n",
        "\n",
        "## 1) Data & Universe\n",
        "\n",
        "### 1.1 Universe\n",
        "- S&P 100 equities (liquid, keeps compute sane).  \n",
        "- Hedging instruments: SPY + sector ETFs (XLY, XLF, XLV, XLK, XLI, XLE, XLP, XLB, XLU, XLRE).  \n",
        "- Source: Yahoo Finance (daily bars).  \n",
        "- Lookback: 10–15 years if available (train 2012→, test recent).  \n",
        "\n",
        "### 1.2 Features\n",
        "- **Returns/vol:** log returns (1–60d lags), realized vol, ATR.  \n",
        "- **Momentum:** 12–1, 6–1, 20d, trend filters (e.g., SMA cross, slope).  \n",
        "- **Value:** B/P, E/P, CF/P, shareholder yield (latest available; forward-fill monthly/quarterly).  \n",
        "- **Quality:** gross profitability, ROE, accruals, leverage, F-Score-like composite.  \n",
        "- **Market context:** VIX, SPY vol, market breadth (% advancers, optional).  \n",
        "- Leakage controls: strictly lag all features, align to t-1; winsorize & z-score cross-sectionally.  \n",
        "\n",
        "### 1.3 Data Hygiene\n",
        "- Survivorship-bias approach: use current S&P 100 for practicality; (optional) point-in-time later.  \n",
        "- Corporate actions: use adjusted prices.  \n",
        "- Missing fundamentals: impute conservatively or drop; record masks for model.  \n",
        "- **Deliverables:** `features.parquet`, `universe.csv`, `meta.yaml`.  \n",
        "\n",
        "---\n",
        "\n",
        "## 2) Regime Modeling\n",
        "\n",
        "### 2.1 HMM (2–3 states)\n",
        "- Inputs: SPY daily returns/vol, VIX level/change, market breadth.  \n",
        "- States: Risk-On, Risk-Off, Transition (labeled by average return/vol).  \n",
        "- **Output:** daily regime label + posterior probabilities.  \n",
        "\n",
        "### 2.2 Usage\n",
        "- Regime-specific ensemble weights, turnover caps, and risk targets.  \n",
        "- Momentum throttled in Risk-Off; quality emphasized.  \n",
        "- **Deliverables:** `regime_labels.parquet`, regime plot.  \n",
        "\n",
        "---\n",
        "\n",
        "## 3) Alpha Layer (Signals)\n",
        "\n",
        "### 3.1 Multifactor Composite\n",
        "- Value/Momentum/Quality composites (winsorized, z-scored).  \n",
        "- Per-regime blend fit with ridge.  \n",
        "- **Output:** factor alpha score per asset/day.  \n",
        "\n",
        "### 3.2 ML Overlays\n",
        "- **LSTM:** 60-day sequences → t+5/t+10 returns; MC-dropout for uncertainty.  \n",
        "- **Tabular ensembles:** LightGBM (primary), XGBoost, small MLP; also quantile versions.  \n",
        "- **Stacking meta-learner:** ridge/LightGBM; OOF training within walk-forward train window.  \n",
        "- **Output:** final forecast (mean) + uncertainty proxy.  \n",
        "\n",
        "### 3.3 Uncertainty → Confidence\n",
        "- Expected Sharpe proxy = mean / std_hat.  \n",
        "- Bucket confidence for analytics.  \n",
        "- **Deliverables:** `alpha_raw.parquet`, `alpha_ensemble.parquet`, feature importance charts.  \n",
        "\n",
        "---\n",
        "\n",
        "## 4) Portfolio Construction & Risk\n",
        "\n",
        "### 4.1 Baseline Weights\n",
        "- Cross-sectional L/S: long top decile, short bottom decile by forecasted Sharpe.  \n",
        "- Beta-neutral, per-name and sector caps.  \n",
        "\n",
        "### 4.2 Black–Litterman (BL)\n",
        "- Prior: market-cap weights → implied μ.  \n",
        "- Views: ensemble alphas scaled by uncertainty.  \n",
        "- Posterior μ̂ → mean-variance with L2 & turnover penalty.  \n",
        "\n",
        "### 4.3 Risk Parity & Vol Target\n",
        "- Equalize risk across sector/factor clusters.  \n",
        "- Target portfolio vol (8–12% ann.).  \n",
        "\n",
        "### 4.4 Dynamic Hedging\n",
        "- Daily orthogonalization vs SPY + sectors; hedge ratios adjustable by RL.  \n",
        "- **Deliverables:** weights, exposures, hedge plots.  \n",
        "\n",
        "---\n",
        "\n",
        "## 5) RL Sizing Policy (PPO)\n",
        "\n",
        "### 5.1 Role\n",
        "- Scales risk target and tunes hedges.  \n",
        "\n",
        "### 5.2 State\n",
        "- Regime, vol, drawdown, alpha strength, uncertainty, turnover, betas, cost model.  \n",
        "\n",
        "### 5.3 Reward\n",
        "- PnL – costs – λ·CVaR_tail – κ·Δdrawdown – penalties.  \n",
        "\n",
        "### 5.4 Training\n",
        "- Train within walk-forward segments; fixed seeds.  \n",
        "- **Deliverables:** `rl_policy.pkl`, diagnostics.  \n",
        "\n",
        "---\n",
        "\n",
        "## 6) Backtesting (Backward Testing) — Rigor\n",
        "\n",
        "### 6.1 Walk-Forward Engine\n",
        "- Rolling/expanding windows; purged & embargoed CV.  \n",
        "- Refit all models per window; test daily with costs.  \n",
        "\n",
        "### 6.2 Significance & Reality Checks\n",
        "- DM test, SPA/White RC, Sharpe inference.  \n",
        "\n",
        "### 6.3 Tail Risk & Stress\n",
        "- VaR/CVaR; stress tests (2008/2020, vol shocks, liquidity cuts).  \n",
        "\n",
        "### 6.4 Monte Carlo Robustness\n",
        "- Block bootstrap; output PnL envelopes.  \n",
        "- **Deliverables:** equity curves, DD charts, ablations.  \n",
        "\n",
        "---\n",
        "\n",
        "## 7) Forward Testing (Shadow Mode)\n",
        "\n",
        "### 7.1 Daily Shadow Run\n",
        "- No backfill; use latest models; log all artifacts.  \n",
        "\n",
        "### 7.2 Retraining Cadence\n",
        "- Weekly or bi-weekly; strict forward-only.  \n",
        "\n",
        "### 7.3 Monthly Auto-Report\n",
        "- Tear sheets with returns, Sharpe, DD, risk, regime PnL, VaR/CVaR.  \n",
        "\n",
        "### 7.4 Duration\n",
        "- Min: 4 weeks; Pref: 8–12 weeks.  \n",
        "- **Deliverables:** daily run files, monthly reports.  \n",
        "\n",
        "---\n",
        "\n",
        "## 8) Cost Model & Execution Assumptions\n",
        "- Costs: 10 bps round-trip (sweep 5–20).  \n",
        "- Slippage: 1–2 bps; higher in Risk-Off.  \n",
        "- Short borrow: 10–50 bps ann.  \n",
        "- Liquidity caps: ≤5–10% ADV.  \n",
        "\n",
        "---\n",
        "\n",
        "## 9) Reproducibility & Testability\n",
        "- Config-driven (`config.yaml`); fixed seeds.  \n",
        "- Unit/integration tests for leakage, CV folds, NaNs, RL bounds.  \n",
        "- Experiment tracking with CSV/JSON + git hash.  \n",
        "\n",
        "---\n",
        "\n",
        "## 10) Visualization & Reporting\n",
        "- Equity curves with regime shading, rolling metrics, exposures, attribution, bucket PnL, by-regime performance, risk dashboards.  \n",
        "\n",
        "---\n",
        "\n",
        "## 11) Automation Options\n",
        "- **Colab:** manual or scheduled;  \n",
        "- **GitHub Actions:** nightly, weekly, monthly;  \n",
        "- **VM + cron:** low-budget option.  \n",
        "\n",
        "---\n",
        "\n",
        "## 12) Optional Alpaca Integration\n",
        "- Disabled by default; forward test never sends orders; later optional paper fills.  \n",
        "\n",
        "</details>\n"
      ],
      "metadata": {
        "id": "Wh8ahohLNbIu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZqgdR3MJxvfk"
      }
    }
  ]
}